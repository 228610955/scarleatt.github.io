<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Anomaly Detection</title>
    <url>/2020/07/03/Anomaly-detection/</url>
    <content><![CDATA[<h4 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h4><p>对于给定的训练数据集，做异常检测的目的是找到一个函数，这个函数可以检测输入的x是不是和训练数据集相似的。异常检测知识检测出和训练数据不一样的输入，检测出来的数据并不一定都是不好的，有可能是好的，也有可能是不好的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703142752745.png" alt="image-20200703142752745" style="zoom:60%;"></p>
<p>那么到底怎么来定义这个similarity呢？这也就是异常检测需要探讨的问题，不同的方法就有不同的定义方式。</p>
<h4 id="What-is-Anomaly"><a href="#What-is-Anomaly" class="headerlink" title="What is Anomaly?"></a>What is Anomaly?</h4><p>Q：那么我们到底如何来定义anomaly呢？如何确定我们检测出来的输入是anomaly的呢？</p>
<p>A：取决于具体的训练数据集。在下图中，如果训练集中有很多雷丘，那么皮卡丘就是异常；但如果有很多只皮卡丘，那么雷丘就是异常,….</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703143729820.png" alt="image-20200703143729820" style="zoom:60%;"></p>
<h4 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h4><p>异常检测有很多应用，以下简要叙述三个：</p>
<ol>
<li><p>Fraud Detection，training data是正常刷卡行为， 那么现有一笔新的交易记录x，我们就可以进行异常检测，来判断x是不是盗刷行为；比如日常都是小额消费，但突然多了一些连续的高额消费，就可以认为这是盗刷行为；</p>
</li>
<li><p>Network Intrusion Detection，training data是正常连接行为，现在有一个新的连接x进来，那么我们就可以让机器自己进行异常检测，来判断新连接是不是异常的；</p>
</li>
<li><p>Cancer Detection，training data是正常细胞的资料，比如细胞核的大小、分裂的频率等，如果来一个新的细胞x，机器可以自己决定到底是正常细胞还是癌细胞。</p>
</li>
</ol>
<h4 id="Binary-Classification"><a href="#Binary-Classification" class="headerlink" title="Binary Classification?"></a>Binary Classification?</h4><p>Q：给定正常的数据（Class 1）为$\{x^1,x^2,..,x^N\}$，异常的数据（Class 2）为$\{\tilde x^1,\tilde x^2,..,\tilde x^N\}$；那么异常检测可以认为是一个二分类问题吗？</p>
<p>A：如果正常数据集是“宝可梦”，那么异常数据就“不是宝可梦”，但“不是宝可梦”包含很多种类别，比如茶壶、树等；而且异常数据并不像正常数据那么容易收集；因此并不能简单地进行一个二分类问题。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703145452812.png" alt="image-20200703145452812" style="zoom:60%;"></p>
<h4 id="Categories"><a href="#Categories" class="headerlink" title="Categories"></a>Categories</h4><p>异常检测可以分为以下几个类别：</p>
<ol>
<li>training data带label，每个训练数据$x^i$都有对应的标签$\hat y ^i$，一共有N个类别，那么我们就可以训练一个classifier，来进行分类，如果机器能辨别出这个input是1-N个类别，那么就认为是正常数据；如果机器不能辨别，即输出类别为“unknown”，我们就认为这是异常数据；</li>
<li>如果training data是unlabel，这时又分为两种情况：（1）training data是clean的，所有的数据都是正常的；（2）training data中有一小部分数据是异常的，比如银行要对用户数据进行异常检测，那么training data就不可避免地包括一些异常数据。</li>
</ol>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703150215978.png" alt="image-20200703150215978" style="zoom:60%;"></p>
<h4 id="Case-1-With-Classifier"><a href="#Case-1-With-Classifier" class="headerlink" title="Case 1: With Classifier"></a>Case 1: With Classifier</h4><h5 id="Example-Application"><a href="#Example-Application" class="headerlink" title="Example Application"></a>Example Application</h5><p>现在我们判断一个人物是不是来自辛普森的家庭。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703151112051.png" alt="image-20200703151112051" style="zoom:60%;"></p>
<p>在下图中对于输入的正常数据$\{x^1,x^2,..,x^N\}$，都有其对应的类别标签$\{\hat y^1,\hat y^2,..,\hat y^N\}$，那么我们就可以训练一个classifier，来预测其输出的类别。一位很喜爱辛普森的学者进行了实验，得出了很不错的分类结果。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703151635273.png" alt="image-20200703151635273" style="zoom:60%;"></p>
<h5 id="How-to-use-the-Classifier"><a href="#How-to-use-the-Classifier" class="headerlink" title="How to use the Classifier"></a>How to use the Classifier</h5><p>那么根据训练出来的classifier来做异常检测，来判断一个人物到底是不是来自辛普森家庭。</p>
<p>我们可以让classifier再输出一个confidence score，同时我们还需要定义一个界限threshold $\lambda$，如果信心分数$c(x)&gt;\lambda$，就认为是正常数据；否则就认为是异常数据。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703152031850.png" alt="image-20200703152031850" style="zoom:60%;"></p>
<p>那么我们如何得到这个confidence score呢？</p>
<p>在下图的例子中，如果是一个辛普森家庭的人物输入classifier，这个分类器能够很自信地得出分类结果是“霸子”（0.97）；但如果是一个非辛普森的人物输入，classifier的输出分类结果就比较均匀，4个分类就过都是0.20+。那么我们就可以认为classifier对第一个input是很自信的，是normal；第二个input则不自信，被认为是anomaly data。</p>
<p>那么我们就可以把这个confidence score认为是分类结果的distribution中的最大值，也可以做cross-entropy。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703153414866.png" alt="image-20200703153414866" style="zoom:60%;"></p>
<p>那么现在我们把辛普森家庭和其他动漫图片的所有数据都进行confidence score计算。在下图中，我们可以发现属于辛普森的confidence score distribution，几乎全都是1；但对于其他动漫的人物，只有10%的score为1，其他大多数的图片得到的分数都是比较低的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703154546578.png" alt="image-20200703154546578" style="zoom:60%;"></p>
<h5 id="Example-Framework"><a href="#Example-Framework" class="headerlink" title="Example Framework"></a><strong>Example Framework</strong></h5><p>训练数据集是带label的，全都是属于辛普森家庭，那么我们可以训练出一个classifier，根据这个分类器，我们可以计算出每张图片的confidence score，根据threshold来判断这张图片是不是anomaly。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703155315020.png" alt="image-20200703155315020" style="zoom:60%;"></p>
<p><strong>development set</strong>中需要包括带label（是否属于辛普森）的图片，<u>但这个图片不仅包括辛普森家庭的人物，还包括不属于辛普森家庭的人物</u>。将数据输入训练好的classifier，根据不同的$\lambda$，得到系统的performance，那么我们就可以通过development set来调整$\lambda$的值，选择使系统得到最好performance的$\lambda$。</p>
<h5 id="Evalution"><a href="#Evalution" class="headerlink" title="Evalution"></a>Evalution</h5><p>现在有100张辛普森人物的图片，5张非辛普森。在图中红色部分则表示非辛普森的score，一共有5个方块，得到的分数都很低。</p>
<p>正确率并不能来准确衡量一个系统的好坏，很可能一个系统有一个很高的正确率，但其实并不好。在下图中，我们可以计算出这个系统所计算出的accuracy为95.2%，但这并不是因为我们的异常检测系统很好，而是因为数据集的分布差异，异常数据所占的比例非常小。这并不是我们想要的异常检测系统，这个系统几乎会把所有的输入都判断成是异常的，但由于异常数据占比很小，最后的正确率还是会很高。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703160756452.png" alt="image-20200703160756452" style="zoom:60%;"></p>
<p>因此我们不能用正确率accuracy来判断系统的performance。</p>
<p>在下图中，我们把threshold设置为大于0.5的值，这时只有1个异常值被检测出来，还有剩下4个异常值没有被检测出来，这4个被认为是<u>missing</u>了；而在所有的100个辛普森人物里面，只有1个被检测为异常值，被认为是<u>False alarm</u>，剩下的99个都认为是正常的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703162036050.png" alt="image-20200703162036050" style="zoom:60%;"></p>
<p>如果我们现在把threshold设置为大于0.8的值，这时只有2个异常值被检测出来，还有剩下3个异常值没有被检测出来（missing）；而在所有的100个辛普森人物里面，只有6个被检测为异常值（false alarm），剩下的94个都认为是正常的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703162819041.png" alt="image-20200703162819041" style="zoom:60%;"></p>
<p>那么这两个threshold分别为0.5和0.8的异常检测系统，到底哪一个更好呢？这取决于你认为missing更严重，还是false alarm更严重。</p>
<p>在下图中的<span style="color: red">Cost Table A</span>，如果是missing，就扣一分；如果是false alarm，就扣100分；如果用这种方式来衡量系统的好坏，那么左边的被扣了104分，右边的被扣了603分。</p>
<p><span style="color: blue">Cost Table B</span>，如果是missing，就扣100分；如果是false alarm，就扣1分；如果用这种方式来衡量系统的好坏，那么左边的被扣了401分，右边的被扣了306分。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703162727493.png" alt="image-20200703162727493" style="zoom:60%;"></p>
<p>在不同的场景下，就会有不同的cost table。比如癌症检测，missing比false alarm造成的影响大得多，如果这个人患了癌症却没被检测出来，后果会非常严重，因此这种情况我们table B比较好。</p>
<h5 id="Possible-Issues"><a href="#Possible-Issues" class="headerlink" title="Possible Issues"></a>Possible Issues</h5><p>下图中展示了猫狗分类的例子，黑色线条表示decision boundary，靠近boundary得到的confidence score不高，如果在boundary上，就说明分类器不能确定图片的类别是猫还是狗。对于一些特别像猫的动物，比如老虎，也能够得到很好的分数，因为老虎有着和猫很像的特征，而这个特征是是分类器进行分类的关键，就可以迷惑分类器，得出错误的分类结果。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703165414744.png" alt="image-20200703165414744" style="zoom:60%;"></p>
<p>如果这个分类器判断是不是辛普森家庭人物的关键是，人物的脸和皮肤是不是黄色。那么我们可以把其他动漫人物的脸涂成黄色，就可以发现分类器判断为辛普森的概率提高了很多。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703165502001.png" alt="image-20200703165502001" style="zoom:60%;"></p>
<h5 id="To-Learn-More-……"><a href="#To-Learn-More-……" class="headerlink" title="To Learn More ……"></a>To Learn More ……</h5><p>也有一些文献提出了解决方法，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703170742579.png" alt="image-20200703170742579" style="zoom:60%;"></p>
<h4 id="Case-2-Without-Labels"><a href="#Case-2-Without-Labels" class="headerlink" title="Case 2: Without Labels"></a>Case 2: Without Labels</h4><h5 id="Twitch-Plays-Pokemon"><a href="#Twitch-Plays-Pokemon" class="headerlink" title="Twitch Plays Pokémon"></a>Twitch Plays Pokémon</h5><p>先介绍一个游戏，Twitch Plays Pokémon，每个用户都可以在switch上输入自己的指令（up、left、…），可以同时有8万个用户，在这些用户中有些是异常用户，会输入无关指令干扰游戏的正常执行。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703171432329.png" alt="image-20200703171432329" style="zoom:60%;"></p>
<h5 id="Problem-Formulation-1"><a href="#Problem-Formulation-1" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h5><p>那么我们如何移除这些游戏干扰者呢？</p>
<p>我们首先对每一个玩家都用x表示，即表示成一个向量；对于其中一个维度$x_2$，如果这个人经常发表无政府言论，我们就可以认为这个人是来游戏中捣乱的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703171722577.png" alt="image-20200703171722577" style="zoom:60%;"></p>
<p>但现在数据集是没有label的，没有classifier了，那么我们可以建立一个模型$P(x)$，这个模型可以告诉我们发生某种行为的概率有多大。如果$P(x)\geq\lambda$，那么我们就可以认为x是正常的，反之则认为是anomaly。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703172257910.png" alt="image-20200703172257910" style="zoom:60%;"></p>
<p>如果现在对于一个玩家用二维向量来表示，一个维度表示“说垃圾话”的几率，另一个表示“无政府状态发言”的几率。对N个玩家的数据进行可视化，可以得到下图中的散点图。如果我们对其中“说垃圾话”维度的数据进行可视化，可以发现并不是所有的玩家都不说垃圾话，大部分都会说一点点；对于第二个维度，到底有多少人的发言是在无政府状态下进行的，可以发现多数人都是在无政府状态下发言的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703172908070.png" alt="image-20200703172908070" style="zoom:60%;"></p>
<p>对于$P(x)$比较大的玩家，我们就可以认为是normal的；对于$P(x)$小的玩家，我们就认为其是anomaly的。但这并不具体，我们应该用数值化的方法来计算每一个玩家的score。</p>
<h5 id="Maximum-Likelihood"><a href="#Maximum-Likelihood" class="headerlink" title="Maximum Likelihood"></a><strong>Maximum Likelihood</strong></h5><p>首先我们假设这些数据点都是从概率密度函数$f_{\theta} (x)$中取出来的，$\theta$决定了概率密度函数的形状，需要从训练数据中学习出来。现在我们已经有了数据点，但不知道其概率分布$f_{\theta} (x)$的形式。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703193052117.png" alt="image-20200703193052117" style="zoom:60%;"></p>
<p>这里引入了likelihood的概念，表示根据我现在有的概率密度函数$f_{\theta} (x^1),f_{\theta} (x^2),..,f_{\theta} (x^N)$，产生左图数据分布的概率有多大。</p>
<p>Q：那么我们怎么算图中的数据被产生出来的几率大小呢？</p>
<p>A：对于图中的所有数据点$\{x^1,x^2,..,x^N\}$，产生数据点$x^1$的概率是$f_{\theta} (x^1)$，产生数据点$x^2$的概率是$f_{\theta} (x^2)$，…，产生数据点$x^N$的概率是$f_{\theta} (x^N)$；那么对于产生所有数据点的概率，就可以用以下的式子表示，</p>
<script type="math/tex; mode=display">
L(\theta)=f_{\theta} (x^1)f_{\theta} (x^2)\cdot\cdot\cdot f_{\theta} (x^N)</script><p>也就是likelihood，和$\theta$是有关的，选择不同的$\theta$，就会有不同的概率密度函数，也就会有不同的likelihood，那么现在我们的任务就是找到$\theta^*$，使得likelihood可以取得最大值，即</p>
<script type="math/tex; mode=display">
\theta ^*=arg \ \mathop{\rm max}_\theta L(\theta)</script><h5 id="Gaussian-Distribution"><a href="#Gaussian-Distribution" class="headerlink" title="Gaussian Distribution"></a><strong>Gaussian Distribution</strong></h5><p>下图中的$f_{\mu,\sum}(x)$为高斯分布，输入为x，输出为x被sample的概率，是一个数值，$\mu$表示均值，$\sum$表示协方差矩阵。 </p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703195322546.png" alt="image-20200703195322546" style="zoom:60%;"></p>
<p>由于此时的概率密度函数发生了变化，我们现在将likelihood的形式进行变化，即</p>
<script type="math/tex; mode=display">
L(\mu,\sum)=f_{\mu,\sum} (x^1)f_{\mu,\sum} (x^2)\cdot\cdot\cdot f_{\mu,\sum} (x^N)</script><p>那么此时我们就需要找到$\mu^<em>,\sum^</em>$，来使likelihood最大化，</p>
<script type="math/tex; mode=display">
\mu^*,\sum=arg \ \mathop{\rm max}_{\mu,\sum} L(\mu,\sum)</script><p>在上图中，由于高斯分布的特性，数据点在$\mu$附近的地方很容易被sample到，越远被sample到的几率就越低；如果$\mu$在数据点很密集的地方，我们就可以认为likelihood的值很大；如果$\mu$在偏离高密度的地方，likelihood会小很多。</p>
<p>$\mu^*$就是输入数据点求均值，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703201627458.png" alt="image-20200703201627458" style="zoom:60%;"></p>
<p>找出了对应的$\mu^<em>,\sum^</em>$之后，我们就可以代入对应的概率密度函数，进行异常检测。如果$f_{\mu^<em>,\sum^</em>}(x)&gt;\lambda$，就认为是正常数据；如果$f_{\mu^<em>,\sum^</em>}(x)\leq\lambda$，则认为是异常数据（anomaly）。</p>
<p>如果把这个二维平面上所有的数据都输入这个概率分布，我们就可以得出下图；颜色越深，表示这越是一个一般的玩家，颜色越浅，表示异常行为越显著。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703201402084.png" alt="image-20200703201402084" style="zoom:60%;"></p>
<p>如果现在有三个玩家$x=(x_1,x_2,x_3,x_4,x_5)^T$，表现出了不同的行为，根据模型算出来不同的结果。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703202451744.png" alt="image-20200703202451744" style="zoom:60%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Anomaly Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Attack and Defense</title>
    <url>/2020/06/26/Attack/</url>
    <content><![CDATA[<h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626095005396.png" alt="image-20200626095005396" style="zoom:50%;"></p>
<h4 id="What-do-we-want-to-do"><a href="#What-do-we-want-to-do" class="headerlink" title="What do we want to do?"></a>What do we want to do?</h4><p>attack要做的事就是把找到原图片$x^0$对应的$x’$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626095352498.png" alt="image-20200626095352498" style="zoom:50%;"></p>
<h4 id="Loss-Function-for-Attack"><a href="#Loss-Function-for-Attack" class="headerlink" title="Loss Function for Attack"></a>Loss Function for Attack</h4><p><strong>Training</strong>：network训练得出的结果$y^0$必须和$y^{true}$越接近越好，此时的loss function为$L_{train}(\theta)=C(y^0,y^{true})$，<u>输入的$x$是固定的</u>，需要不断调整$\theta$的值，使得$L_{train}(\theta)$取得最小值；</p>
<p><strong>Non-targeted Attack</strong>：如果我们需要attack一个network，此时network的输出$y’$和$y^{true}$应该越大越好，此时的loss function为$L(x’)=-C(y’,y^{true})$，前面多了一个负号。此时的<u>network中的参数$\theta$是固定的</u>，需要不断调整输入$x’$，使network的输出$y’$和$y^{true}$的差距尽量远；</p>
<p><strong>Targeted Attack</strong>：如果我们不仅想要$y’,y^{true}$之间的距离尽量远，还想使$y’,y^{false}$之间的距离尽量近，就需要使用targeted attack；此时的loss function为$L(x’)=-C(y’,y^{true})+C(y’,y^{false})$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626095914128.png" alt="image-20200626095914128" style="zoom:50%;"></p>
<p>不仅需要限制输出之间的差异，还需要限制输入$x^0,x’$之间的差异，只有这输入之间的差异d小于$\epsilon$，我们才可以认为$x’$是与$x^0$相似的，才达到了attack 一个network的目的，即使输入尽可能具有迷惑性，从而使网络输出错误的结果</p>
<h4 id="Constraint"><a href="#Constraint" class="headerlink" title="Constraint"></a>Constraint</h4><p>那么我们怎么计算d呢？</p>
<p>有以下两种主要的方法：</p>
<ul>
<li>L2-norm，为$x^0,x’$之间每个像素差异的平方和；</li>
<li>L-infinity，为$x^0,x’$之间每个像素差异的最大值</li>
</ul>
<p>如果我们改变图中的每个pixel，另外一幅图只改变其中一个pixel，使得这两者之间的L2-norm是一样的，但第二种方式得出的L-infinity更大</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626102113579.png" alt="image-20200626102113579" style="zoom:50%;"></p>
<h4 id="How-to-Attack"><a href="#How-to-Attack" class="headerlink" title="How to Attack"></a>How to Attack</h4><p>就像我们训练一个neural network一样，但需要训练的参数是$x’$，此时就需要找到一个参数$x^*$，来最小化$L(x’)$，限制条件是$d(x^0,x’)\leq\epsilon$</p>
<script type="math/tex; mode=display">
x^*=arg\mathop{min}_\limits {d(x^0,x')\leq\epsilon}L(x')</script><p>这里我们也使用了gradient descent算法，只是此时需要调整的参数变成了$x^t$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626103758330.png" alt="image-20200626103758330" style="zoom:50%;"></p>
<p>当$d(x^0,x^t)&gt;\epsilon$时，就需要更新这个参数了，使用$fix(x^t)$来更新</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626123032000.png" alt="image-20200626123032000" style="zoom:50%;"></p>
<p>更新的参数需要满足一定的条件，</p>
<ul>
<li>如果使用L2-norm，必须选择在差值在半径以内的参数，超过了这个半径，就设为$\epsilon $；</li>
<li>如果使用L-infinity，现在超过了这个方形的区域，就必须想办法把它拉回来，在y轴方向超过了$\epsilon$，就把值设为$\epsilon$；在x轴方向超过了$\epsilon$，就把值设为$\epsilon$</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626123713569.png" alt="image-20200626123713569" style="zoom:50%;"></p>
<h4 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h4><p>如果我们现在要attack一个network，其真实类别为Tiger cat，但attack之后的network认为这是star fish</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626161331898.png" alt="image-20200626161331898" style="zoom:50%;"></p>
<p>由于这两者之间的差异很小，很难识别，这里我们将差值x50来进行展示</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626161942541.png" alt="image-20200626161942541" style="zoom:50%;"></p>
<p>可能猫和猫之间是比较类似的，这里我们将attack的target设置为keyboard，该network认为这是keyboard的概率为0.98</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626162037439.png" alt="image-20200626162037439" style="zoom:50%;"></p>
<p>如果我们再对图片加入噪声，network认为这是Persian cat，再继续加入噪声，我们都快分辨不出这张图是一只猫了，network就认为这是fire screen</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626162150972.png" alt="image-20200626162150972" style="zoom:50%;"></p>
<p>我们假设$x_0$是在高维平面上的一个点，沿着任意方向随机移动，我们可以看到在接近$x^0$ 的时候，是 tiger cat（正确分类）的可能性是很高的，但如果再移动多一点，是Persian cat和Egyptian cat的可能性是很高的 </p>
<p>上述说的是随机方向进行移动，如果图片是225*225pixel的，那么就是5万多个高维的方向，现在我们选取其中几个特定的方向。在这几个特定的方向中，$x^0$可变化的范围就变得非常狭窄，$x^0$稍微变化一下，network输出为另一个类别（key board）的可能性就很高</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626163736373.png" alt="image-20200626163736373" style="zoom:50%;"></p>
<h4 id="Attack-Approaches"><a href="#Attack-Approaches" class="headerlink" title="Attack Approaches"></a>Attack Approaches</h4><ul>
<li><p>FGSM (<a href="https://arxiv.org/abs/1412.6572" target="_blank" rel="noopener">https://arxiv.org/abs/1412.6572</a>)</p>
</li>
<li><p>Basic iterative method (<a href="https://arxiv.org/abs/1607.02533" target="_blank" rel="noopener">https://arxiv.org/abs/1607.02533</a>) </p>
</li>
<li><p>L-BFGS (<a href="https://arxiv.org/abs/1312.6199" target="_blank" rel="noopener">https://arxiv.org/abs/1312.6199</a>)</p>
</li>
<li>Deepfool (<a href="https://arxiv.org/abs/1511.04599" target="_blank" rel="noopener">https://arxiv.org/abs/1511.04599</a>)</li>
<li>JSMA (<a href="https://arxiv.org/abs/1511.07528" target="_blank" rel="noopener">https://arxiv.org/abs/1511.07528</a>)</li>
<li>C&amp;W (<a href="https://arxiv.org/abs/1608.04644" target="_blank" rel="noopener">https://arxiv.org/abs/1608.04644</a>)</li>
<li>Elastic net attack (<a href="https://arxiv.org/abs/1709.04114" target="_blank" rel="noopener">https://arxiv.org/abs/1709.04114</a>)</li>
<li>Spatially Transformed (<a href="https://arxiv.org/abs/1801.02612" target="_blank" rel="noopener">https://arxiv.org/abs/1801.02612</a>) </li>
<li>One Pixel Attack (<a href="https://arxiv.org/abs/1710.08864" target="_blank" rel="noopener">https://arxiv.org/abs/1710.08864</a>)</li>
<li>…… only list a few</li>
</ul>
<p>虽然有很多方法都可以用来attack network，但这些方法的主要区别在于使用了不同的constrains，或者使用了不同的optimization methods</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626170309446.png" alt="image-20200626170309446" style="zoom:50%;"></p>
<p>我们先介绍一下<strong>FGSM</strong>，对于第一维，如果$\frac{\partial L}{\partial x_1}$是大于0的值，那么$\Delta x_1=sign(\frac{\partial L}{\partial x_1})=+1$；如果对于第二维，如果$\frac{\partial L}{\partial x_2}$是小于0的值，不管值多大，都得出$\Delta x_2=sign(\frac{\partial L}{\partial x_2})=-1$；即对于$x^0$的所有维，要么$+\epsilon$，要么$-\epsilon$，即可得到最好的结果$x^*$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626171523647.png" alt="image-20200626171523647" style="zoom:50%;"></p>
<p>FGSM使用L-infinity作为distance constrain，如果gradient指向左下角，那么$x^<em>$就在方框的右上角；如果gradient指向左上角，那么$x^</em>$就在方框的右下角；因此，在FGSM里面，我们只在意gradient的方向，不在意其具体的大小 </p>
<p>那么FGSM到底是怎么运作的呢？</p>
<p>我们可以看作FGSM是使用了非常大的一个learning rate，使x飞出了方形区域，由于L-infinity的限制，输出会被限制到方形区域内部，即$x^*$在方形区域的右上角</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626174020656.png" alt="image-20200626174020656" style="zoom:60%;"></p>
<h4 id="White-Box-v-s-Black-Box"><a href="#White-Box-v-s-Black-Box" class="headerlink" title="White Box v.s. Black Box"></a>White Box v.s. Black Box</h4><p>在之前的attack中，我们假设已经知道了network的参数$\theta$，目标是找到最优化的$x’$，这种attack就称为<strong>White Box</strong></p>
<p>但在大多数的情况中，我们都不知道network的参数，但也需要去attack这个network，这就是<strong>Black Box</strong></p>
<h4 id="Black-Box-Attack"><a href="#Black-Box-Attack" class="headerlink" title="Black Box Attack"></a>Black Box Attack</h4><p>如果我们现在已经知道了black network的training data，那么我们就可以用同样的training data来训练一个proxy network，再生成attacked object，如果能过成功attack新的proxy network，那么我们就可以把这个object也作为black network的输入，也可以attack成功</p>
<p>如果不能得到相应的training data，network如果是一个在线版本，我们可以输入大量的图片，得出相对应的分类结果，从而可以组合成相对应的训练资料，来得出proxy network</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626174930630.png" alt="image-20200626174930630" style="zoom:50%;"></p>
<p>也有一些实验数据证明黑箱攻击是有可能成功的，现在假设Black Box有五种，现在我们来训练proxy network，我们用ResNet-152生成的图片，如果black box的network也是ResNet-152，那么attack成功的几率就会非常高，表格中的4%表示系统辨识的准确率</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626175459885.png" alt="image-20200626175459885" style="zoom:50%;"></p>
<h4 id="Attack-in-the-Real-World"><a href="#Attack-in-the-Real-World" class="headerlink" title="Attack in the Real World"></a>Attack in the Real World</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626183115909.png" alt="image-20200626183115909" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626183139180.png" alt="image-20200626183139180" style="zoom:50%;"></p>
<h4 id="Defense"><a href="#Defense" class="headerlink" title="Defense"></a>Defense</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626183341651.png" alt="image-20200626183341651" style="zoom:50%;"></p>
<h5 id="Passive-defense"><a href="#Passive-defense" class="headerlink" title="Passive defense"></a>Passive defense</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626183453090.png" alt="image-20200626183453090" style="zoom:50%;"></p>
<p>这个filter可以是smoothing，对于原来输入的attack图片network认为其是keyboard，经过smoothing之后，network就认为是tiger cat了，是正确的分类结果</p>
<p>Q：那么为什么smoothing可以达到这种效果呢？</p>
<p>A：只有某几种方向上的的信号可以使attack成功。如果使用了smoothing这种filter，就把这几种信号改变了，那么attack就失效了 ；加上smoothing并不会伤害原来的图片，所以network仍然可以得出正确的结果</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626183523115.png" alt="image-20200626183523115" style="zoom:50%;"></p>
<p>根据这种思想，有学者就提出了feature squeeze</p>
<p>对于同一个input，我们先得出model的输出结果$Prediction_0$，再根据$Squeeze_1，Squeeze_2$得出结果$Prediction_1,Prediction_2$，如果$Prediction_0$和$Prediction_1,Prediction_2$之间的差值d很大，那么我们就可以认为input是来attack的</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626184313560.png" alt="image-20200626184313560" style="zoom:50%;"></p>
<p>还有另外一种方法</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626184847976.png" alt="image-20200626184847976" style="zoom:50%;"></p>
<h5 id="Proactive-defense"><a href="#Proactive-defense" class="headerlink" title="Proactive defense"></a>Proactive defense</h5><p>首先我们通过某种算法找出漏洞，找到相应的adversarial input，再把这些input和之前的training data一起作为新的input data，输入network，相当于进行了data augmentation，这个过程进行T次，每次的input data都是不一样的</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626185155952.png" alt="image-20200626185155952" style="zoom:50%;"></p>
<p>如果attacker知道了我们是使用算法A来进行模拟，那么attacker可以使用算法B来进行attack，那么我们的network并不能抵御这种attack</p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Attack and Defense</tag>
      </tags>
  </entry>
  <entry>
    <title>Backpropagation</title>
    <url>/2020/06/06/Backpropagation/</url>
    <content><![CDATA[<h4 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h4><p>这里我们再回顾一下gradient descent，对于网络中的参数$w_1,w_2,…,b_1,b_2,…$，通过求出相对应的梯度$\Delta L(\theta)$，再根据梯度更新网络结构中的参数</p>
<script type="math/tex; mode=display">
\theta ^i=\theta^{i-1}-\eta\Delta L(\theta^{i-1})</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606200029374.png" alt="image-20200606200029374" style="zoom:50%;"></p>
<h4 id="Chain-Rule"><a href="#Chain-Rule" class="headerlink" title="Chain Rule"></a>Chain Rule</h4><p>backpropagation的核心思想就是链式法则</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606200855314.png" alt="image-20200606200855314" style="zoom:50%;"></p>
<h4 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h4><p>这里我们先定义了一个loss函数，$l^n(\theta)$表示training data中$y^n$和$\hat y ^n$之间的loss，这个loss可以通过cross entropy或者MSE计算，再将所有的loss进行求和，得到$L(\theta)$</p>
<script type="math/tex; mode=display">
L(\theta)=\sum_{n=1}^Nl^n(\theta)</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606200827557.png" alt="image-20200606200827557" style="zoom:50%;"></p>
<p>对w求导，可得$\frac{\partial l^n}{\partial w}=\frac{\partial z}{\partial w}\frac{\partial l}{\partial z}$，这里我们截取了网络中的部分结构，神经网络的forward过程计算$\frac{\partial z}{\partial w}$，backward过程计算$\frac{\partial l}{\partial z}$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606201320644.png" alt="image-20200606201320644" style="zoom:50%;"></p>
<h4 id="Forward-pass"><a href="#Forward-pass" class="headerlink" title="Forward pass"></a>Forward pass</h4><p>forward过程计算$\frac{\partial z}{\partial w}$，即为权重所对应的上一层神经元的值(x1,x2)</p>
<script type="math/tex; mode=display">
z=x_1w_1+x_2+w_2+b \\
\frac{\partial z}{\partial w_1}=x_1,\quad\frac{\partial z}{\partial w_2}=x_2</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606201854092.png" alt="image-20200606201854092" style="zoom:50%;"></p>
<p>对于forward过程，计算出上层神经元的值后，才可以继续计算下一层神经元的梯度$\frac{\partial z}{\partial w}$，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606202207055.png" alt="image-20200606202207055" style="zoom:50%;"></p>
<h4 id="Backward-pass"><a href="#Backward-pass" class="headerlink" title="Backward pass"></a>Backward pass</h4><h5 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h5><p>backward过程计算$\frac{\partial l}{\partial z}$，z为激活函数$\sigma (z)$的输入值。这里我们令$a=\sigma(z)$，简化表达式的形式，根据chain rule</p>
<script type="math/tex; mode=display">
\frac{\partial l}{\partial z}=\frac{\partial a}{\partial z}\frac{\partial l}{\partial a}</script><p>其中$\frac{\partial a}{\partial z}=\sigma’(z)$，对激活函数求一阶导数，可以很轻松地表示出来</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606203500895.png" alt="image-20200606203500895" style="zoom:50%;"></p>
<p>接下来我们开始$\frac{\partial l}{\partial a}$的计算，a会影响$z’,z’’$，因此可以再次使用链式法则</p>
<script type="math/tex; mode=display">
\frac{\partial l}{\partial a}=\frac{\partial z'}{\partial a}\frac{\partial l}{\partial z'}
+ \frac{\partial z''}{\partial a}\frac{\partial l}{\partial z''}</script><p>由于$z’=aw_3+…,\quad z’’=aw_4+…$，那么我们可以得出</p>
<script type="math/tex; mode=display">
\frac{\partial z'}{\partial a}=w_3,\quad\frac{\partial z''}{\partial a}=w_4</script><p>代入原式，可得</p>
<script type="math/tex; mode=display">
\frac{\partial l}{\partial z}=\frac{\partial a}{\partial z}\frac{\partial l}{\partial a} =\sigma'(z)\left[w_3\frac{\partial l}{\partial z'}+ w_4\frac{\partial l}{\partial z''}\right]</script><p>对于$\frac{\partial l}{\partial z’},\frac{\partial l}{\partial z’’}$，我们假设可以通过某种方式求得他们的值</p>
<h5 id="另一个观点"><a href="#另一个观点" class="headerlink" title="另一个观点"></a>另一个观点</h5><p>我们可以从这个图中更加直观地了解backpropagation的过程，这里我们假设有一个神经元（图中的三角形），它不在原来的网络结构中，可以通过$w_3\frac{\partial l}{\partial z’}+ w_4\frac{\partial l}{\partial z’’}$来计算，<u>前面再乘上一个放大系数$\sigma’(z)$，这个放大系数的值是根据forward过程计算的，是一个常数</u>，就可以得出$\frac{\partial l}{\partial z}$的值</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606204707606.png" alt="image-20200606204707606" style="zoom:50%;"></p>
<h5 id="两种情况"><a href="#两种情况" class="headerlink" title="两种情况"></a>两种情况</h5><p>公式内的其他项都已经计算出来，还有$\frac{\partial l}{\partial z’},\frac{\partial l}{\partial z’’}$没有得出具体的表达式，此步骤是为了求解$\frac{\partial l}{\partial z’},\frac{\partial l}{\partial z’’}$的表达式</p>
<p><strong>Case1：Output Layer</strong></p>
<p>为了求出$\frac{\partial l}{\partial z’},\frac{\partial l}{\partial z’’}$，这里再次使用了chain rule</p>
<script type="math/tex; mode=display">
\frac{\partial l}{\partial z'}=\frac{\partial y_1}{\partial z'}\frac{\partial l}{\partial y_1}</script><p>其中$\frac{\partial y_1}{\partial z’}$就是激活函数的输出值再对$z’$求导；</p>
<p>$\frac{\partial l}{\partial y_1}$为相对应的loss函数对y1求导，这个loss函数可以是cross entropy，也可以是MSE，对于不同的loss函数，计算出来的导数也不同</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606213905635.png" alt="image-20200606213905635" style="zoom:50%;"></p>
<p><strong>Case2: Not output layer</strong></p>
<p>如果现在假设后层不是output layer，而是hidden layer中的其中一层，计算方式就发生了一些小小的变化</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606215916597.png" alt="image-20200606215916597" style="zoom:50%;"></p>
<p>根据Case1的推导，知道了$\frac{\partial l}{\partial z’},\frac{\partial l}{\partial z’’}$之后，就可以对$\frac{\partial l}{\partial z’}$进行求解</p>
<p>同理可得，知道了$\frac{\partial l}{\partial z_a},\frac{\partial l}{\partial z_b}$，就可以对$\frac{\partial l}{\partial z’}$求解，如下图所示，$\frac{\partial l}{\partial z_a},\frac{\partial l}{\partial z_b}$分别乘上对应的权重$w_5,w_6$，前面再乘一个放大系数，就可得出$\frac{\partial l}{\partial z’}$的表达式</p>
<script type="math/tex; mode=display">
\frac{\partial l}{\partial z'}=\sigma'(z')\left[w_5\frac{\partial l}{\partial z_a}+ w_6\frac{\partial l}{\partial z_b}\right]</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606221608597.png" alt="image-20200606221608597" style="zoom:50%;"></p>
<p>知道了$z’,z’’$的值后，就可以根据公式计算z的值；知道了$z_a,z_b$之后，就可以计算$z’$ 的值；……一直循环这个步骤，直到到达output layer为止</p>
<p>下图中有6个neural，分别是$z_1,z_2,z_3,z_4,z_5,z_6$，为激活函数的输入值，现在要分别求得$l$对这些函数的偏微分$\frac{\partial l}{\partial z_i}$。按照我们之前的做法，要求$\frac{\partial l}{\partial z_1}$，就必须要求$\frac{\partial l}{\partial z_3},\frac{\partial l}{\partial z_4}$，而要求$\frac{\partial l}{\partial z_3}，\frac{\partial l}{\partial z_4}$的值，就必须要分别求两次$\frac{\partial l}{\partial z_5},\frac{\partial l}{\partial z_6}$的值，计算效率很低</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606222718315.png" alt="image-20200606222718315" style="zoom:50%;"></p>
<p>如果我们先计算$\frac{\partial l}{\partial z_5},\frac{\partial l}{\partial z_6}$，就可以接着计算出$\frac{\partial l}{\partial z_3},\frac{\partial l}{\partial z_4}$的值，再计算出$\frac{\partial l}{\partial z_1},\frac{\partial l}{\partial z_2}$的值，计算效率可以提高很多</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606222732356.png" alt="image-20200606222732356" style="zoom:50%;"></p>
<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>神经网络的forward过程计算$\frac{\partial z}{\partial w}$，backward过程计算$\frac{\partial l}{\partial z}$，再相乘，就可以得出loss函数对每个hidden layer神经元的梯度，</p>
<script type="math/tex; mode=display">
\frac{\partial l}{\partial w}=\frac{\partial z}{\partial w}\frac{\partial l}{\partial z}</script><p>代入每次参数更新的公式，就可以得出每次的参数更新结果</p>
<script type="math/tex; mode=display">
w ^i=w^{i-1}-\eta\Delta L(w^{i-1})</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606224217277.png" alt="image-20200606224217277" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>backpropagation</tag>
      </tags>
  </entry>
  <entry>
    <title>Bert模型精讲</title>
    <url>/2020/07/13/Bert/</url>
    <content><![CDATA[<h4 id="Autoregressive-LM-与-Autoencoder-LM"><a href="#Autoregressive-LM-与-Autoencoder-LM" class="headerlink" title="Autoregressive LM 与 Autoencoder LM"></a>Autoregressive LM 与 Autoencoder LM</h4><p>Autoregressive语言模型：根据上文内容预测下一个可能跟随的单词，或者根据下文来预测前文的单词，但并不能同时根据上下文来预测单词。GPT是典型的自回归语言模型。ELMO虽然有两个方向，一个是自左向右的自回归语言模型，另一个是自右向左的自回归语言模型，由两个自回归语言模型组成，本质上还是自回归语言模型。</p>
<ul>
<li>缺点：只能根据上文或下文来预测单词，并不能同时依据上下文；</li>
<li>优点：对生成类NLP任务有很大优势，比如机器翻译，就是从左向右的，并不需要同时根据上下文的内容进行判断。</li>
</ul>
<p>Autoencoder语言模型：在输入中随机Mask一些单词，使模型可以同时根据上下文的内容来预测这些被mask的单词。</p>
<ul>
<li>缺点：由于引入了Mask标记，而fine-tuning阶段并不能看到Mask标记，因此会造成预训练和fine-tune阶段不一致的问题；</li>
<li>优点：可以很好地融入双向语言模型。</li>
</ul>
<h4 id="DAE与Masked-Language-Model"><a href="#DAE与Masked-Language-Model" class="headerlink" title="DAE与Masked Language Model"></a>DAE与Masked Language Model</h4><h5 id="AutoEncoder"><a href="#AutoEncoder" class="headerlink" title="AutoEncoder"></a>AutoEncoder</h5><p>对于输入大小为28×28的图像，先经过encoder进行编码，得到的结果通常比784维要小，code表示比原来的image更加精简的特征；但现在我们进行的是unsupervised learning，这个code到底长什么样子也不知道；那么我们现在就先来做一个decoder，把code恢复成一张image；</p>
<p>decoder和encoder单独是没办法训练的，因为是无监督学习，因此现在我们将这两者一起学习，学习的目标是使input和output之间的差距越小越好。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200712121526873.png" alt="image-20200712121526873" style="zoom:80%;"></p>
<h5 id="Denoising-AutoEncoder（DAE）"><a href="#Denoising-AutoEncoder（DAE）" class="headerlink" title="Denoising AutoEncoder（DAE）"></a>Denoising AutoEncoder（DAE）</h5><p>为了缓解原来的autoencoder很容易<font color="red">过拟合</font>的问题，我们可以在输入中加入一些<font color="red">随机噪声</font>。对于原来的input x，我们先加入一些noise得到$x’$(破损数据)，进行encode之后再进行decode，使之和最早的input之间的差值最小；encoder现在不仅学习到了encode这件事，还学习到了把noise过滤掉这件事。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200712121915387.png" alt="image-20200712121915387"></p>
<p><strong>DAE模型的优势：</strong></p>
<ul>
<li><p>可以减小训练数据和测试数据之间的gap；</p>
</li>
<li><p>和非破损数据的训练相比，由于DAR在训练的时候不小心把输入的噪声也擦除了，因此破损数据训练出来的weight噪声比较小。</p>
</li>
</ul>
<p>在下图中，左图为没有使用任何噪声的resulted filters，右图为使用了30%噪声的resulted filter，可以发现使用了噪声的autoencoder，其训练好的weight噪声要低很多。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/271419017086934.png" alt="271419017086934" style="zoom:50%;"></p>
<h5 id="DAE-与-Masked-Language-Model"><a href="#DAE-与-Masked-Language-Model" class="headerlink" title="DAE 与 Masked Language Model"></a>DAE 与 Masked Language Model</h5><ul>
<li>BERT是基于<strong>Transformer Encoder</strong>来构建的一种模型；</li>
<li>BERT是基于<strong>DAE</strong>（Denoising Autoencoder，去噪自动编码器）的，这部分在BERT中被称为<strong>Masked Language Model</strong>；</li>
<li>Masked Language Model并不是严格意义上的语言模型，这仅仅是训练语言模型的一种方式。BERT在输入中随机Mask一些单词，使模型可以同时根据上下文的内容来预测这些被mask的单词，这个过程其实就是DAE的过程（Mask标签就相当于加入的噪声，DAE要去掉这些噪声，并还原出原来的图像）。</li>
</ul>
<p>DAE的代码实现为：<a href="http://deeplearning.net/tutorial/dA.html" target="_blank" rel="noopener">http://deeplearning.net/tutorial/dA.html</a></p>
<h4 id="Transformer模型回顾"><a href="#Transformer模型回顾" class="headerlink" title="Transformer模型回顾"></a>Transformer模型回顾</h4><p>详见<a href="https://scarleatt.github.io/2020/07/13/Transformer/" target="_blank" rel="noopener">https://scarleatt.github.io/2020/07/13/Transformer/</a></p>
<h5 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h5><p>transform模型由多个encoder和decoder构成，如下图所示</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200712185243525.png" alt="image-20200712185243525" style="zoom: 80%;"></p>
<p><strong>Encoder</strong>由N=6个相同的layer组成，layer就是上图中左边的方框部分，左边的英文字母“Nx”表示layer的数量，这里N=6。每个layer都有两个sub-layer，即multi-head attention和position-wise feed-forward network。每个sub-layer还加入了residual connection和layer normalization，因此每个sub-layer的输出为</p>
<script type="math/tex; mode=display">
\rm sub\_layer_{output}=LayerNorm(x+Sublayer(x))</script><p><strong>Decoder</strong>也由6个相同的layer组成，layer就是上图的右边的方框部分，此时N=6。layer由3个sub-layer组成，除了encoder提到的两个sub-layer，还有Masked Multi-Head Attention。</p>
<p>新加入的masked multi-head attention，在进行softmax之前对未来的position进行mask（设为-inf），可以保证对位置i的预测只依赖位置小于i的已知输出，不会接触到未来位置的信息</p>
<ul>
<li>输出：对应i位置的输出词的概率分布，如果词库中有一万个单词，输出的vector就是一万维；</li>
<li>输入：encoder的输出 &amp; 对应i-1位置decoder的输出。中间的attention（masked）并不是self-attention，K，V来自encoder，Q来自上一位置decoder的输出；</li>
<li>解码：编码可以并行计算，一次性全部encoding出来。但解码不是一次把所有的序列解出来，由于要用到上一个位置encoder的出书作为这次的query，因此解码是像rnn一样一个一个解出来的。</li>
</ul>
<h4 id="BERT模型"><a href="#BERT模型" class="headerlink" title="BERT模型"></a>BERT模型</h4><h5 id="模型结构-1"><a href="#模型结构-1" class="headerlink" title="模型结构"></a>模型结构</h5><p>Bert模型全称是BidirectionalEncoder Representations from Transformer，是基于Transformer中的encoder的双向结构。模型的主要创新点在pre-training上，包括Mask Language Model、Next Sentence Prediction，来分别捕捉词语和句子级别的representation。</p>
<p>Bert的模型结构示意图如下图左1所示，是双向的，而GPT则是单向的；</p>
<p>ELMo虽然也是双向的，但这两者的目标函数是不同的，ELMo的目标函数是两个$P(w_i|w_1,..,w_{i-1}),P(w_i|w_{i+1},…,w_n)$，是独立训练出两个不同的representation然后拼接；但Bert的目标函数是$P(w_i|w_1,..,w_n)$，包括两个方向的单词。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200713224006961.png" alt></p>
<h5 id="Pre-training"><a href="#Pre-training" class="headerlink" title="Pre-training"></a>Pre-training</h5><p><strong>Task #1: Mask LM</strong>   对于输入的一段文字，随机mask这句话中的几个词，要求根据剩余词汇来确定被mask的词。我们把要mask的长度定为15%，在被mask的文字中，80%采用[Mask]标记替换，10%采用任意词替换，剩余10%保持原词汇不变。</p>
<p><strong>Task #2: Next Sentence Prediction</strong>   由于很多重要的下游任务，比如Question Answering (QA) 和 Natural Language Inference (NLI)，都需要理解两个句子之间的关系，因此增加了NSP这个预训练任务，使得模型可以理解两个句子之间的关系。在一篇文章里，我们选择了A和B两个句子，B有50%的几率是A的下一句（isNext），输入这两个句子，BERT模型来判断B是不是A的下一句。</p>
<p>由于attention的计算开销是输入序列长度的平方，如果输入序列太长会影响整个模型的训练速度，因此90%的序列都选用seq_len=128来进行训练，剩下的10%都选用512的序列长度。</p>
<h5 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine-tuning"></a>Fine-tuning</h5><p>从头开始训练Bert模型的任务是非常艰巨的，google公布了在多种语言上预训练好的模型，我们可以在此基础上进行fine-tuning，比从头开始训练所需的计算量要小得多。对于特定的任务，Bert模型只需要添加一个输出层来进行微调即可。这些任务可以是问题解答、语言推理等。</p>
<p>官网最开始提出的模型是文本分类，如果要用于其他任务，则需要进行一些改进，如图所示，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200713221407425.png" alt="image-20200713221407425"></p>
<p>可以调整的参数和取值范围有：</p>
<ul>
<li>Batch size: 16, 32</li>
<li>Learning rate (Adam): 5e-5, 3e-5, 2e-5</li>
<li>Number of epochs: 3, 4</li>
</ul>
<p>Q：为什么说Bert是双向的 ？</p>
<p>A：由于输入的序列有部分是被mask了，由于Transformer的特性，模型会注意上下文所有的单词，即两个方向的单词，这就实现了双向表示，说明Bert是一个双向语言模型。</p>
<h4 id="BERT在实际场景中的应用"><a href="#BERT在实际场景中的应用" class="headerlink" title="BERT在实际场景中的应用"></a>BERT在实际场景中的应用</h4><p>可以应用到：</p>
<ul>
<li>情感分类：质量问题分析；</li>
<li>意图识别；</li>
<li>问答匹配；</li>
<li>命名实体识别。</li>
</ul>
<p>bert在实际场景中的使用：</p>
<ul>
<li>筛选训练数据，提出过短或过长的数据；</li>
<li>尝试bert+conv，bert+conv+avg_max_pooling，bert_last_layer_concat等方式；</li>
<li>针对本场景数据，进行少步数的进一步预训练。</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li>引入了Masked LM，使用双向LM做模型预训练。</li>
<li>为预训练引入了新目标NSP，它可以学习两个句子之间的关系。</li>
<li>为下游任务引入了很通用的求解框架，不再为任务做模型定制。</li>
<li>刷新了多项NLP任务的记录，引爆了NLP无监督预训练技术。</li>
</ul>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><ol>
<li>帮助理解Transform模型中的attention机制，<a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">https://jalammar.github.io/illustrated-transformer/</a></li>
<li><p>Bert模型的理解，<a href="https://zhuanlan.zhihu.com/p/46652512" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46652512</a></p>
</li>
<li><p><a href="https://www.cnblogs.com/Milburn/p/12031501.html" target="_blank" rel="noopener">BERT模型图解</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1M5411x7FZ?from=search&amp;seid=15336884376384843690" target="_blank" rel="noopener">Bert模型精讲</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>Knowledge Graph</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>AutoEncoder</tag>
        <tag>Denoising AutoEncoder</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>CCF-JSON查询</title>
    <url>/2017/12/02/CCF-JSON%E6%9F%A5%E8%AF%A2/</url>
    <content><![CDATA[<p>试题编号：   201709-3<br>试题名称：   JSON查询<br>时间限制：   1.0s<br>内存限制：   256.0MB<br>问题描述：<br>　　JSON (JavaScript Object Notation) 是一种轻量级的数据交换格式，可以用来描述半结构化的数据。JSON 格式中的基本单元是值 (value)，出于简化的目的本题只涉及 2 种类型的值：<br>　　&#42; 字符串 (string)：字符串是由双引号 “ 括起来的一组字符（可以为空）。如果字符串的内容中出现双引号 “，在双引号前面加反斜杠，也就是用 \” 表示；如果出现反斜杠 \，则用两个反斜杠 \\ 表示。反斜杠后面不能出现 “ 和 \ 以外的字符。例如：””、”hello”、”\”\\”。<br>　　&#42; 对象 (object)：对象是一组键值对的无序集合（可以为空）。键值对表示对象的属性，键是属性名，值是属性的内容。对象以左花括号 { 开始，右花括号 } 结束，键值对之间以逗号 , 分隔。一个键值对的键和值之间以冒号 : 分隔。键必须是字符串，同一个对象所有键值对的键必须两两都不相同；值可以是字符串，也可以是另一个对象。例如：{}、{“foo”: “bar”}、{“Mon”: “weekday”, “Tue”: “weekday”, “Sun”: “weekend”}。<br>　　除了字符串内部的位置，其他位置都可以插入一个或多个空格使得 JSON 的呈现更加美观，也可以在一些地方换行，不会影响所表示的数据内容。例如，上面举例的最后一个 JSON 数据也可以写成如下形式。<br>　　{<br>　　“Mon”: “weekday”,<br>　　“Tue”: “weekday”,<br>　　“Sun”: “weekend”<br>　　}<br>　　给出一个 JSON 格式描述的数据，以及若干查询，编程返回这些查询的结果。<br>输入格式<br>　　第一行是两个正整数 n 和 m，分别表示 JSON 数据的行数和查询的个数。<br>　　接下来 n 行，描述一个 JSON 数据，保证输入是一个合法的 JSON 对象。<br>　　接下来 m 行，每行描述一个查询。给出要查询的属性名，要求返回对应属性的内容。需要支持多层查询，各层的属性名之间用小数点 . 连接。保证查询的格式都是合法的。<br>输出格式<br>　　对于输入的每一个查询，按顺序输出查询结果，每个结果占一行。<br>　　如果查询结果是一个字符串，则输出 STRING <string>，其中 <string> 是字符串的值，中间用一个空格分隔。<br>　　如果查询结果是一个对象，则输出 OBJECT，不需要输出对象的内容。<br>　　如果查询结果不存在，则输出 NOTEXIST。</string></string></p>
<p>样例输入<br>10 5<br>{<br>“firstName”: “John”,<br>“lastName”: “Smith”,<br>“address”: {<br>“streetAddress”: “2ndStreet”,<br>“city”: “NewYork”,<br>“state”: “NY”<br>},<br>“esc\\aped”: “\”hello\””<br>}<br>firstName<br>address<br>address.city<br>address.postal<br>esc\aped<br>样例输出<br>STRING John<br>OBJECT<br>STRING NewYork<br>NOTEXIST<br>STRING “hello”<br>评测用例规模与约定<br>　　n ≤ 100，每行不超过 80 个字符。<br>　　m ≤ 100，每个查询的长度不超过 80 个字符。<br>　　字符串中的字符均为 ASCII 码 33-126 的可打印字符，不会出现空格。所有字符串都不是空串。<br>　　所有作为键的字符串不会包含小数点 .。查询时键的大小写敏感。<br>　　50%的评测用例输入的对象只有 1 层结构，80%的评测用例输入的对象结构层数不超过 2 层。举例来说，{“a”: “b”} 是一层结构的对象，{“a”: {“b”: “c”}} 是二层结构的对象，以此类推。<br>  <strong>思路</strong>：加一些标识位，用vector数据类型来存储键值</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">string</span> json = <span class="string">""</span>;</span><br><span class="line">    <span class="keyword">int</span> n, m;</span><br><span class="line">    <span class="built_in">cin</span>&gt;&gt;n&gt;&gt;m;</span><br><span class="line">    getchar();</span><br><span class="line">    <span class="keyword">while</span> (n--) &#123;</span><br><span class="line">        <span class="built_in">string</span> temp;</span><br><span class="line">        getline(<span class="built_in">cin</span>, temp);</span><br><span class="line">        json += temp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt; all;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; pre;</span><br><span class="line">    <span class="built_in">string</span> preHost;</span><br><span class="line">    <span class="keyword">int</span> iskey = <span class="literal">true</span>, isvalue = <span class="literal">false</span>, isstring = <span class="literal">false</span>, blank = <span class="literal">false</span>;</span><br><span class="line">    <span class="built_in">string</span> key, value;</span><br><span class="line">    <span class="built_in">string</span> now_key;</span><br><span class="line">    <span class="built_in">string</span> temp_key;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; json.length(); i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (json[i] == <span class="string">' '</span>) blank = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">else</span> blank = <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">if</span> (json[i]==<span class="string">':'</span>) &#123;</span><br><span class="line">            isvalue = <span class="literal">true</span>;</span><br><span class="line">            iskey = <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (isvalue &amp;&amp; json[i]==<span class="string">'&#123;'</span>) &#123;</span><br><span class="line">            isstring = <span class="literal">false</span>;</span><br><span class="line">            pre.push_back(key);</span><br><span class="line">            temp_key = <span class="string">""</span>;</span><br><span class="line">            <span class="keyword">if</span> (pre.size()&gt;<span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; pre.size()<span class="number">-1</span>; j++) &#123;</span><br><span class="line">                    temp_key += pre[j];</span><br><span class="line">                    temp_key += <span class="string">"."</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                temp_key += pre[pre.size()<span class="number">-1</span>];</span><br><span class="line">            &#125;</span><br><span class="line">            all[temp_key] = <span class="string">"&#123;"</span>;</span><br><span class="line">            preHost += key;</span><br><span class="line">            preHost += <span class="string">"."</span>;</span><br><span class="line">            key = <span class="string">""</span>;</span><br><span class="line">            iskey = <span class="literal">true</span>;</span><br><span class="line">            isvalue = <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (json[i]==<span class="string">'&#125;'</span>) &#123;</span><br><span class="line">            isvalue = <span class="literal">false</span>;</span><br><span class="line">            iskey = <span class="literal">true</span>;</span><br><span class="line">            i++;</span><br><span class="line">            temp_key = <span class="string">""</span>;</span><br><span class="line">            <span class="keyword">if</span> (pre.size()&gt;<span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; pre.size(); j++) &#123;</span><br><span class="line">                    temp_key += pre[j];</span><br><span class="line">                    temp_key += <span class="string">"."</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            temp_key+=key;</span><br><span class="line">            all[temp_key] = value;</span><br><span class="line"></span><br><span class="line">            preHost = <span class="string">""</span>;</span><br><span class="line">            key = <span class="string">""</span>; value = <span class="string">""</span>;</span><br><span class="line">            <span class="keyword">if</span> (pre.size()&gt;<span class="number">0</span>)</span><br><span class="line">                pre.pop_back();</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (isvalue &amp;&amp; json[i]==<span class="string">','</span>) &#123;</span><br><span class="line">            isvalue = <span class="literal">false</span>;</span><br><span class="line">            iskey = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">            temp_key = <span class="string">""</span>;</span><br><span class="line">            <span class="keyword">if</span> (pre.size()&gt;<span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; pre.size(); j++) &#123;</span><br><span class="line">                    temp_key += pre[j];</span><br><span class="line">                    temp_key += <span class="string">"."</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            temp_key+=key;</span><br><span class="line">            all[temp_key] = value;</span><br><span class="line"></span><br><span class="line">            key = <span class="string">""</span>; value = <span class="string">""</span>;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (isvalue &amp;&amp; json[i]==<span class="string">'"'</span>) &#123;</span><br><span class="line">            isstring = <span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (iskey &amp;&amp; json[i]==<span class="string">'"'</span>) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">if</span> ((json[i]==<span class="string">'\\'</span>&amp;&amp;json[i+<span class="number">1</span>]==<span class="string">'"'</span>) || (json[i]==<span class="string">'\\'</span>&amp;&amp;json[i+<span class="number">1</span>]==<span class="string">'\\'</span>)) i++;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (iskey &amp;&amp; !blank) key+=json[i];</span><br><span class="line">        <span class="keyword">if</span> (isvalue &amp;&amp; isstring &amp;&amp; !blank) value+=json[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">        <span class="built_in">string</span> temp;</span><br><span class="line">        getline(<span class="built_in">cin</span>, temp);</span><br><span class="line">        <span class="keyword">if</span> (all[temp]==<span class="string">""</span>) <span class="built_in">cout</span>&lt;&lt;<span class="string">"NOTEXIST"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (all[temp]==<span class="string">"&#123;"</span>) <span class="built_in">cout</span>&lt;&lt;<span class="string">"OBJECT"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="built_in">cout</span>&lt;&lt;<span class="string">"STRING "</span>&lt;&lt;all[temp]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>CCF Answer</category>
      </categories>
      <tags>
        <tag>CCF</tag>
      </tags>
  </entry>
  <entry>
    <title>Conditional GAN (CGAN)</title>
    <url>/2020/07/05/CGAN/</url>
    <content><![CDATA[<p>对于一般的GAN，都是随机生成一个vector，然后输入GAN，再生成一张image，但我们并不能控制output。对于本文要讲述的CGAN，我们则可以操控其输出的结果。</p>
<h4 id="Text-to-Image"><a href="#Text-to-Image" class="headerlink" title="Text-to-Image"></a>Text-to-Image</h4><p>对于传统的监督学习的方法，训练数据集是一些带有描述的图片，网络的input是一段文字，output则是一张图片，我们希望输出的图片和target越接近越好。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705145505825.png" alt="image-20200705145505825" style="zoom:60%;"></p>
<p>如果现在网络的输入文字是“train”，网络会觉得正面的火车是对的，侧面的火车也是对的，网络最后会取这些值的一个平均值，因此会得到一个非常模糊的火车图片。</p>
<h4 id="Conditional-GAN"><a href="#Conditional-GAN" class="headerlink" title="Conditional GAN"></a>Conditional GAN</h4><p>现在我们使用CGAN来完成这个任务。CGAN现在的generator的输入不仅有z，还有一个condition c（“train”），G的输出为$x=G(z,c)$；</p>
<p>这时我们还使用原来的Discriminator，把x输入D，D可以对其进行评价分数scalar，对真实的图像输出为1，生成的图像输出为0。</p>
<p>那么现在就出现了一个新问题，G可以完全不管输入的condition，只生成高质量的、接近真实的图像即可。比如我们把condition设置为“dog”，G如果生成一只猫的图像，这个图像很接近真实图像，那么就可以骗过discriminator。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705150242620.png" alt="image-20200705150242620" style="zoom:60%;"></p>
<p>因此，这里的discriminator也要做出改变，需要把condition也作为D的其中一个输入。</p>
<p>那么现在D输出的分数就有两部分组成：（1）x的真实性；（2）x是不是满足condition的条件。如果这个图片和文字是match的，而且图片很接近真实图像，D就会给这个图像一个高分。</p>
<p>给低分0的两个case：（1）如果给出了正确的文字，但生成了模糊的图像；（2）虽然生成了清晰的图像，但和随机输入的文字（condition）是不匹配的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705151419726.png" alt="image-20200705151419726" style="zoom:60%;"></p>
<p>这里是具体的算法。</p>
<p>Learning D：输入是文字和图像的pair，即$\{(c^1,x^1),…,(c^m,x^m)\}$，从（高斯）分布中取出noise $\{z^1,z^2…,z^m\}$，输入G，得到生成的图像 $\tilde x^i=G(c^i,z^i)$；从database中取出m个真实的图像数据 $\{\hat x^1,…,\hat x^m\}$，再输入discriminator D，不断调整$\theta_d$，使得得到的分数越大越好，</p>
<script type="math/tex; mode=display">
\tilde V =\frac{1}{m}\sum_{i=1}^m logD(c^i,x^i)+\frac{1}{m}\sum_{i=1}^mlog(1-D(c^i,\tilde x^i))
+ \frac{1}{m}\sum_{i=1}^mlog(1-D(c^i,\hat x^i))</script><p>其中$D(c^i,x^i)$表示真实图像所得到的分数，D的目标就是使真实图像获得的分数越大越好；而$D(c^i,\tilde x^i)$表示G生成的图像所得到的分数，应该越小越好，所以前面加了负号；而$D(c^i,\hat x^i)$表示生成了清晰的图像，但和condition不匹配，所以前面加了负号。后两个case都是给低分的情况。</p>
<p>其他过程和之前的GAN差别不大，主要是计算低分数的情况多了一个case。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705153051152.png" alt="image-20200705153051152" style="zoom:67%;"></p>
<h4 id="Conditional-GAN-Discriminator"><a href="#Conditional-GAN-Discriminator" class="headerlink" title="Conditional GAN - Discriminator"></a>Conditional GAN - Discriminator</h4><p>对于一般的discriminator架构，输入为图像和文字，输出为两部分的分数（x的真实性、x和condition是不是match）。如果这个图片和文字是match的，而且图片很接近真实图像，D就会给这个图像一个高分。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705154525712.png" alt="image-20200705154525712" style="zoom:60%;"></p>
<p>有学者在其他论文中也提出了其他架构，而且效果还不错。首先有object输入<span style="color: green">network</span>，得出第一个分数，表示x是否真实；<span style="color: green">network</span>还会输出一个embedding，再和condition结合，输入<span style="color: blue">network</span>后得到另外一个分数，表示x和c是否match。</p>
<p>选择第一种架构方法有一个缺点。前文我们提到CGAN会给低分的两种case：x和condition是match的，但生成的图像不好；x和condition是不match的，但生成的图像质量高。如果我们使用第一种架构，网络会比较confused，网络并不知道分数低的原因到底是哪一种case，有可能是图像不够realistic，也有可能是和condition不够match。但<span style="color: red">对于第二种架构，网络则可以清楚地知道到底是因为哪个原因导致的低分。</span></p>
<p>下面是结果的展示，输入文字condition，输出对应的图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705191337832.png" alt="image-20200705191337832" style="zoom:60%;"></p>
<h4 id="Stack-GAN"><a href="#Stack-GAN" class="headerlink" title="Stack GAN"></a>Stack GAN</h4><p>首先输入一段文字，这些文字先经过embedding的过程，再经过conditioning augmentation的过程（加入噪声），输入generator之后，会生成一张图像（$64\times 64$）；discriminator再来判断这个生成的image和输入的文字是不是match的；</p>
<p>这里还有第二个generator，输入为刚才$64\times64$的图像和文字的embedding结果；再生成一张$256\times256$的图像；再把新生成的图像输入discriminator，看到底是不是realistic。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705191647515.png" alt="image-20200705191647515" style="zoom: 100%;"></p>
<h4 id="Image-to-image"><a href="#Image-to-image" class="headerlink" title="Image-to-image"></a>Image-to-image</h4><h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>CGAN不仅可以输入一段文字产生一张图像，也可以是image-to-image，比如把简单的几何模型转化为真实的房屋模型，把灰度图转化为彩色图，</p>
<p><img src="/2020/07/05/CGAN/image-20200705193817063.png" alt="image-20200705193817063"></p>
<p>如果使用传统的监督学习的方法来完成这个任务，首先需要收集训练数据（几何模型图、对应的真实房屋图），训练network，output是一张图片，我们希望输出的图片和target越接近越好。由于network会产生多种多样的房子，最后的结果会取一个平均值，因此会产生一个非常模糊的图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705194028821.png" alt="image-20200705194028821" style="zoom:60%;"></p>
<p>我们可以使用GAN来完成这个任务。首先把从distribution中sample出来的z和结构图作为G的输入，G会生成一张新的Image；再把Image和原来的结构图（一个pair）输入D，会输出一个分数scalar。可以发现GAN生成的图要相对清晰很多。</p>
<p>但我们这时发现了一个新问题，GAN产生了一些原来的结构图中没有的东西，比如在图的左上角，产生了一个像是窗户或者天线的东西。这时我们可以加入一个新的constrain（真实的房屋图），希望generator产生的image和训练数据集中对应的图像也越靠近越好。</p>
<p>这时generator的目标就有两个：产生出足够清晰可以骗过D的图像；产生的新图像和原来的target要接近。这样就会产生结果很好的图（GAN+close），图足够清晰，也不会产生一些奇怪的东西。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705195049882.png" alt="image-20200705195049882" style="zoom:60%;"></p>
<h5 id="Patch-GAN"><a href="#Patch-GAN" class="headerlink" title="Patch GAN"></a>Patch GAN</h5><p>现在要生成的是一张很大的图像，如果这时discriminator的输入还是一整张大的图像，那么D要对其进行评分，就肯定需要更多的参数，很有可能产生overfiting或者训练所需的时间会很长。</p>
<p>在这篇论文中，作者也对discriminator的设计进行了变化。discriminator只需要检查图中的一小部分，对这一小块图片来输出评价分数 。区域的大小也是需要调整的参数之一。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705200803367.png" alt="image-20200705200803367" style="zoom:60%;"></p>
<h4 id="Speech-Enhancement"><a href="#Speech-Enhancement" class="headerlink" title="Speech Enhancement"></a>Speech Enhancement</h4><p>如果我们使用传统的深度学习方法来做语音增强，首先需要把纯净语音加上一些noise再输入CNN，不断地训练CNN，使其能够输出去噪后的纯净语音。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705201419793.png" alt="image-20200705201419793" style="zoom:60%;"></p>
<p>直接产生同样会有语音频谱图比较模糊的情况，我们在这里也可以使用CGAN算法。</p>
<p>输入为带noise的语音信号，G的输出为增强语音，增强语音和纯净语音之间应该越接近越好。discriminator的输入为增强语音和带噪语音，输出评价分数，看这个output是不是clean的，还要看output和noise这个pair是不是match的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705201744963.png" alt="image-20200705201744963" style="zoom:60%;"></p>
<h4 id="Video-Generation"><a href="#Video-Generation" class="headerlink" title="Video Generation"></a>Video Generation</h4><p>输入一段video，让generator预测下一步会发生什么，产生对应的video；discriminator要同时考虑generator的input和output，可以把它们接到一起，变成一段完整的影片，让discriminator来判断到底是不是一个合理的影片。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705202357625.png" alt="image-20200705202357625" style="zoom:60%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Conditional GAN</tag>
        <tag>CGAN</tag>
        <tag>Speech Enhancement</tag>
        <tag>Video Generation</tag>
        <tag>Patch GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>Convolutional Neural Network</title>
    <url>/2020/06/08/CNN/</url>
    <content><![CDATA[<h4 id="Why-CNN-for-Image"><a href="#Why-CNN-for-Image" class="headerlink" title="Why CNN for Image"></a>Why CNN for Image</h4><p>可以使用相同的参数来提取不同图像中的同一种特征，这样就可以降低网络需要学习的参数</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608170112931.png" alt="image-20200608170112931" style="zoom:50%;"></p>
<p>CNN还有另外一种操作可以减少网络需要训练的参数，即下采样（subsampling），subsampling可以减少图像的大小。采样层就是使用pooling的技术来实现的，可以用max pooling或average pooling，获取某个像素点及其周围区域的最大值或平均值，将这些像素都用一个像素来表示，就可以缩小图像的大小。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608170313635.png" alt="image-20200608170313635" style="zoom:50%;"></p>
<p>一个完整的CNN网络结构，由多个convolution和pooling层、以及全连接层组成。输入图像先经过多次的convolution、pooling，提取图像中的特征，再把这些特征flatten成一个一维的向量，即全连接层，最后再得出分类结果 cat or dog</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608170601092.png" alt="image-20200608170601092" style="zoom:50%;"></p>
<h4 id="convolution"><a href="#convolution" class="headerlink" title="convolution"></a>convolution</h4><p>对于我们想要提取图像中的两个特征，我们使用filter1和filter2这两个过滤器，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608172139320.png" alt="image-20200608172139320" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608172200059.png" alt="image-20200608172200059" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608172256317.png" alt="image-20200608172256317" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608172311889.png" alt="image-20200608172311889" style="zoom:50%;"></p>
<h4 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h4><h5 id="local-field"><a href="#local-field" class="headerlink" title="local field"></a>local field</h5><p>下一层的neural 3，只与前层的9个neural相连接，而不是和前层的全部neural连接</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608172513373.png" alt="image-20200608172513373" style="zoom:50%;"></p>
<h5 id="weight-sharing"><a href="#weight-sharing" class="headerlink" title="weight sharing"></a>weight sharing</h5><p>对于图像的同一种特征，只需要一个过滤器filter即可，这样就可以进一步减少图像中的参数</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608173532837.png" alt="image-20200608173532837" style="zoom:50%;"></p>
<h4 id="Flatten"><a href="#Flatten" class="headerlink" title="Flatten"></a>Flatten</h4><p>每经过一次convolution和pooling操作，都会生成一张新的图片，这张新图片的大小比原图要小很多，减少了需要训练的参数</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608173901231.png" alt="image-20200608173901231" style="zoom:50%;"></p>
<p>再把经过多次convolution和pooling的图像flatten，展开成一维的向量，再输入全连接层</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608174020284.png" alt="image-20200608174020284" style="zoom:50%;"></p>
<h4 id="CNN-in-Keras"><a href="#CNN-in-Keras" class="headerlink" title="CNN in Keras"></a>CNN in Keras</h4><p>首先先介绍一下convolution和pooling对图像大小变化的公式，设输入图像的宽度和高度分别为w和h，卷积核大小为$F\times F$，步长stride大小为S，如果再加入Padding操作，经过卷积或池化操作后图像的大小为W和H</p>
<script type="math/tex; mode=display">
H/W=\frac{(h/w - F+2P)}{S}+1</script><p>这里引入了keras，先介绍几个主要函数的参数。</p>
<p>我们可以先通过以下函数来生成一个convolution layer</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model2.add(Convolution2D(<span class="number">25</span>,<span class="number">3</span>,<span class="number">3</span>, input_shape=(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)))</span><br></pre></td></tr></table></figure>
<p>其中25表示filter的数量，$3\times3$表示filter的大小，<code>model2.add()</code>其中的<code>Convolution2D</code>还有一个参数input_shape，表示函数输入的图像大小，例子中表示一个单通道的$28\times28$图像，如果需要输入彩色图像，则是$28\times28\times3$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608174611511.png" alt="image-20200608174611511" style="zoom:50%;"></p>
<p>下面我们将继续介绍通过convolution和pooling操作后参数数量的变化。对于下图中的例子，我们先考虑输入$28\times28$大小的图像，即$w=28,h=28$，这里默认步长为1，padding为0，经过$3\times3$卷积核大小的过滤，图像的大小就变为$26\times26$</p>
<script type="math/tex; mode=display">
\frac{28-3+2*0}{1}+1=26</script><p>由于输入的图像只有1个channel，卷积核大小为$3\times3$，因此第一层只有9个参数</p>
<p>由于前一层有25个filter，经过一次convolution操作后，图像大小为$26\times26$，本层的neural个数为$25\times26\times26$；还需要再进行一次pooling操作，经过pooling操作之后图像大小为$13\times13$，max pooling操作没有filter，不算参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model2.add(MaxPooling2D((<span class="number">2</span>,<span class="number">2</span>)))</span><br></pre></td></tr></table></figure>
<p>pooling操作后图像宽度的大小为，</p>
<script type="math/tex; mode=display">
\frac{26-2+2*0}{2}+1=13</script><p>经过一次convolution和pooling操作之后，neural个数为$25\times13\times13$；</p>
<p>对于第二次的convolution操作，有50个大小为$3\times3$的filter，输出的neural数量为$50\times11\times11$；一次pooling操作所包含的区域大小为$2\times2$，输出的neural数量为$50\times5\times5$</p>
<script type="math/tex; mode=display">
\frac{13-3+2*0}{1}+1=11\\
\frac{11-2+2*0}{2}+1=5</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608175330480.png" alt="image-20200608175330480" style="zoom:50%;"></p>
<p>经过两次的convolution和pooling操作之后，neural的数量为$50\times5\times5$，下一步操作就是将这些神经元flatten，展开成一维向量；再输入激活函数和全连接层</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608202515115.png" alt="image-20200608202515115" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Classification</title>
    <url>/2020/06/05/Classification/</url>
    <content><![CDATA[<p>考虑到宝可梦的两个属性（<strong>Defense</strong>、<strong>SP Defense</strong>），将输入的宝可梦进行属性分类（Water、Normal）</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605204445258.png" alt="image-20200605204445258" style="zoom:50%;"></p>
<h4 id="Gaussian-Distribution"><a href="#Gaussian-Distribution" class="headerlink" title="Gaussian Distribution"></a>Gaussian Distribution</h4><p>假设图中的点服从高斯分布，由于只考虑了两个属性，$\mu$为一个二维向量，则有高斯分布公式，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605202837571.png" alt="image-20200605202837571" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605204129951.png" alt="image-20200605204129951" style="zoom:50%;"></p>
<h4 id="Maximum-Likelihood"><a href="#Maximum-Likelihood" class="headerlink" title="Maximum Likelihood"></a>Maximum Likelihood</h4><p>第i个example的概率密度函数为$f_{\mu,\sum}(x^i)$，可得出最大似然函数$L(\mu,\sum)$的表达式，其中$\mu=\mu^<em>,\sum=\sum^</em>$时，函数L得到最大值</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605203041766.png" alt="image-20200605203041766" style="zoom:50%;"></p>
<p>这里我们假设有两个类别，其参数和分布如下，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605210600803.png" alt="image-20200605210600803" style="zoom:50%;"></p>
<p>输入样例x是Class 1:Water的概率为，</p>
<script type="math/tex; mode=display">
P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x)}=\frac{P(x|C_1)P(C_1)}
{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}</script><p>再代入相应的表达式，其中$P(x|C_1)=f_{\mu^1,\sum^1},P(x|C_2)=f_{\mu^2,\sum^2}$，即可算出x为C1的概率，如果算出这个概率大于0.5，我们就可以认为x的属性为C1</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605212347236.png" alt="image-20200605212347236" style="zoom:50%;"></p>
<p>但这样子算出来的分类精确度很低，只有47%，就算加入其他的属性，精确度也只提高到了54%</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605214028329.png" alt="image-20200605214028329" style="zoom:50%;"></p>
<h4 id="Modifying-Model"><a href="#Modifying-Model" class="headerlink" title="Modifying Model"></a>Modifying Model</h4><p>由于上面模型的精确度都不高，所以在此我们对模型进行了修改，模型的参数就只有三个，$\mu^1,\mu^2,\sum=\sum^1=\sum^2$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605214245755.png" alt="image-20200605214245755" style="zoom:50%;"></p>
<p>对于Water属性的宝可梦对应参数为$\mu^1$，Normal属性的宝可梦为$\mu^2$，共同属性为$\sum$，这时对应的最大似然函数为$L(\mu^1,\mu^2,\sum)$，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605214446675.png" alt="image-20200605214446675" style="zoom:50%;"></p>
<p>修改后的模型准确率可以达到54%，如果加入更多的属性，准确率可以提高到73%</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605215021833.png" alt="image-20200605215021833" style="zoom:50%;"></p>
<h4 id="Three-Steps"><a href="#Three-Steps" class="headerlink" title="Three Steps"></a>Three Steps</h4><p>这里再回忆一下三个步骤：<br>（1）Function Set，计算分类为该类的概率$P(C_1|x)$，如果大于0.5，则认为类别为1，否则为类别2<br>（2）找出相对应的$\mu,\sum$，使得似然函数L取得最大值；<br>（3）得出使似然函数最大化的参数，$\mu^<em>,\sum^</em>$，</p>
<script type="math/tex; mode=display">
u^*=\frac{1}{79}\sum_{n=1}^{79}x^n,\quad\sum^*=\frac{1}{79}\sum_{n=1}^{79}(x^n-\mu^*)(x^n-\mu^*)^T</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605220705405.png" alt="image-20200605220705405" style="zoom:50%;"></p>
<h4 id="Probability-Distribution"><a href="#Probability-Distribution" class="headerlink" title="Probability Distribution"></a>Probability Distribution</h4><p>这里我们所使用的概率分布是</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605221617855.png" alt="image-20200605221617855" style="zoom:50%;"></p>
<p>下面开始公式推导，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605221733838.png" alt="image-20200605221733838" style="zoom:50%;"></p>
<p>得出了我们的Sigmoid函数，$\sigma(z)$的函数图像为s型，值域范围为[0,1]，将z化简，并代入$P(C_i)=\frac{N_i}{N_1+N_2}$ (i=1,2)，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605221911457.png" alt="image-20200605221911457" style="zoom:50%;"></p>
<p>将$P(x|C_1),P(x|C_2)$的表达式代入$ln\frac{P(x|C_1)}{P(x|C_2)}$，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605222200879.png" alt="image-20200605222200879" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605223207505.png" alt="image-20200605223207505" style="zoom:50%;"></p>
<p>再进行进一步化简，带入$\sum^1=\sum^2=\sum$，我们可以得出z的简易表达式$z=w\cdot x + b$，可得出$P(C_1|x)=\sigma(z)=\sigma(w\cdot x+b)$。当得出$N_1,N_2,\mu^1,\mu^2,\sum$时，就可以计算出w和b的值。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605223410530.png" alt="image-20200605223410530" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>classification</tag>
        <tag>sigmoid function</tag>
      </tags>
  </entry>
  <entry>
    <title>Unsupervised Conditional Generation</title>
    <url>/2020/07/06/CycleGAN/</url>
    <content><![CDATA[<p>在以前的文章中，我们提到过<a href="https://scarleatt.github.io/2020/07/05/CGAN/" target="_blank" rel="noopener">CGAN</a>，训练数据包括图像和其对应的文字描述，是一种监督学习的方法。本文将叙述一种使用CGAN进行无监督学习的方法，主要包括两大类的方法：Direct Transformation和Projection to Common Space。</p>
<h4 id="Unsupervised-Conditional-Generation"><a href="#Unsupervised-Conditional-Generation" class="headerlink" title="Unsupervised Conditional Generation"></a>Unsupervised Conditional Generation</h4><p>如果现在有一张真实的风景照X，还有一张图像是梵谷的画作Y，那么我们就可以训练一个generator，使其输出像梵谷画作的图像。现在我们要完成的任务是对图像进行风格转换，我们可以收集很多真实的风景照，也可以收集很多梵谷的画作，但我们很难收集到这两者之间的联系，相当于训练数据是没有label的，因此就需要进行无监督的学习。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706144703530.png" alt="image-20200706144703530" style="zoom:60%;"></p>
<p>这种技术不仅可以用到图像领域，也可以用到其他领域，比如语音处理领域。</p>
<p>根据收集到的论文，Unsupervised CGAN可以分为两大类的方法：</p>
<ol>
<li>Direct Transformation：学习一个generator，直接将Domain X的图像转化为Domain Y的图像；这种处理方式不会对input进行太大的改变，如果是影像的话，通常只会修改一下颜色、质地之类的；</li>
<li>Projection to Common Space：input和output差距很大，不仅仅是只有颜色和纹理的变化；这时就需要先使用encoder，得出input的特征，比如这是个男生、还戴着眼镜，再用decoder生成对应的动漫角色</li>
</ol>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706151909380.png" alt="image-20200706151909380" style="zoom:60%;"></p>
<h4 id="Direct-Transformation"><a href="#Direct-Transformation" class="headerlink" title="Direct Transformation"></a>Direct Transformation</h4><h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>现在是无监督的学习，generator如何得知自己是不是产生了类似Domain Y的图像呢？这时我们就需要训练一个Domain Y的discriminator，这个discriminator看过很多属于domain Y的图像；对于给定的图像，就可以判断出到底属于哪个domain的图像。</p>
<p>对于generator，这时的训练目标就是生成能骗过discriminator的图像。如果generator能生成骗过discriminator的图像，那么我们就可以认为generator生成了和domain Y类似的图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706153136496.png" alt="image-20200706153136496" style="zoom:60%;"></p>
<p>generator可以产生很像梵谷画作的图像，但这个图像可以是和input毫无关系的，这并不是我们想要的结果。因此我们并不能只要求generator生成的图像能骗过discriminator就好。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706154211838.png" alt="image-20200706154211838" style="zoom:60%;"></p>
<p>实际上，在generator不加额外限制的条件下，generator的input和output通常差别不会特别大（比如input是风景图，output是梵谷的自画像），generator通常只希望改一小部分内容能骗过discriminator就好，并不希望进行太大的改变。因此如果不加额外的constrain，这个GAN也是可以work的。</p>
<p>有学者在论文中提出了其他的解决方法。</p>
<p>(1) 如果generator比较shallow，不那么deep，不用加额外的constrain，就可以使input和output差距不大；如果generator很深，就可以使input和output差距很大，这时就需要一些额外的constrain。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706154355533.png" alt="image-20200706154355533" style="zoom:60%;"></p>
<p>(2) 还可以使用另外一种方法。现在有一个pre-trained的network（VGG等），把generator的input和output输入这个network，会输出一个embedding（<a href="https://scarleatt.github.io/2020/06/11/Word-Embedding/" target="_blank" rel="noopener">word embedding</a>）。那么generator的目标就有两个：输出和梵谷画作类似的图像；其input和output之间的差距也不能太大。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706155513844.png" alt="image-20200706155513844" style="zoom:60%;"></p>
<h5 id="Cycle-GAN"><a href="#Cycle-GAN" class="headerlink" title="Cycle GAN"></a>Cycle GAN</h5><p>(3) <strong>Cycle GAN</strong>，现在有一个<span style="color: blue">generator</span> $G_{X\rightarrow Y}$可以生成domain X到Y的图像，还有另外一个<span style="color: orange">generator</span> $G_{Y\rightarrow X}$可以生成domain Y到X的图像output，生成的图像应与原来的input越接近越好。现在的generator有了两个目标：产生能骗过discriminator的图像；使对应的input和output越接近越好。</p>
<p>那么现在就不可能在中间产生一个像梵谷自画像的图像，因为这时第二个generator就不可能从这个自画像返回原来的自画像，不满足两个限制条件。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706160720337.png" alt="image-20200706160720337" style="zoom:60%;"></p>
<p>Cycle GAN也可以是双向的。原来的网络是使domain $X\rightarrow Y,Y\rightarrow X$，现在我们加入了另外一个GAN网络，使输入domain Y的图input转化为X的图，再使domain X的图转化为Y的图output，input和output之间的差距也应该越小越好。这时还对应了两个discriminator。现在我们就可以同时train这两个网络。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706162008139.png" alt="image-20200706162008139" style="zoom:60%;"></p>
<h5 id="Issue-of-Cycle-Consistency"><a href="#Issue-of-Cycle-Consistency" class="headerlink" title="Issue of Cycle Consistency"></a>Issue of Cycle Consistency</h5><p>CycleGAN会把input的一些信息藏起来，output的时候会把这些信息又呈现出来。</p>
<p>下图中的网络是使输入的真实图像转化为类似卫星图的图像。第一个generator把input转化为卫星图，第二个generator再转化为原来的真实图像output。我们可以在input的红色方框内有一些黑点，中间卫星图的部分却没有黑点，再output的红色方框内又出现了这些黑点。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706162852240.png" alt="image-20200706162852240" style="zoom:60%;"></p>
<p>Q：那么为什么generator可以从第二张图像中生成output呢？</p>
<p>A：答案是第一个generator把这些关键信息隐藏了，只是我们人眼并不能看到这些藏起来的信息。</p>
<p>下图中还有一些其他GAN网络（Dual GAN，Disco GAN），但核心思想都是和Cycle GAN差不多的，只是论文提交到了不同期刊上。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706163814535.png" alt="image-20200706163814535" style="zoom:60%;"></p>
<h5 id="StarGAN"><a href="#StarGAN" class="headerlink" title="StarGAN"></a>StarGAN</h5><p>如果我们现在不是在两个Domain之间互转，而是在4个Domain之间，在理论上就需要12个GAN网络才可以实现。但StarGAN只学习了1个generator，就可以实现在多个Domain之间互转。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706164130509.png" alt="image-20200706164130509" style="zoom:60%;"></p>
<p>下面简要叙述StarGAN的算法：</p>
<p>(a) 首先训练了一个discriminator，需要来鉴别输入的图像是real/fake，还需要得出图像到底是属于哪一个domain；</p>
<p>(b) 还需要学习一个generator，输入是一张图像和目标domain，即你想让input转化成哪一个domain，生成一个Fake Image；</p>
<p>(c) 把生成的图像再输入同一个generator，目标domain也作为输入，生成一张新的图像（reconstructed image），我们希望reconstructed image和input image之间越接近越好；</p>
<p>(d) 把Fake Image再输入discriminator，看到底是不是符合要求的图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706164451665.png" alt="image-20200706164451665"></p>
<p>下图是一个更加realistic的展示。domain可以有多个，所以用一串编码表示，比如现在输入图像的domain是Brown、Young，对应的编码为00101，记作CelevA label。</p>
<p>(a) 现在把一张图像输入discriminator，来判断到底是不是真实的图像，且输出domain对应的代号；(b) 把input image和target domain label（10011，Black、Male、Young）输入generator；(c) 把上一部生成的图像再输入同一个generator，让其生成00101（brown、young）的图像output，我们希望input和output越接近越好；(d) 把第一次generator生成的图像输入discriminator，看是不是realistic的图像，且输出对应的domain代号。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706165503010.png" alt="image-20200706165503010"></p>
<h4 id="Projection-to-Common-Space"><a href="#Projection-to-Common-Space" class="headerlink" title="Projection to Common Space"></a>Projection to Common Space</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706182808106.png" alt="image-20200706182808106" style="zoom:60%;"></p>
<h5 id="Target"><a href="#Target" class="headerlink" title="Target"></a>Target</h5><p>现在Domain X是真实人物图像，Domain Y是动漫人物，X和Y之间差距很大，就不能用之前的direct transformation。</p>
<p>这里我们可以先使用encoder $EN_X$提取出X的特征，用另一个encoder $EN_Y$提取Y的特征，即图像输入encoder会输出一个vector；把vector输入decoder，如果输入domain X的decoder，就产生真实人物的图像，如果输入domain Y的decoder，就产生动漫人物的图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706183118051.png" style="zoom:60%;"></p>
<p>再回顾一下我们的问题，我们希望输入真实人物的图像，网络输出动漫人物的图像。这里vector，表示人脸的特征，其每一个维度对应人脸的某个属性，比如戴不戴眼镜。即我们希望decoder能够根据这些attribute生成对应的动漫人物。</p>
<p>如果我们知道X和Y之间的关系，这个问题用supervise学习可以很简单地解决，但现在这是一个unsupervised问题，我们可以收集domain X的很多数据，也可以收集domain Y的很多数据，但这两者之间的联系我们却很难收集。</p>
<p>那么我们怎么来训练这个encoder和decoder呢？</p>
<h5 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h5><p>我们可以先把属于domain X的encoder和decoder结合起来，组成一个auto-encoder，输入一张domain X的图，经过encode-decode的过程，使其reconstruct成原来输入的图，使这两者之间的reconstruction error最小化。属于domain Y的encoder和decoder也使用类似的操作。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706190811253.png" alt="image-20200706190811253" style="zoom:60%;"></p>
<p>这样做会造成一个新的问题，这两个encoder和decoder之间是没有关联的。</p>
<p>我们可以再多加discriminator进来，让输入domain X的decoder的输出更像X。如果我们只是来学习这个auto-encoder，使reconstruction error最小化，会使decoder的output非常模糊。</p>
<p>现在这个属于domain X的encoder和decoder，以及discriminator和起来，就相当于一个VAE GAN；属于domain Y的encoder和decoder，discriminator相当于另外一个VAE GAN。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706191458656.png" alt="image-20200706191458656" style="zoom:60%;"></p>
<p>由于这两个auto-encoder是分开学习的，如果现在输入一张真实的人脸，属于domain Y的decoder很可能输出一张截然不同的人脸。两个encoder是分开训练的，很可能encoder $EN_X$输出的vector的第一维代表性别、第二维代表戴不戴眼镜，encoder  $EN_Y$输出的vector的第二维代表性别、第三维代表戴不戴眼镜。</p>
<p><u>Solution 1:</u> 为了解决这个问题，有学者提出了新的解法。对于不同domain的encoder和decoder，我们可以让其tie到一起。具体做法是：属于domain X和Y的网络结构都有多个hidden layer，我们可以让这两者的后面某几层hidden layer的参数是共用的；对应的decoder，可以前面几个hidden layer是共用的，后面几个不是共用的。</p>
<p>如果我们共用encoder的后面几个hidden layer，属于domain X和Y的encoder所输出的vector，都是属于同一个latent space的，用同样的dimensions来表示人脸的同一个特征，即第一维都表示男性，第二维都表示戴眼镜。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706193608486.png" alt="image-20200706193608486" style="zoom:60%;"></p>
<p><u>Solution 2:</u> 加一个domain discriminator，可以对domain X和Y的encoder所输出的vector进行判断，看到底是属于哪一个domain的图像。如果这个domain discriminator不能进行判断，那么我们就可以认为这两个encoder所生成的vector其distribution都是一样的，从而这两个distribution中相同的维度表示相同的意思。</p>
<p>假设domain X和Y中男女比例、戴不戴眼镜的比例都是一样的，现在domain discriminator可以强迫让这个embedding的latent feature是一样的，因此就会用同样的dimension来表示这个vector（The domain discriminator forces the output of 𝐸𝑁x and 𝐸𝑁y have the same distribution. ）。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706194620613.png" alt="image-20200706194620613" style="zoom:60%;"></p>
<p><u>Solution 3</u>: 还可以用cycle consistency。真实人物的图像input输入domain X的encoder，生成对应的code再输入domain Y的decoder，重建输入的图像；再输入domain Y的encoder，对应的code输入domain X的decoder，得到output，目标是使input和output之间的error越小越好。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706200901735.png" alt="image-20200706200901735" style="zoom:60%;"></p>
<h4 id="Voice-Conversion"><a href="#Voice-Conversion" class="headerlink" title="Voice Conversion"></a>Voice Conversion</h4><p>把一个人的声音转化成另一个人的声音。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706202208008.png" alt="image-20200706202208008" style="zoom:60%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Cycle GAN</tag>
        <tag>StarGAN</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>Tips for Training DNN</title>
    <url>/2020/06/07/DNN-tip/</url>
    <content><![CDATA[<blockquote>
<p>本文的主要思路：针对training set和testing set上的performance分别提出针对性的解决方法 1、在training set上准确率不高：   new activation function：ReLU、Maxout   adaptive learning rate：Adagrad、RMSProp、Momentum、Adam<br>2、在testing set上准确率不高：Early Stopping、Regularization or Dropout</p>
</blockquote>
<h3 id="Recipe"><a href="#Recipe" class="headerlink" title="Recipe"></a>Recipe</h3><h4 id="three-steps-of-deep-learning"><a href="#three-steps-of-deep-learning" class="headerlink" title="three steps of deep learning"></a>three steps of deep learning</h4><p>做深度学习也遵循这三个步骤：</p>
<ul>
<li>define a set of function，</li>
<li>goodness of function，找到loss function</li>
<li>pick the best function，找到使loss最小化的参数</li>
</ul>
<p>overfitting是指模型在训练集上表现良好，但在测试集上表现却很差的现象。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607122016217.png" alt="image-20200607122016217" style="zoom:50%;"></p>
<h4 id="Do-not-always-blame-Overfitting"><a href="#Do-not-always-blame-Overfitting" class="headerlink" title="Do not always blame Overfitting"></a>Do not always blame Overfitting</h4><p>在下图中，我们展示了一个20层和56层的network，在训练集和测试集上的error。黄色表示20层network，红色表示56层network。</p>
<p>由于模型在训练集上的表现，20层的network表现得比较好，有同学就认为这是overfitting，但其实这并不是overfitting问题，因为这个模型在训练集上的表现，也是20层的network表现好</p>
<p>之所以出现这个20层和56层network表现（都是20层network表现好），是由于模型的训练没有训练好</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607123330505.png" alt="image-20200607123330505" style="zoom:50%;"></p>
<h4 id="Different-approaches-for-different-problems"><a href="#Different-approaches-for-different-problems" class="headerlink" title="Different approaches for different problems."></a>Different approaches for different problems.</h4><p>在网络的训练过程中，我们要针对网络的不同问题提供不同的解决方法，主要有两个问题</p>
<ul>
<li>在training data上表现不好</li>
<li>在testing data上表现不好</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607123926844.png" alt="image-20200607123926844" style="zoom:50%;"></p>
<h3 id="Good-Results-on-Training-Data"><a href="#Good-Results-on-Training-Data" class="headerlink" title="Good Results on Training Data?"></a>Good Results on Training Data?</h3><p>要在training data上获得好的结果，可以使用new activation function和adaptive learning rate</p>
<h4 id="New-Activation-Function"><a href="#New-Activation-Function" class="headerlink" title="New Activation Function"></a>New Activation Function</h4><h5 id="deeper-is-better-？"><a href="#deeper-is-better-？" class="headerlink" title="deeper is better ？"></a>deeper is better ？</h5><p>在1980年代，network中主要使用sigmoid funcion作为激活函数，从下图中我们可以看出，使用sigmoid function并不能保证网络结构越深，训练结果越好</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607124417000.png" alt="image-20200607124417000" style="zoom:50%;"></p>
<h5 id="Vanishing-Gradient-Problem"><a href="#Vanishing-Gradient-Problem" class="headerlink" title="Vanishing Gradient Problem"></a>Vanishing Gradient Problem</h5><p>出现上面这个问题的原因并不是overfitting，而是vanishing gradient（梯度消失）</p>
<p>当网络层数很深的时候，在靠近input layer的位置，常常会有很小的 gradient，学习速度也很慢；而在靠近output layer的地方，常常会有更大的gradient，学习速度也会很快，很快就到了converge（收敛）了</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607125209255.png" alt="image-20200607125209255" style="zoom:50%;"></p>
<p>下面将叙述出现这个问题的原因。对于下图的中sigmoid function，输出范围为[0,1]，对于很大的输入，输出往往会被压缩成一个较小的值</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607125550128.png" alt="image-20200607125550128" style="zoom: 50%;"></p>
<p>对于loss function对其中一个参数w的微分$\frac{\partial l}{\partial w}$，这里我们将其表达式写为$\Delta w$，可以表示当前参数w对结果loss的影响，当把这个参数w进行变化时，对loss的影响会有多大</p>
<p>如下图所示，如果我们输入一个很大的$\Delta w$，在经过sigmoid function运算之后，其值就缩小一次；当经过后面多层的压缩之后，$\Delta w$的值就变得越来越小;……；因此gradient在input layer附近的值会很大，但在output layer附近的值经过多次的压缩就变得很小了。</p>
<p>当缩小的gradient达到我们设置的那个临界值，即gradient接近于0，就发生了梯度消失问题</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607125910525.png" alt="image-20200607125910525" style="zoom:50%;"></p>
<p>要解决这个问题，可以修改一下network中用到的激活函数</p>
<h5 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h5><p>我们选取ReLU函数的原因有以下几个：</p>
<ul>
<li>可以快速计算，不管是函数值还是对应的梯度</li>
<li>结合了生物上的一些观察</li>
<li>无穷多个不同bias的sigmoid function叠加的结果可以变成ReLU</li>
<li><u>可以解决梯度消失问题</u>；</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607131402741.png" alt="image-20200607131402741" style="zoom:50%;"></p>
<p>使用ReLU函数之后，代入具体的网络结构</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607131816149.png" alt="image-20200607131816149" style="zoom:50%;"></p>
<p>对于input为0的值，network将不再计算其相对应的weight，而对于input不为0的值，就相当于一个线性函数$y=x$。这样做可以简化网络结构，网络结构中也就不存在gradient很小的neural</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607131920750.png" alt="image-20200607131920750" style="zoom:50%;"></p>
<p>Q：但这时出现了一个新问题，ReLU函数在z=0这一点是不可导的，那么我们在根据loss function如何来计算gradient呢？</p>
<p>A：这里我们将输入z<0的数的gradient看作0，相当于从network中抹去了这部分神经元；对于z>=0的数，gradient=1</0的数的gradient看作0，相当于从network中抹去了这部分神经元；对于z></p>
<h5 id="ReLU-variant"><a href="#ReLU-variant" class="headerlink" title="ReLU - variant"></a>ReLU - variant</h5><p>对于ReLU，当x&lt;=0时，函数的输出值就为0了，网络中的参数也没办法更新。因此，就有学者提出了Leaky ReLU，当x&lt;=0时，函数的输出值不是0，而是乘以一个系数0.01，这时的函数就称作<strong>Leaky ReLU</strong></p>
<p>还有另外一种ReLU函数的变体，<strong>Parametric ReLU</strong>，前面乘上的系数也可以进行训练</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607143129117.png" alt="image-20200607143129117" style="zoom:50%;"></p>
<h5 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h5><blockquote>
<p>ReLU is a special cases of Maxout</p>
</blockquote>
<p>Maxout的主要思想是：让network自己去学习对应的activation function，可以学习出ReLU，也可以是其他的activation function</p>
<p>Maxout激活函数是对前几个神经元取最大值，再输出相应的最大值</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607144717796.png" alt="image-20200607144717796" style="zoom:50%;"></p>
<p><strong>Maxout-&gt;ReLU</strong></p>
<p>在下图中，对于左图中的ReLU function</p>
<ul>
<li>input为蓝色直线，表示$z=wx+b$，</li>
<li><p>output：当z<0时，relu也输出为0；当z>0时，ReLU的图像和z是一致的</0时，relu也输出为0；当z></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607144943655.png" alt="image-20200607144943655" style="zoom:50%;"></p>
</li>
</ul>
<p>对于右图中的Maxout function，$z_1$对应的权重是$w,b$，而$z_2$对应的权重则是0，</p>
<ul>
<li>input为$z_1,z_2$，<ul>
<li>蓝色直线表示$z_1=wx+b$，</li>
<li>红色表示$z_2=0$</li>
</ul>
</li>
<li>这时neural的output为$max\{z_1,z_2\}$，输出则是和ReLU一致的（图中绿色直线）</li>
</ul>
<p><strong>Maxout-&gt;more than ReLU</strong></p>
<p>maxout不仅可以学习ReLU，也可以学习其他的activation function</p>
<p>对于右图中的新的输入，$z_2$对应的权重则变成了$w’,b’$，那么相应的input和output为</p>
<ul>
<li>input，$z_1=wx+b,z_2=w’x+b’$，分别对应图中蓝色和绿色直线；</li>
<li>output为$max\{z_1,z_2\}$，表现为图中绿色直线</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607150829520.png" alt="image-20200607150829520" style="zoom:50%;"></p>
<p>这时我们得到的activation function就是图中的绿色直线，是通过网络training出来的，训练的参数为$w,w’,b,b’$，训练结束即可得出我们的activation function</p>
<p><strong>Summary</strong></p>
<p>这里我们先对maxout做一个总结，maxout是一个可学习的activation function，可以学习出任何分段线性凸函数（piecewise linear convex function），具体的分段数取决于在group中的元素个数</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607151653515.png" alt="image-20200607151653515" style="zoom:50%;"></p>
<p>对于maxout函数的训练，如果是下图中的网络结构，我们假设已经知道$z_1^1,z_4^1,z_2^2,z_3^2$为对应的最大值，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607152054887.png" alt="image-20200607152054887" style="zoom:50%;"></p>
<p>那么network可以再次被化简，neural也可以变少</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607152242384.png" alt="image-20200607152242384" style="zoom:50%;"></p>
<h4 id="Adaptive-Learning-Rate"><a href="#Adaptive-Learning-Rate" class="headerlink" title="Adaptive Learning Rate"></a>Adaptive Learning Rate</h4><h5 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h5><p><strong>Adagrad</strong></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607152520774.png" alt="image-20200607152520774" style="zoom:50%;"></p>
<p><strong>RMSprop</strong></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607153455066.png" alt="image-20200607153455066" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607153507651.png" alt="image-20200607153507651" style="zoom:50%;"></p>
<p>gradient为0的点可以是local minimum，也可以是saddle point，还可以是在很平缓的plateau中的某些点</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607153541752.png" alt="image-20200607153541752" style="zoom:50%;"></p>
<p>而在物理世界，物体本身是带有momentum的，再加上gradient的作用，就很可能可以跳出saddle point，继续训练</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607153719322.png" alt="image-20200607153719322" style="zoom:50%;"></p>
<h5 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h5><p>图中蓝色箭头表示Movement（前进方向），红色箭头表示gradient的方向，绿色虚线表示上一次movement对本次的影响（惯性）</p>
<p>再$\theta^0$处，movement为$v^0=0$；对于在$\theta^1$处的前进方向，先计算出在$\theta^0$处的梯度$\Delta L(\theta^0)$，我们要移动的方向是由上一个时间点的gradient为$\Delta L(\theta^0)$和前进方向$v_0$决定的，即</p>
<script type="math/tex; mode=display">
v^1=\lambda v^0-\eta \Delta L(\theta^0)=-\eta \Delta L(\theta^0)</script><p>其中$\lambda$也是一个可以手动调整的参数</p>
<p>对于下一个时间点的移动方向$v^2$，是和当前时间节点的移动方向和梯度$v^1,\Delta L(\theta^1)$决定的，即</p>
<script type="math/tex; mode=display">
\begin{aligned}
v^2&=\lambda v^1-\eta \Delta L(\theta^1)\\
&=-\lambda\eta \Delta L(\theta^0)-\eta \Delta L(\theta^1)
\end{aligned}</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607154125395.png" alt="image-20200607154125395" style="zoom:50%;"></p>
<p>再回到之前的例子，红色箭头表示gradient的反方向，绿色表示momentum的方向，蓝色箭头表示受到gradient和momentum影响后的真实运动方向</p>
<p>初始点的momentum值为0；在下一个plateau上的点，虽然gradient的值很小很小，但由于受到上一个很大的momentum的影响，真实的movement还是向前的，步长也没有因为gradient的变小而变得很小；</p>
<p>如果我们现在走到了local minimum，此时gradient=0，此时由于momentum的影响，如果momentum的值足够大，还会继续向前运动</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607161327324.png" alt="image-20200607161327324" style="zoom:50%;"></p>
<h5 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h5><blockquote>
<p>RMSProp + Momentum</p>
<p>Adam其实就是结合了RMSProp和Momentum思想的方法</p>
</blockquote>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607162646245.png" alt="image-20200607162646245" style="zoom: 67%;"></p>
<p>先将动量momentum和初始移动方向初始化为0，即$m_0=0,v_0=0$，$v_0$表示RMSProp中分母上的参数$\sigma$</p>
<p>计算在t时的梯度$g_t$，</p>
<script type="math/tex; mode=display">
g_t=\Delta_\theta f_t(\theta_{\theta-1})</script><p>根据上一个时间点的要走的方向$m_t$和gradient，因此t时的移动方向为$m_t$———Momentum</p>
<script type="math/tex; mode=display">
m_t=\beta_1\cdot m_{t-1}+(1-\beta_1)\cdot g_t</script><p>根据上一个时间点的移动方向$v_{t-1}$和gradient，则此时的真实移动方向$v_t$为———-RMSprop</p>
<script type="math/tex; mode=display">
v_t=\beta_1\cdot v_{t-1}+(1-\beta_2)\cdot g_t</script><p>该算法还进行了bias corrected，</p>
<script type="math/tex; mode=display">
\hat m_t=\frac{m_t }{(1-\beta_1^t)},\quad
\hat v_t=\frac{v_t}{(1-\beta_2^t)}</script><p>将进行了bias corrected的参数再输入公式，更新参数</p>
<script type="math/tex; mode=display">
\theta_t=\theta_{t-1} -\alpha \cdot \hat m _t/\sqrt{\hat v _t}+\epsilon</script><h3 id="Good-Results-on-Testing-Data"><a href="#Good-Results-on-Testing-Data" class="headerlink" title="Good Results on Testing Data?"></a>Good Results on Testing Data?</h3><h4 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607163018382.png" alt="image-20200607163018382" style="zoom:50%;"></p>
<h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><h5 id="L2-Regularization"><a href="#L2-Regularization" class="headerlink" title="L2 Regularization"></a>L2 Regularization</h5><p>正则化就引入了一个新的loss function，加上了一个新的正则项，这个正则项将所有需要training的参数都包括进来了，通常不包括bias</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607163135769.png" alt="image-20200607163135769" style="zoom:50%;"></p>
<p>这个新的loss function再对w求偏微分，对参数进行更新</p>
<script type="math/tex; mode=display">
w^{t+1}=(1-\eta\lambda)w^t-\eta \frac{\partial L}{\partial w}</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607163452933.png" alt="image-20200607163452933" style="zoom:50%;"></p>
<p>与原来的参数更新公式相比，可以发现$w^t$前面多了一项$(1-\eta\lambda)$，通常这个$\eta,\lambda$都是很小的值，这里我们假设$(1-\eta\lambda)$是很接近于1的值，约等于0.99; regularization所做的事就是，在每次更新参数时，全都在前面乘上了一个小于1的数，在经过若干次的训练之后，$(1-\eta\lambda)w^t$的值就很接近0了</p>
<p>虽然$(1-\eta\lambda)w^t$的值每次都会变得越来越小，但参数更新的公式中，后面还有另外一项$\eta \frac{\partial L}{\partial w}$，会使得梯度的值不会变成0，达到平衡</p>
<h5 id="L1-Regularization"><a href="#L1-Regularization" class="headerlink" title="L1 Regularization"></a>L1 Regularization</h5><p>既然L2可以作为正则项，L1也可以作为正则项，正则项为参数的绝对值相加。对这些参数求导，当$w_i$大于0时，gradient=1，当$w_i$小雨0时，gradient=-1，即为$sgn(w)$函数</p>
<p>L1的参数更新公式为</p>
<script type="math/tex; mode=display">
w^{t+1}=w^t-\eta \frac{\partial L}{\partial w} - \eta\lambda\ \rm{sgn}(w^t)</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607171442670.png" alt="image-20200607171442670" style="zoom:50%;"></p>
<p>与原来的参数更新公式相比较，可以发现后面多了一项$- \eta\lambda\ \rm{sgn}(w^t)$，表示参数在原来的基础上都要减去一个小于1的数</p>
<h5 id="L1-vs-L2"><a href="#L1-vs-L2" class="headerlink" title="L1 vs L2"></a>L1 vs L2</h5><p>参数更新公式分别如下，</p>
<script type="math/tex; mode=display">
L1:\ w^{t+1}=w^t-\eta \frac{\partial L}{\partial w} - \eta\lambda\ \rm{sgn}(w^t)\\
L2:\ w^{t+1}=(1-\eta\lambda)w^t-\eta \frac{\partial L}{\partial w}</script><ul>
<li>L1参数更新公式每次都多<strong>减去了一个小于1的固定值（constant）</strong></li>
<li>L2参数更新公式中每次都将<strong>前一次的参数乘上一个小于1的值</strong></li>
</ul>
<h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173403475.png" alt="image-20200607173403475" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173419426.png" alt="image-20200607173419426" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173455150.png" alt="image-20200607173455150" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173629804.png" alt="image-20200607173629804" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173828093.png" alt="image-20200607173828093" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173729715.png" alt="image-20200607173729715" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173943611.png" alt="image-20200607173943611" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>DNN</tag>
        <tag>Momentum</tag>
        <tag>Adam</tag>
        <tag>Regularization</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Autoencoder</title>
    <url>/2020/06/29/Deep-autoencoder/</url>
    <content><![CDATA[<h4 id="Auto-encoder"><a href="#Auto-encoder" class="headerlink" title="Auto-encoder"></a>Auto-encoder</h4><p>对于输入大小为$28\times28$的图像，先经过encoder进行编码，得到的结果通常比784维要小，code表示比原来的image更加精简（compact）的特征；</p>
<p>但现在我们进行的是unsupervised learning，这个code到底长什么样子也不知道；</p>
<p>那么我们现在就先来做一个decoder，把code恢复成一张image；</p>
<p>decoder和encoder单独是没办法训练的，因为是无监督学习，因此现在我们将这两者一起学习</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629094326636.png" alt="image-20200629094326636" style="zoom:50%;"></p>
<h4 id="Recap-PCA"><a href="#Recap-PCA" class="headerlink" title="Recap: PCA"></a>Recap: PCA</h4><p>这里先回顾一下<a href="https://scarleatt.github.io/2020/06/28/dimension-reduction/" target="_blank" rel="noopener">PCA</a>，对于输入的图像$x$为$784$维，经过PCA降维，可以降低到324维的图像$\hat x$，在encoder和decoder的学习过程中，应达到的目标是最小化$(x-\hat x)^2$，这个结构可以看成是具有一个hidden layer的network</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629100232021.png" alt="image-20200629100232021" style="zoom:50%;"></p>
<h4 id="Deep-Auto-encoder"><a href="#Deep-Auto-encoder" class="headerlink" title="Deep Auto-encoder"></a>Deep Auto-encoder</h4><h5 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h5><p>中间的hidden layer也可以不止一个，也可以是多个的hidden layer，这种结构就称作deep auto-encoder</p>
<p><span style="color: red">注：encoder和decoder的结构不一定非得是对称的。</span></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629101531636.png" alt="image-20200629101531636" style="zoom:50%;"></p>
<h5 id="PCA-vs-Deep-Auto-encoder"><a href="#PCA-vs-Deep-Auto-encoder" class="headerlink" title="PCA. vs Deep Auto-encoder"></a>PCA. vs Deep Auto-encoder</h5><p>对于下图中的图像(0,1,2,3,4)，如果我们使用PCA，只有中间一个hidden layer，先从784维降到30维，再从30维恢复到784维，可以发现图像变得比较模糊；</p>
<p>如果使用deep auto-encoder，先从784到1000，1000到500，500到250，250到300，再使用类似的encoder恢复图像，可以发现结果图像非常清晰</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629101748853.png" alt="image-20200629101748853" style="zoom:50%;"></p>
<p>如果我们先使用PCA进行降维，把原图从784降到2维，对二维的数据进行可视化，可以发现不同的digit（不同的颜色代表不同的数字）都叠在一起了；</p>
<p>如果使用deep autoencoder，可以发现这个数字都是分开的</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629102817764.png" alt="image-20200629102817764" style="zoom:50%;"></p>
<h4 id="Text-Retrieval"><a href="#Text-Retrieval" class="headerlink" title="Text Retrieval"></a>Text Retrieval</h4><p>下图中蓝色的圆点都表示一个document，我们将输入的query也加入这个document，再计算查询的词汇query和每个document之间的inner product或similarity等，距离最近的document，similarity的值是最大的，因此会retrieval距离红色箭头最近的其他两个蓝色箭头</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629103259176.png" alt="image-20200629103259176" style="zoom:50%;"></p>
<p>还有另外一种方法，使用一个vector来表示所有的词汇，vector对应的值就是每个character出现的次数；但这种方式没有考虑原来的语义顺序，每个character都是independent的</p>
<p>在下图中，假设bag里面有2000个词汇，把输入的document或query变成对应的vector，再输入相应的encoder，降维成2维，对这二维的数据进行可视化，如右图所示，不同的颜色代表不同的document；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629104843900.png" alt="image-20200629104843900" style="zoom:50%;"></p>
<p>如果我们现在输入一个query，再降维到二维，可以发现和图中对应的document是有关的，我们就可以看到query所属的类别是Energy market</p>
<p>但用LSA就得不到对应的结果</p>
<h4 id="Similar-Image-Search"><a href="#Similar-Image-Search" class="headerlink" title="Similar Image Search"></a>Similar Image Search</h4><p>以图找图，如果我们只是做pixel上的相似程度，那么我们可以得到以下的结果，迈克杰克逊和马蹄铁也很像，这显然不符合常理</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629110009994.png" alt="image-20200629110009994" style="zoom:50%;"></p>
<p>我们可以把输入的image经过deep auto-encoder，变成一个code，再去做搜寻；由于auto-encoder是unsupervised learning，收集多少数据都行，不缺数据</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629110134069.png" alt="image-20200629110134069" style="zoom:50%;"></p>
<p>用deep auto-encoder来找类似迈克杰克逊，也可以得到更好的结果（至少全是人脸 ）</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629110527622.png" alt="image-20200629110527622" style="zoom:50%;"></p>
<h4 id="Pre-training-DNN"><a href="#Pre-training-DNN" class="headerlink" title="Pre-training DNN"></a>Pre-training DNN</h4><p>autoencoder也可以用到network的pre-training过程，来初始化所需要的weight；在下图中，如果我们要对第一个hidden layer的weight进行初始化，那么我们可以使用autoencoder，先将784维到1000维，再进行reconstruct，使1000维降到784维，来使$x,\hat x$之间的差值最小</p>
<p>但这样可能会出现一个问题，由于1000维比784维更高维，autoencoder很可能将input直接embed到1000维里，再进行恢复，这样很可能network什么都没有学到；</p>
<p>对于这个问题，我们可以把1000维再加上一个regularization，比如L1，这1000维的数据中有某些维必须是为0的，这样就可以避免autoencoder直接将input进行embed</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629110742780.png" alt="image-20200629110742780" style="zoom:50%;"></p>
<p>我们可以学习这样的一个autoencoder，先将input转化成一个1000维的vector，再把这个vector转化为<span style="color: green">1000维的code</span>，再把这个code转化为1000维的vector；使input和output越接近越好，<strong>把$W^2$的值保存下来</strong>，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629111953318.png" alt="image-20200629111953318" style="zoom:50%;"></p>
<p>再继续下一个layer，使用第三个autoencoder</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629112400780.png" alt="image-20200629112400780" style="zoom:50%;"></p>
<p>学习到$W^1,W^2,W^3$之后，就把这个weight作为初始值，$W^4$则进行random init，再使用<span style="color: red">back propagation</span>进行fine-tune</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629112513885.png" alt="image-20200629112513885" style="zoom:50%;"></p>
<p><span style="color: red">如果我们有大量的unlabeled data，labeled data只有很小一部分，那么我们就可以通过这种无监督学习的方式来初始化weight，再使用剩下的label data来对network进行fine-tune即可</span></p>
<h4 id="De-noising-auto-encoder"><a href="#De-noising-auto-encoder" class="headerlink" title="De-noising auto-encoder"></a>De-noising auto-encoder</h4><p>对于原来的input x，我们先加入一些noise得到$x’$，进行encode之后再进行decode，使之和最早的input之间的差值最小；encoder现在不仅学习到了encode这件事，还学习到了把noise过滤掉这件事</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629113151447.png" alt="image-20200629113151447" style="zoom:50%;"></p>
<h4 id="Auto-encoder-for-CNN"><a href="#Auto-encoder-for-CNN" class="headerlink" title="Auto-encoder for CNN"></a>Auto-encoder for CNN</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629114519998.png" alt="image-20200629114519998" style="zoom:50%;"></p>
<h5 id="CNN-Unpooling"><a href="#CNN-Unpooling" class="headerlink" title="CNN -Unpooling"></a>CNN -Unpooling</h5><p>在pooling时，会选择四个方框中的最大值，并记住最大值的location；</p>
<p>在upooling时，就会用到上面的location</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629114730398.png" alt="image-20200629114730398" style="zoom:50%;"></p>
<h5 id="Deconvolution"><a href="#Deconvolution" class="headerlink" title="Deconvolution"></a>Deconvolution</h5><p>Deconvolution其实就是在做convolution，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629114919151.png" alt="image-20200629114919151" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Unsupervised Learning</tag>
        <tag>Deep Autoencoder</tag>
        <tag>Autoencoder</tag>
      </tags>
  </entry>
  <entry>
    <title>Explainable Machine Learning</title>
    <url>/2020/06/12/Explainable-AI/</url>
    <content><![CDATA[<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>机器不仅要告诉我们结果cat，还要告诉我们为什么 </p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611112308714.png" alt="image-20200611112308714" style="zoom:50%;"></p>
<h5 id="Why-we-need-Explainable-ML"><a href="#Why-we-need-Explainable-ML" class="headerlink" title="Why we need Explainable ML?"></a>Why we need Explainable ML?</h5><p><u>我们不仅需要机器结果的精确度，还需要进行模型诊断，看机器学习得怎么样；有的任务精确度很高，但实际上机器什么都没学到</u></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611112911240.png" alt="image-20200611112911240" style="zoom:50%;"></p>
<p>有模型诊断后，我们就可以根据模型诊断的结果再来调整我们的模型</p>
<h5 id="Interpretable-v-s-Powerful"><a href="#Interpretable-v-s-Powerful" class="headerlink" title="Interpretable v.s. Powerful"></a>Interpretable v.s. Powerful</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611195115774.png" alt="image-20200611195115774" style="zoom:50%;"></p>
<p>那么有没有model是Interpretable，也是powerful的呢 ？</p>
<p>决策树可以interpretable，也是比较powerful的；对于第一个分支节点，“这些动物呼吸空气吗？”，就包含了interpretable的信息</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611195220007.png" alt="image-20200611195220007" style="zoom:50%;"></p>
<p>当分支特别多的时候，决策树的表现也会很差</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611195426930.png" alt="image-20200611195426930" style="zoom:50%;"></p>
<h4 id="Local-Explanation"><a href="#Local-Explanation" class="headerlink" title="Local Explanation"></a>Local Explanation</h4><h5 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h5><p>对于输入的x，我们将其分成components $\{x_1,…,x_n,…x_N\}$，每个component由一个像素，或者一小块组成</p>
<p>我们现在的目标是知道每个component对making the decision的重要性有多少，那么我们可以通过remove或者modify其中一个component的值，看此时的decision会有什么变化</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611200230303.png" alt="image-20200611200230303" style="zoom:50%;"></p>
<p>把灰色方块放到图像中，覆盖图像的一小部分；如果我们把灰色方块放到下图中的红色区域，那么对解释的结果影响不大，第一幅图还是一只狗</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611200829405.png" alt="image-20200611200829405" style="zoom:50%;"></p>
<p>还有另一种方法</p>
<p>对于输入的$\{x_1,…,x_n,..,x_N\}$，对于其中的某个关键的pixel $x_n$加上$\Delta x$，这个pixel对我们识别这是不是一只狗具有很重要的作用</p>
<p>那么我们就可以用$\frac{\Delta y}{\Delta x}$来表示这个小小的扰动对y的影响，可以通过$\frac{\partial y_k}{\partial x_n}$来进行计算，表示$y_k$对$x_n$的偏微分，最后取绝对值，表示某一个pixel对现在y影响的大小</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611214135944.png" alt="image-20200611214135944" style="zoom:50%;"></p>
<p>在上图中，下半部分由3幅图saliency map，亮度越大，绝对值就越大，亮度越大的地方就表示该pixel对结果的影响越大</p>
<h5 id="Limitation-of-Gradient-based-Approaches"><a href="#Limitation-of-Gradient-based-Approaches" class="headerlink" title="Limitation of Gradient based Approaches"></a>Limitation of Gradient based Approaches</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611220206992.png" alt="image-20200611220206992" style="zoom:50%;"></p>
<h5 id="Attack-Interpretation"><a href="#Attack-Interpretation" class="headerlink" title="Attack Interpretation"></a>Attack Interpretation</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611221032938.png" alt="image-20200611221032938" style="zoom:50%;"></p>
<h4 id="Global-Explanation"><a href="#Global-Explanation" class="headerlink" title="Global Explanation"></a>Global Explanation</h4><p>Interprete the whole Model</p>
<h5 id="Activation-Minimization-review"><a href="#Activation-Minimization-review" class="headerlink" title="Activation Minimization (review)"></a><strong>Activation Minimization</strong> (review)</h5><p>让我们先review一下activation minimization，现在我们的目标是找到一个$x^*$，使得输出的值$y_i$最大</p>
<p>我们可以加入一些噪声，加上噪声后人并不能识别出来，但机器可以识别出来，看出来下图中的噪声是0 1 2 3 4 5 6 7 8</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611221428166.png" alt="image-20200611221428166" style="zoom:50%;"></p>
<p>之前我们的目标是找到一个image，使得输出的y达到最大值；现在我们的目标不仅是找到x使输出y达到最大值，还需要把image变得更像是一个digit，不像左边那个图，几乎全部的像素点都是白色，右边的图只有和输出的digit相关的pixel才是白色</p>
<p>这里我们通过加入了一个新的限制$R(x)$来实现，可以表示图像和digit的相似度</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611223152859.png" alt="image-20200611223152859" style="zoom:50%;"></p>
<h5 id="Constraint-from-Generator"><a href="#Constraint-from-Generator" class="headerlink" title="Constraint from Generator"></a>Constraint from Generator</h5><p>如下图所示，我们输入一个低维的vector z到generator里面，输出Image x；</p>
<p>现在我们将生成的Image x再输入Image classifier，输出分类结果$y_i$，那么我们现在的目标就是找到$z^*$，使得属于那个类别的可能性$y_i$最大</p>
<script type="math/tex; mode=display">
z^*=arg \ max y_i</script><p>找到最好的$z^<em>$，再输入Generator，根据$x^</em>=G(z^<em>)$得出$x^</em>$，产生一个好的Image</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200612105833703.png" alt="image-20200612105833703" style="zoom:50%;"></p>
<p>结果展示。现在你问机器蚂蚁长什么样子呢？机器就会给你画一堆蚂蚁的图片出来，再放到classifier里面，得出分类结果到底是火山还是蚂蚁</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200612113355921.png" alt="image-20200612113355921" style="zoom:50%;"></p>
<h4 id="Using-a-model-to-explain-another"><a href="#Using-a-model-to-explain-another" class="headerlink" title="Using a model to explain another"></a>Using a model to explain another</h4><p>现在我们使用一个interpretable model来模仿另外一个uninterpretable model；下图中的Black Box为uninterpretable model，比如Neural Network，蓝色方框是一个interpretable model，比如Linear model；现在我们的目标是使用相同的输入$x^1,x^2,…,x^N$，使linear model和Neural Network有相近的输出</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200612155736839.png" alt="image-20200612155736839" style="zoom:50%;"></p>
<p>实际上并不能使用linear model来模拟整个neural network，但可以用来模拟其中一个local region</p>
<h5 id="Local-Interpretable-Model-Agnostic-Explanations-LIME"><a href="#Local-Interpretable-Model-Agnostic-Explanations-LIME" class="headerlink" title="Local Interpretable Model-Agnostic Explanations (LIME)"></a>Local Interpretable Model-Agnostic Explanations (LIME)</h5><p><strong>General</strong></p>
<p>下图中input为x，output为y，都是一维的，表示Black Box中x和y的关系，由于我们并不能用linear model来模拟整个neural network，但可以用来模拟其中一个local region</p>
<ol>
<li><p>首先给出要explain的point，代入black box里面</p>
</li>
<li><p>在第三个蓝色point（我们想要模拟的区域）周围sample附近的point，nearby的区域不同，结果也会不同</p>
</li>
<li><p>使用linear model来模拟neural network在这个区域的行为</p>
</li>
<li><p>得知了该区域的linear model之后，我们就可以知道在该区域x和y的关系，即x越大，y越小，也就interpret了原来的neural network在这部分区域的行为</p>
</li>
</ol>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200612161607923.png" alt="image-20200612161607923" style="zoom:50%;"></p>
<p>那么到底什么算是nearby呢？<u>用不同的方法进行sample，结果不太一样。</u>对于下图中的region，可以看到离第三个蓝色point的距离很远，取得的效果就非常不好了</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200612163829515.png" alt="image-20200612163829515" style="zoom:50%;"></p>
<p><strong>LIME-Image</strong></p>
<p>刚才说了general的情况，下面我们讲解LIME应用于image的情况</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200612164116470.png" alt="image-20200612164116470" style="zoom:50%;"></p>
<ol>
<li>首先需要一张需要解释的image；为什么这张图片可以被classify为树蛙？</li>
<li>sample at the nearby：首先把image分成多个segment，再随机去掉图中的一些segment，就得到了不同的新图片，这些新的图片就是sample的结果；再把这些新生成的图片输入black box，得到新图片是frog的可能性；</li>
<li>fit with linear model：即找到一个linear model来fit第3步输出的结果；先extract生成的新图片的特征，再把这些特征输入linear model；</li>
</ol>
<p>Q：那么如何将image转化为一个vector呢？</p>
<p>A：这里我们将image中的每个segment使用$x_i$来表示，其中$i=1,…,m,…,M$，M为segment的数量；$x_i$为1，表示当前segment被deleted，如果为0，表示exist；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200612165520599.png" alt="image-20200612165520599" style="zoom:50%;"></p>
<p>4.$\ $Interpret the model：对于学习出来的linear model，我们就可以对其进行interpret；首先需要将$x_i$和y的关系用一个公式表示出来，即</p>
<script type="math/tex; mode=display">
y=w_1x_1+..+w_mx_m+...+w_Mx_M</script><p>对于$w_m$的值，有以下三种情况：</p>
<ul>
<li>$w_m\approx 0$，segment $x_m$被认为对分类为frog没有影响；</li>
<li>$w_m&gt; 0$， $x_m$对图片分类为frog是有正面的影响的；</li>
<li>$w_m&lt;0$， 看到这个segment，反而会让机器认为图片不是frog</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200612170336857.png" alt="image-20200612170336857" style="zoom:50%;"></p>
<h5 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h5><p>如果我们用不限制深度的decision tree，那么我们就可以使用decision tree来模拟black box（neural network），使两者的输出相近</p>
<p>但decision tree的深度不可能是没有限制的。这里我们设neural network的参数为$\theta$，decision tree的参数为$T_\theta$，使用$O(T_\theta)$来表示$T_\theta$的复杂度，复杂度可以用$T_\theta$的深度来表示，也可以用neural的个数来表示；<u>现在我们的目标不仅是使两者输出相近，还需要使$O(T_\theta)$的值最小化</u></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200612171916382.png" alt="image-20200612171916382" style="zoom:50%;"></p>
<p>那么我们如何来实现使$O(T_\theta)$越小越好呢？</p>
<p>如下图所示，我们首先训练一个network，这个network可以很容易地被decision tree解释，使decision tree的复杂度没有那么高；这里我们加入了一个正则项$\lambda O(T_\theta)$，在训练network的同时，不仅要最小化loss function，还需要使$O(T_\theta)$的值尽量小，这时需要找到的network参数为$\theta^*$，</p>
<script type="math/tex; mode=display">
\theta^*=arg\ {\rm min}\ L(\theta) + \lambda O(T_\theta)</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200612172838929.png" alt="image-20200612172838929" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>explainable machine learning</tag>
        <tag>LIME</tag>
      </tags>
  </entry>
  <entry>
    <title>Intelligent Photo Editing</title>
    <url>/2020/07/15/GAN-Intelligent-Photo-Editing/</url>
    <content><![CDATA[<p>本文主要介绍了使用GAN来进行智能图像编辑，包括编辑原图像，Image super resolution和Image Completion等。</p>
<h4 id="Modifying-Input-Code"><a href="#Modifying-Input-Code" class="headerlink" title="Modifying Input Code"></a>Modifying Input Code</h4><p>generator的输入是一个vector，输出一张人脸。input的vector的每个维度都表示某种特征，我们现在要做的就是根据生成的图像，来反推出输入vector的每个dimension表示什么特征。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200715094805474.png" alt="image-20200715094805474" style="zoom:67%;"></p>
<h4 id="GAN-Autoencoder"><a href="#GAN-Autoencoder" class="headerlink" title="GAN+Autoencoder"></a>GAN+Autoencoder</h4><p>我们可以收集一堆有label的数据，这个label可以表示这个图像具有的特征是金发、年轻、男性、女性等。</p>
<p>但现在有一个问题，根据给出的image，我们并不知道其输入的随机vector是什么。我们可以学习另外一个encoder，这个encoder和generator就组成了一个autoencoder。</p>
<p>为了学习出输入的vector，我们需要固定generator的参数，可以用discriminator的参数来初始化encoder的参数，再来学习encoder。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200715095135708.png" alt="image-20200715095135708" style="zoom:67%;"></p>
<h4 id="Attribute-Representation"><a href="#Attribute-Representation" class="headerlink" title="Attribute Representation"></a>Attribute Representation</h4><p>encoder训练完成后，我们就可以知道图像x对应的vector z是什么，什么样的vector可以生成这张图像。</p>
<p>在下图中，我们算出每个短发图像的vector之后，再做一个平均，就可以知道短发图像的vector；把长发图像的vector做一个平均，就可以得到长发图像的vector。把这两个vector相减得到$z_{long}$，就可以知道要做什么样的变化，使短发的脸变成长发的脸。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200715100537911.png" alt="image-20200715100537911" style="zoom:67%;"></p>
<p>现在我们有一张短发的图像x，把它输入encoder得到code，把这个code和$z_{long}$相加，就可以得到长发脸的vector $z’$，把这个vector再输入generator，就可以得到长发脸的图像$Gen(z’)$。</p>
<p>那么现在对于随机输入的一张图像，我们把其code再加上我们想要的特征向量(比如$z_{long}$)，就可以得到我们想要的特征图的输出。</p>
<p>智能的ps演示，<a href="https://www.youtube.com/watch?v=9c4z6YsBGQ0" target="_blank" rel="noopener">https://www.youtube.com/watch?v=9c4z6YsBGQ0</a></p>
<p>那么这种智能的ps是怎么作用的呢？</p>
<h4 id="Photo-Editing"><a href="#Photo-Editing" class="headerlink" title="Photo Editing"></a><strong>Photo Editing</strong></h4><h5 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h5><p>首先需要训练一个generator，在不同的latent space上sample出不同的点，generator就可以生成对应的商品。</p>
<p>那么刚才那个智能的ps中，对原图进行小小的修改就可以变成一件新的商品，这个是怎么完成的呢？</p>
<p>首先需要对输入的商品图像进行反推，反推出在这个code space上的位置，在这个基础上，再进行变化。在满足使用者给出的constrain后，就可以产生一张新的图。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200715102116230.png" alt="image-20200715102116230" style="zoom:67%;"></p>
<h5 id="Back-to-z"><a href="#Back-to-z" class="headerlink" title="Back to z"></a>Back to z</h5><p>我们首先需要完成的事，根据输入的图像反推出对应的code。有以下三种方式。</p>
<p><strong>Method 1</strong>  当成一个optimization problem，找到对应的$z^*$，使其对应的generator的输出和原图之间的差值最小化，loss function可以有很多种，可以是计算像素级的差值，也可以使用其他的classifier网络，让这两者的embedding越接近越好。</p>
<script type="math/tex; mode=display">
z^*=arg\mathop{\rm min}_zL(G(z),x^T)</script><p><strong>Method 2</strong> 使用一个autoencoder ，使input和output越接近越好。</p>
<p><strong>Method 3</strong> 结合了前两个方法，第一个方法要用到gradient descent，可能会遇到local minimal这个问题，给z不同的初始化值，最后得到的结果很可能是不一样的。因此我们可以用autoencoder得到的z来作为方法1的初始值，再来进行优化调整。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200715103603424.png" alt="image-20200715103603424" style="zoom:67%;"></p>
<h5 id="Editing-Photos"><a href="#Editing-Photos" class="headerlink" title="Editing Photos"></a>Editing Photos</h5><p>从图像反推出z之后，我们还需要求解另外一个optimization problem。</p>
<p>把z输入generator之后，产生image的还需要满足一个额外的constrain $U(G(z))$；新的图像和原来的图像应该越接近越好，原来是黑色的鞋子，现在我们希望新图像还是鞋子，不能差太多；discriminator还需要检测生成的新图像是不是realistic；</p>
<script type="math/tex; mode=display">
z^*=arg\mathop{\rm min}_z U(G(z))+\lambda_1||z-z_0||^2-\lambda_2D(G(z))</script><p>求解这个optimization problem，找到满足这些constrain的$z^*$。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200715104644372.png" alt="image-20200715104644372" style="zoom:67%;"></p>
<h4 id="Image-super-resolution"><a href="#Image-super-resolution" class="headerlink" title="Image super resolution"></a>Image super resolution</h4><p>GAN也可以进行Image super resolution，输入一张模糊的图，输出一张清晰的图。可以发现使用GAN的结果，更加清晰了。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200715102031232.png" alt="image-20200715102031232" style="zoom:67%;"></p>
<h4 id="Image-Completion"><a href="#Image-Completion" class="headerlink" title="Image Completion"></a>Image Completion</h4><p>我们可以把图像的某部分挖空，机器要来恢复这部分被挖空的图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200715105653264.png" alt="image-20200715105653264" style="zoom:67%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>GAN</tag>
        <tag>Autoencoder</tag>
        <tag>Intelligent Photo Editing</tag>
      </tags>
  </entry>
  <entry>
    <title>Evaluation of Generative Models</title>
    <url>/2020/07/17/GAN-evalution/</url>
    <content><![CDATA[<p>本文主要叙述了几种评价GAN生成结果好坏的方法。首先叙述了使用传统的likelihood，由于传统的likelihood方法有一些局限性，本文接着使用了Inception Score的思想，来对GAN进行评价。</p>
<h4 id="Likelihood"><a href="#Likelihood" class="headerlink" title="Likelihood"></a>Likelihood</h4><p>对于传统的评价generator生成结果的方法，是计算每个结果产生的likelihood。</p>
<ul>
<li>先从真实数据的distribution中sample出$\{x^1,x^2,…x^m\}$；</li>
<li>把$x^i$代入现在的已知的distribution $P_G(x^i;\theta)$，表示$x^i$是从现在这个distribution中sample出来的概率；</li>
<li>把这些概率相乘，得到似然函数L；最后找到对应的参数$\theta$，使似然函数取得最大值。</li>
</ul>
<script type="math/tex; mode=display">
L=\frac{1}{N}\sum_{i}logP_G(x^i;\theta)</script><p>这个likelihood就可以用来评价generator的好坏，如果likelihood很高，表示这个generator有很高的几率可以产生真实数据，就是一个好的generator。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200717141511915.png" alt="image-20200717141511915" style="zoom:67%;"></p>
<p>现在会遇到一个新问题。我们没有办法计算$P_G(x^i)$，训练好的generator就可以看作是一个network，这个network可以实现输入一个vector，然后输出一张图像，但没有办法算出generator产生某个特定图像的几率。generator虽然可以产生图像，但没有办法产生特定的图像。</p>
<p>如果generator是一个gaussian mixture model，我们就可以算出这个likelihood；但现在是复杂的模型，并不能计算likelihood；</p>
<h5 id="Likelihood-Kernel-Density-Estimation"><a href="#Likelihood-Kernel-Density-Estimation" class="headerlink" title="Likelihood - Kernel Density Estimation"></a>Likelihood - Kernel Density Estimation</h5><p>我们可以尝试 Kernel Density Estimation来计算likelihood。</p>
<p>首先让generator产生很多data，然后我们再使用多个gaussian distribution来逼近这些data。如果现在是图像问题，先让generator生成很多图像（高维的vector），把这些vector当作gaussian model的mean，每个mean都有一个固定的variance；把这些gaussian model都融合起来，就得到了一个gaussian mixture model;</p>
<p><a href="https://scarleatt.github.io/2020/06/29/generative-models/" target="_blank" rel="noopener"><strong>Gaussian Mixture Model</strong>：</a>如果现在有m个gaussian model，其中$P(m)$表示选择该对应model的可能性，$P(x|m)$表示在选定了第k个model后，该mode产生x的概率，即</p>
<script type="math/tex; mode=display">
P(x)=\sum_mP(m)P(x|m)</script><p>那么现在我们就有了分布$P_G$的近似分布$P(x)$，那么我们就可以计算$P_G$产生真实数据$x^i$的概率$P_G(x^i)$了，我们也可以继续计算likelihood。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200717152812318.png" alt="image-20200717152812318" style="zoom:67%;"></p>
<h5 id="Likelihood-v-s-Quality"><a href="#Likelihood-v-s-Quality" class="headerlink" title="Likelihood v.s. Quality"></a>Likelihood v.s. Quality</h5><p>就算我们真的算出了这个likelihood，这个likelihood也并不一定就代表了generator的quality，很可能会出现以下两种情况：</p>
<ul>
<li><p>low likelihood，很可能产生high quality的图像。在下图中，很可能generator生成了很接近真实图像的动漫头像，但这时我们要来计算likelihood，生成图像和真实图像很可能差别很大，那么generator生成数据库中的真实图像的可能性就很小，即$P_G(x^i)=0$；</p>
</li>
<li><p>high likelihood，很可能产生low quality的图像。下图中有两个generator，算出来的likelihood值分别为$L_1,L_2$。generator 1生成的图像质量很高，且和数据库中真实的图像也很接近，我们可以先算出这个likelihood，</p>
</li>
</ul>
<script type="math/tex; mode=display">
L_1=\frac{1}{N}\sum_{i}logP_G(x^i)</script><p>对于generator 2生成的图像，有0.99的概率生成模糊的图像，有0.01的概率生成真实的清晰图像。相对于generator 1而言，$P_G(x^i)$减小了100倍，</p>
<script type="math/tex; mode=display">
L_2=\frac{1}{N}\sum_{i}log\frac{P_G(x^i)}{100}=-log100+\frac{1}{N}\sum_{i}logP_G(x^i)</script><p>其中$log100$的值接近4.6，但likelihood的值算出来的值一般都是几百的，generator 1和2的likelihood算出来其实差别不大。但其实这两个generator的差别很大，generator 1比2要好一百倍，只是likelihood的值算出来差别不大而已。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200717152908621.png" alt="image-20200717152908621" style="zoom:67%;"></p>
<p>因此，likelihood的值与generator的好坏其实是没有必然联系的。</p>
<h4 id="Objective-Evaluation"><a href="#Objective-Evaluation" class="headerlink" title="Objective Evaluation"></a>Objective Evaluation</h4><p>我们可以用一个训练好的classifier来判断生成结果的好坏，加入我们现在在做人脸生成问题，我们就可以用人脸识别程序来判断，如果能进行识别，就说明生成的结果还不错。这个classifier是训练好的，比如VGG。</p>
<p>在下图中，generator生成了一张图像x，我们把x输入这个classifier，会输出对应类别的distribution，即x属于class 1，2，3的概率。如果这个分布产生的概率越集中（某个类别概率很大，其他类别概率很小），就表示generator产生的图像品质很高，classifier可以很自信地进行分类；</p>
<p>很可能generator就只能产生那个类别的清晰图像而已，产生的其他类别图像质量都很差，这并不是我们想要的结果。因此，我们不仅仅衡量输出概率的distribution，还需要从diverse的方向来衡量；</p>
<p>那么什么叫从diverse的方向来衡量呢？</p>
<p>现在让generator生成一堆图像，这里用三张不同的图像作为例子，把这些图像输入CNN，得到三个不同的distribution，再分别求三个distribution中属于同一类别概率的平均值。如果平均值的分布比较uniform，就表示每一种class都有可能被产生，就表示是diverse的；如果平均完之后的值有的很大，是某一个class的概率特别高，model就比较倾向于产生这个类别的事物，就说明output不够diverse。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200717155627711.png" alt="image-20200717155627711" style="zoom:67%;"></p>
<p>现在对于我们的模型就有两个要求，</p>
<ul>
<li><strong>图片质量</strong>：classifier得出的distribution要足够sharp、集中，classifier可以很自信地确定x是属于哪个类别；这可以用条件概率$P(y|x)$来表示，越大越好；</li>
<li><strong>图片多样性</strong>：生成的图像还要足够diverse，标签的分布要均匀，因为我们不希望生成的图像都是属于某一个类别的；这可以用$p(y)$来表示。</li>
</ul>
<p>有了这些限制之后，就可以定义对应的score，我们这里使用的是inception network来进行evaluation，对应的分数是inception score，</p>
<script type="math/tex; mode=display">
{\rm Inception\ score}=\sum_x\sum_yP(y|x)logP(y|x)-\sum_yP(y)logP(y)</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200717182906616.png" alt="image-20200717182906616" style="zoom:67%;"></p>
<h4 id="We-don’t-want-memory-GAN"><a href="#We-don’t-want-memory-GAN" class="headerlink" title="We don’t want memory GAN."></a>We don’t want memory GAN.</h4><p>在训练GAN的时候，还有另外一个问题，有时候计算generator生成的图像特别清晰，这个结果也不见得是好的。因为很可能generator只是记住了training data中的某几张图像而已，而我们希望生成的图像不是database里面的，而是有创造性的图像。</p>
<p>那么我们怎么知道生成的图像是不是database里面的呢？要一张一张去对比吗？</p>
<p>我们可以在generator每生成一张图像的时候，都自动和database里面的每张图像进行对比，可以进行pixel-level的相似度对比；</p>
<p>但只进行pixel-level的相似度对比是远远不够的。在下图中，有一只羊的图像，每一条曲线都代表一张图像，纵坐标表示database中的图和羊的近似程度，横坐标表示把羊图像向左移动的pixel数目。如果往左边移动一个pixel，发现这张图像还是和原来的图像最接近；如果移动两个pixel，最像的图片就变成了下图中红色方框内的图像，对应图中的红色曲线</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200717195325432.png" alt="image-20200717195325432" style="zoom:67%;"></p>
<h4 id="Mode-Dropping"><a href="#Mode-Dropping" class="headerlink" title="Mode Dropping"></a>Mode Dropping</h4><p>在训练GAN的时候，很可能会出现mode dropping，生成的图像出现多样性不够的问题。</p>
<p>对于DCGAN（Deep Convolutional GAN），我们怎么检测其生成的人脸是多样性的呢？</p>
<p>首先DCGAN产生很多张image，看这些生成的image中有没有非常像的，是人都可以辨识出来的。对于判断是不是同一张人脸，可以使用下列论文中提到的方法，先把DCGAN生成的图像丢到一个classifier里面，如果机器认为这是同一张图像，人再来进行观察，看是不是同一张图像。</p>
<p>那么DCGAN到底可以产生多少张不一样的image呢？</p>
<p>现在我们sample出400张image出来，如果有50%的概率，可以从400张image里面找出2张相同的人脸；我们就可以根据这个数据来反推到底产生了多少不同的人脸</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200717200642896.png" alt="image-20200717200642896" style="zoom:50%;"></p>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><ol>
<li><p>Lucas Theis, Aäron van den Oord, Matthias Bethge, “A note on the evaluation of generative models”, arXiv preprint, 2015</p>
</li>
<li><p><a href="https://blog.csdn.net/qq_27261889/article/details/86483505" target="_blank" rel="noopener">【深度理解】如何评价GAN网络的好坏？IS（inception score）和FID（Fréchet Inception Distance）</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>GAN</tag>
        <tag>Inception Score</tag>
      </tags>
  </entry>
  <entry>
    <title>Feature Extraction of GAN</title>
    <url>/2020/07/14/GAN-feature-extraction/</url>
    <content><![CDATA[<p>本文主要叙述GAN进行feature extraction的应用，包括InfoGAN，BiGAN，VAE-GAN，Triple GAN，Domain-adversarial training。</p>
<h4 id="InfoGAN"><a href="#InfoGAN" class="headerlink" title="InfoGAN"></a>InfoGAN</h4><p>GAN会input一个random的vector，然后output一个object。input的每个dimension都代表着某个特征，如果我们对input的某个dimension进行变化，output也会有相对应的变化，但实际上我们并不能那么直接观察到input和output之间的关系。</p>
<p>在下图中，展示了一个GAN手写数字生成的例子，横轴表示input的某个维度，纵轴表示变化后的output，但我们并不能观察出改变某个维度后，output会进行怎样的变化。</p>
<p>我们希望像下左图中蓝色的区块一样，在那一块区域的vector都具有相同的特征，如果用同一个区块的vector来作为generator的input，那么output就会有蓝色的特征，即我们假设这些特征是有一定规律性的。但实际上，这些特征的分布是非常不均匀的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714193948778.png" alt="image-20200714193948778" style="zoom:67%;"></p>
<h4 id="What-is-InfoGAN"><a href="#What-is-InfoGAN" class="headerlink" title="What is InfoGAN?"></a>What is InfoGAN?</h4><p>infoGAN把输入的z分成两部分，一半是c，一半是z’，还加入了一个额外的classifier，来对generator的输出x进行classify，classifier要能从x中反推出原来的c。我们就可以把这里的generator看作是encoder，把classifier看作是decoder，这两者和起来就可以看作是一个“autoencoder”。这里的“autoencoder”和传统的autoencoder是相反的，“autoencoder”先把code变成image，再从image变成code。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714195902210.png" alt="image-20200714195902210" style="zoom:67%;"></p>
<p>那么这里的discriminator是不是一定要存在呢？</p>
<p>答案是一定的，discriminator的存在非常有必要。generator要产生能使classifier进行分类的图像，如果没有discriminator，generator可以生成一张把c写在中间的图像，这样classifier读中间的数字，就知道c是什么。但这并不是我们想要的结果，generator生成的图像还必须接受discriminator的检验，保证生成的是一张真实的图像。</p>
<p>discriminator和classifier的参数可以是共享的，由于它们的输入都是一样的图像x，只有最后一层的输出不太一样，classifier输出code，discriminator输出scalar。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714201451998.png" alt="image-20200714201451998" style="zoom: 67%;"></p>
<p>那么infoGAN怎么解决input的feature，对output的影响不明确这件事呢？</p>
<p>如果generator可以学习到c的每个维度都对output x有一个明确的影响，那么classifier就可以很容易地根据x来反推出输入的c是什么；如果现在generator没有学习到某个dimensions对output的影响，就像本文刚开始说的手写数字生成的那个例子那样，改变输入的某个dimensions，对输出的影响是很奇怪的，那么classifier就不能从x反推出原来的c。</p>
<p>为什么这里的z要这样划分呢，例如前面一半是c，后面一半是z’？</p>
<p>这里的c并不是因为它代表了某些特征而被归类为c，而是因为被归类为c，所以代表某些特征。</p>
<p>在下图中，(a) 改变了c的第一维，对于infoGAN，刚好这个第一维就表示digit，生成的图像就变成了数字0-9；(b) 对应了一般的GAN，也改变了c的第一维，但生成的图像就比较奇怪；(c) 表示改变了c中的rotation，可以发现infoGAN生成的图像有不同程度的旋转 ；(d) 表示改变了c中的width，笔画慢慢从细到粗。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714203004487.png" alt="image-20200714203004487" style="zoom:67%;"></p>
<h4 id="VAE-GAN"><a href="#VAE-GAN" class="headerlink" title="VAE-GAN"></a>VAE-GAN</h4><p>在原来的encoder和decoder（generator）的基础上，加入了discriminator。如果把encoder和decoder和起来，就是一个VAE，如果把generator和discriminator和起来，就相当于是一个GAN。generator生成的图像与原图像的reconstruction error应该越小越好，同时还要越接近真实图像越好。</p>
<p><strong>对于VAE来说</strong>，如果只是让VAE的input和output越接近越好，不见得output的图像会realistic，生成的图像会很模糊。因此加入了一个discriminator，可以让generator生成的图像更加realistic。</p>
<p><strong>对于GAN来说</strong>，generator的input是随机的，它并不知道真实的图像长什么样，要花很大的代价才能让generator产生真实的图像。那么现在加入了encoder，generator根据encoder的输出，就知道真实的图像长什么样子了。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714203614559.png" alt="image-20200714203614559" style="zoom:67%;"></p>
<p>那么现在对于VAE-GAN来讲，encoder和decoder的目标除了最小化reconstruction error，</p>
<ul>
<li>encoder还需要将输出的编码接近normal distribution，</li>
<li>decoder（generator）生成的图像还需要骗过discriminator；</li>
<li>对于discriminator来讲，还需要能辨别real，generated 和 reconstructed image。</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714205158662.png" alt="image-20200714205158662" style="zoom:67%;"></p>
<p>下面我们将简要叙述VAE-GAN的算法流程，首先初始化encoder，decoder，discriminator的参数，</p>
<ul>
<li>sample出M个真实的数据$x^i$出来；</li>
<li>把真实数据输入encoder，得到$\tilde z^i=En(x^i)$；</li>
<li>把$\tilde z^i$输入decoder，得到$\tilde x^i=De(\tilde z^i)$；</li>
<li>从正态分布$P(z)$中sample出M个code $z^i$，再输入decoder得到对应的图像$\hat x^i=De(z^i)$；</li>
<li>更新encoder来减小reconstruction error$||\tilde x^i-x^i||$，还需要让encoder生成的code和正态分布越接近越好；</li>
<li>更新decoder来减小reconstruction error$||\tilde x^i-x^i||$，还希望生成的图像能骗过discriminator；</li>
<li>discriminator给真实的image高分，给reconstructed和generated的图像低分。</li>
</ul>
<p>之前我们都是让discriminator进行二分类，这里我们可以让他进行三分类，real，gen，recon。</p>
<h4 id="BiGAN"><a href="#BiGAN" class="headerlink" title="BiGAN"></a>BiGAN</h4><p>BiGAN也是修改了Autoencoder，但不是像VAE-GAN那样，encoder和decoder是连在一起的，BiGAN的encoder和decoder不是连在一起的。encoder首先输入一张真实的图像，输出code z，decoder的输入则是从normal distribution中sample出来的，再输出一张图像。</p>
<p>那么要怎么学习这个encoder和decoder呢？</p>
<p>BiGAN的做法是再加入一个discriminator，鉴别真实的图像和根据code z生成的图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714211339964.png" alt="image-20200714211339964" style="zoom:67%;"></p>
<p>算法流程，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714212205788.png" alt="image-20200714212205788" style="zoom:67%;"></p>
<p>如果输入图像数据的分布是$P(x,z)$，我们希望decoder的输出数据的分布$Q(x,z)$和原来的P是接近的，那么discriminator就是在衡量P和Q之间的差异。经过discriminator的引导，P和Q分布会越来越接近。在理想情况下，P和Q会变成一摸一样的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714212442862.png" alt="image-20200714212442862" style="zoom:67%;"></p>
<p>虽然encoder和decoder没有连在一起，但在discriminator的引导下，在理想的情况下会达成以下特性，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714213755034.png" alt="image-20200714213755034" style="zoom:67%;"></p>
<p>那么如果我们分别训练一个正向和反向的autoencoder，可不可以也达到同样的效果呢？</p>
<p>BiGAN和这种方式的optimal solution是一样的，但这两者的error surface是不一样的，如果这两者都训练到optimal的case，这两者的结果是一样的。但实际上并不可能训练到optimal case，autoencoder不可能真的学习到input和output一摸一样，BiGAN也不可能学习到P和Q的分布一摸一样。因此如果这两者学习不到optimal case，那么这两者学习出来的情况也是不一样的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714213945300.png" alt="image-20200714213945300" style="zoom:67%;"></p>
<h4 id="Triple-GAN"><a href="#Triple-GAN" class="headerlink" title="Triple GAN"></a>Triple GAN</h4><p>Triple GAN的主要结构有三种，discriminator，generator，classifier。如果现在不考虑classifier，就相当于是一个conditional GAN，其中的Y就相当于是一个condition。</p>
<p>Triple GAN其实是一种semi-supervised learning，如果有少量的label data，大量的unlabeled data。我们可以把labeled data来训练classifier，input x，就输出对应的Y；同时我们也可以训练generator，使其可以输入Y，输出一个X，就组成了一个pair(X,Y)，可以把这个pair再当成classifier的训练资料，增加训练数据。</p>
<p>classifier可以输入X，再输出对应的Y，组成一个pair，discriminator可以来辨别这个pair是不是真实的pair。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714214632976.png" alt="image-20200714214632976" style="zoom:67%;"></p>
<h4 id="Domain-adversarial-training"><a href="#Domain-adversarial-training" class="headerlink" title="Domain-adversarial training"></a>Domain-adversarial training</h4><h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>很可能我们的training data和testing data差别很大，比如training data都是黑白色的，但testing data却是彩色的。</p>
<p>那么现在我们可以通过generator提取出这两者的feature，虽然原始数据的domain不一样，但提取出来的feature却有着相同的分布。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714220051869.png" alt="image-20200714220051869" style="zoom:67%;"></p>
<p>那么现在有个generator可以进行feature extractor，还有一个discriminator进行domain classifier，来判断这个feature到底来自哪一个domain；现在还有一个额外的classifier，来进行label predict，判断feature属于哪一个类别。</p>
<p>这三者可以同时训练，也可以分开训练，就像GAN一样，要训练D，我们会先固定G的参数。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714220514948.png" alt="image-20200714220514948" style="zoom:67%;"></p>
<h5 id="Feature-Disentangle"><a href="#Feature-Disentangle" class="headerlink" title="Feature Disentangle"></a><strong>Feature Disentangle</strong></h5><p>对于原始的seq2seq autoencoder，input segment输入encoder之后，会生成一个code，这个code包含多种信息，phonetic information, speaker information, etc.</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714221631221.png" alt="image-20200714221631221" style="zoom:67%;"></p>
<p>我们需要进行<strong>Feature Disentangle</strong>，可以先进行两个encoder，即Phonetic Encoder和Speaker Encoder，再进行decoder，还原出原来的语音信号。</p>
<p>我们可以先加入一些constrain，对于同一个人的声音信号进行encode，虽然这两个输入的声音信号有些差别，但我们希望生成的embedding是接近的；如果是不同的人说的，这个output的embedding也必须有有一些差别。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714222046440.png" alt="image-20200714222046440" style="zoom:67%;"></p>
<p>我们还必须要训练另外一个speaker classifier，来判断两个vector到底是不是同一个人说的。如果输入的是两个不同的人说的语音信号，经过phonetic encoder的学习后，可以被提取出相同的feature，我们就说这个encoder可以过滤掉一些无关的杂讯，只关注一些有用的资讯。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714222412124.png" alt="image-20200714222412124" style="zoom:67%;"></p>
<p>结果展示，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714222845907.png" alt="image-20200714222845907" style="zoom:67%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>GAN</tag>
        <tag>VAE</tag>
        <tag>InfoGAN</tag>
        <tag>BiGAN</tag>
        <tag>VAE-GAN</tag>
        <tag>Triple GAN</tag>
        <tag>Domain-adversarial training</tag>
      </tags>
  </entry>
  <entry>
    <title>Theory behind GAN</title>
    <url>/2020/07/08/GAN-theory/</url>
    <content><![CDATA[<p>本文主要介绍了GAN的基础理论。还对似然函数和KL散度的关系进行了推导。</p>
<h4 id="Generation"><a href="#Generation" class="headerlink" title="Generation"></a>Generation</h4><p>现在我们用x来表示一张图像，是一个高维的vector，比如图像大小是$64\times64$维的，那么vector的维数就是$64\times64$。每张图像都是这个高维空间中的一个点。为了方便展示，下图中我们假设图像是二维空间中的一个点。</p>
<p>对于我们要产生的图像，有一个固定的distribution $P_{data}(x)$。在整个图像所构成的高维空间中，只有一小部分sample出来的图像和人脸接近，其他部分都不像人脸。比如我们从下图中蓝色的distribution中进行sample，看起来就很像是人脸，在其他区域就不像人脸。</p>
<p>那么我们现在的目标就是找到这个distribution。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708134708130.png" alt="image-20200708134708130" style="zoom:60%;"></p>
<p>在有GAN之前，我们通常用Maximum Likelihood Estimation来做这件事。</p>
<h4 id="Maximum-Likelihood-Estimation"><a href="#Maximum-Likelihood-Estimation" class="headerlink" title="Maximum Likelihood Estimation"></a>Maximum Likelihood Estimation</h4><ol>
<li>我们可以从这个distribution中sample图像，但我们并不知道其对应的formula长什么样子；</li>
<li>那么我们现在就可以找到另外一个distribution $P_G(x;\theta)$，比如其对应的参数可以是$\mu,\sum$，来使这个distribution的参数和原来的相接近。</li>
</ol>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708141336553.png" alt="image-20200708141336553" style="zoom:60%;"></p>
<p>具体做法如下，</p>
<ul>
<li>先从原来的distribution中sample出$\{x^1,x^2,…x^m\}$；</li>
<li>把$x^i$代入现在的已知的distribution $P_G(x^i;\theta)$，表示$x^i$是从现在这个distribution中sample出来的概率；</li>
<li>把这些概率相乘，得到似然函数L；最后找到对应的参数$\theta$，使似然函数取得最大值。</li>
</ul>
<script type="math/tex; mode=display">
L=\prod_{i=1}^mP_G(x^i;\theta)</script><h4 id="Minimize-KL-Divergence"><a href="#Minimize-KL-Divergence" class="headerlink" title="Minimize KL Divergence"></a>Minimize KL Divergence</h4><p>最大似然估计也就等同于来最小化KL divergence。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708142636137.png" alt="image-20200708142636137" style="zoom:60%;"></p>
<p>现在我们的问题是找到参数$\theta^*$，使得$E_{x\sim P_{data}}[logP_G(x;\theta)]$可以取得最大值。$\{x^1,x^2,…x^m\}$是从distribution $P_{data}$中sample出来的，我们把这里的$E_{x\sim P_{data}}$展开，从离散值变到连续值，即</p>
<script type="math/tex; mode=display">
E_{x\sim P_{data}}[logP_G(x;\theta)]=\int_xP_{data}(x)[logP_G(x;\theta)]dx</script><p>由于我们的目标是找到$P_G$分布对应的参数，现在加入一个常数项$\int_xP_{data}(x)[logP_{data}(x;\theta)]$，对最大化的问题也不会产生影响，即</p>
<script type="math/tex; mode=display">
\begin{aligned}
&arg\mathop{\rm  max}_{\theta} E_{x\sim P_{data}}[logP_G(x;\theta)]\\
=&arg\mathop{\rm  max}_{\theta}\int_xP_{data}(x)[logP_G(x;\theta)]dx\\
=&arg\mathop{\rm  max}_{\theta}\int_xP_{data}(x)[logP_G(x;\theta)]dx-\int_xP_{data}(x)[logP_{data}(x;\theta)]dx\\
=&arg\mathop{\rm  max}_{\theta}\int_xP_{data}(x)[logP_G(x;\theta)-logP_{data}(x;\theta)]dx\\
=&arg\mathop{\rm  max}_{\theta}\int_xP_{data}(x)[log\frac{P_G(x;\theta)}{P_{data}(x;\theta)}]dx\\
=&arg \mathop{\rm  max}_{\theta}-\int_xP_{data}(x)[log\frac{P_{data}(x;\theta)}{P_{G}(x;\theta)}]dx\\
=&arg \mathop{\rm  min}_{\theta}\int_xP_{data}(x)[log\frac{P_{data}(x;\theta)}{P_{G}(x;\theta)}]dx\\
=&arg \mathop{\rm  min}_{\theta}KL(P_{data}||P_G)
\end{aligned}</script><p>就把这个最大化似然函数问题转化为了最小化KL divergence的问题。</p>
<p>那么我们如何来定义$P_G$的表达式呢？</p>
<p>首先$P_G$是类似于高斯分布这样的distribution，很容易计算出其对应的likelihood；但如果是neural network这样的distribution，就很难算出这个likelihood。</p>
<h4 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h4><p>现在分布是neural network，如果我们还是用高斯分布的公式来进行调整，那么我们不管怎么变化mean和variance，其分布都不可能和neural network相接近。因此我们需要一个更加general的方式来学习generative这件事。</p>
<p>现在我们有一个generator G，input z是从normal distribution中sample出来的，output为$x=G(z)$，不同的z就会有不同的x，x就组成了一个新的distribution $P_G(x)$。这个distribution可以非常复杂，比如neural network。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708154052356.png" alt="image-20200708154052356" style="zoom:60%;"></p>
<p>我们希望通过G得出的这个distribution $P_G(x)$，与目标$P_{data}(x)$可以越接近越好，即$P_G,P_{data}$之间的divergence可以越小越好，可以是KLdivergence，也可以是其他的divergence，即</p>
<script type="math/tex; mode=display">
G^*=arg\mathop{\rm  min}_{\theta}Div(P_G,P_{data})</script><p>那么我们怎么来minimize这个divergence呢？</p>
<p>如果我们知道$P_G,P_{data}$的formulation，那么我们就可以计算出divergence，再使用gradient descent算法。但现在我们并不知道他们的formulation，就不能使用gradient descent算法。</p>
<h4 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h4><p>虽然我们并不知道这两者的distribution，但我们可以从这两个分布sample 很多data出来。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708154159852.png" alt="image-20200708154159852" style="zoom:60%;"></p>
<p>从这两个分布中sample很多data出来，又如何来计算分布之间的divergence呢？</p>
<p>我们可以使用GAN的discriminator来完成这个任务。</p>
<p>在下图中，我们使用蓝色星星表示从$P_{data}$中sample出来的数据，红色星星表示从$P_G$中sample出来的数据。再来训练我们的discriminator D，D对$P_{data}$中sample出来的数据会给高分，从$P_G$中sample出来的数据给低分。这个训练的结果就可以告诉我们$P_G,P_{data}$之间的divergence。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708161546891.png" alt="image-20200708161546891" style="zoom:60%;"></p>
<p>先fix掉G的参数，再来训练discriminator D，D得出的分数越大越好。找到$D^*$，使得$V(G,D)$最大化。</p>
<script type="math/tex; mode=display">
V(G,D)=E_{x\sim P_{data}}[log(D(x))]+E_{x\sim P_{G}}[1-log(D(x))]</script><p>其中$E_{x\sim P_{data}}[log(D(x))]$表示真实图像所得到的分数，D的目标就是使真实图像获得的分数越大越好；而$E_{x\sim P_{G}}[log(D(x))]$表示G生成的图像所得到的分数，应该越小越好，所以前面加了负号。</p>
<p>这个$V(D,G)$的表达式其实和二分类的问题是一样的，红色星星表示class 1，蓝色星星表示class 2，discriminator的任务就是对这两个class进行分类，来最小化cross entropy，也就相当于在解这个问题</p>
<script type="math/tex; mode=display">
D^*=arg\mathop{\rm  max}_{G}V(D,G)</script><p>我们最后找到的$D^*$，能使objective function $V(D,G)$取得最大值。这个objective function和divergence是有一定关系的。如果这两个类别之间很接近、很难区分，分类器训练的时候loss就会很大，对应的V的值就会很低，对应的divergence的值也会很低；如果这两个类别很好区分，discriminator就很容易找到$D$，使得V取得很大的值，从而divergence的值就会很大。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708163901367.png" alt="image-20200708163901367" style="zoom:60%;"></p>
<h4 id="V-G-D-和divergence之间的关系"><a href="#V-G-D-和divergence之间的关系" class="headerlink" title="V(G,D)和divergence之间的关系"></a>V(G,D)和divergence之间的关系</h4><p>根据大数定律，</p>
<script type="math/tex; mode=display">
\begin{aligned}
V(G,D)&=E_{x\sim P_{data}}[log(D(x))]+E_{x\sim P_{G}}[1-log(D(x))]\\
&=\int_x[P_{data}(x)logD(x)+P_G(x)log(1-D(x))]dx
\end{aligned}</script><p>其中我们假设$D(x)$可以是任意函数，现在我们的目标是找到其中某个D使V最大化。我们可以把积分中的x分开来算，对于其中的任意一个x，都可以分配一个最好的D函数。那么现在的问题就变成：对于给定的x，来找到最优值$D^*$使V最大化，即最大化</p>
<script type="math/tex; mode=display">
P_{data}(x)logD(x)+P_G(x)log(1-D(x))</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708164929216.png" alt="image-20200708164929216" style="zoom:60%;"></p>
<p>现在我们用$a=P_{data},b=P_G,D=D(x)$，来简化式子，即找到$D^*$来最大化</p>
<script type="math/tex; mode=display">
f(D)={\rm a}\ log(D)+{\rm b}\ log(1-D)</script><p>对D求导，并令成0，可以得到</p>
<script type="math/tex; mode=display">
D^*=\frac{a}{a+b}\quad\rightarrow\quad D^*(x)=\frac{P_{data}}{P_{data}+P_G}</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708170505053.png" alt="image-20200708170505053" style="zoom:60%;"></p>
<p>把$D^*(x)$代入objective function，可以得到</p>
<script type="math/tex; mode=display">
\begin{aligned}
&maxV(G,D)=V(G,D^*)\\&=E_{x\sim P_{data}}[log\frac{P_{data}}{P_{data}+P_G}]+E_{x\sim P_{G}}[log\frac{P_{G}}{P_{data}+P_G}]\\
&=\int_x[P_{data}(x)log\frac{P_{data}}{P_{data}+P_G}+P_G(x)log\frac{P_{G}}{P_{data}+P_G}]dx
\end{aligned}</script><p>对式子中log部分的分子分母同时除以2，把分子上的1/2提出来，可得</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\quad maxV(G,D)\\
&=-2log2+\int_x[P_{data}(x)log\frac{P_{data}}{(P_{data}+P_G)/2}+P_G(x)log\frac{P_{G}}{(P_{data}+P_G)/2}]dx\\
&=-2log2+KL(P_{data}||\frac{P_{data}+P_G}{2})+KL(P_{G}||\frac{P_{data}+P_G}{2})\\
&=-2log2+2JSD(P_{data}||P_G)\quad\quad(1)
\end{aligned}</script><p>也就得到了我们的<span style="color: red"> Jensen-Shannon divergence</span>，即JSD。</p>
<p>我们希望通过G得出的这个distribution $P_G(x)$，与目标$P_{data}(x)$可以越接近越好，即$P_G,P_{data}$之间的divergence可以越小越好，即</p>
<script type="math/tex; mode=display">
G^*=arg\mathop{\rm  min}_{G}Div(P_G,P_{data})</script><p>那么我们到底怎么算$P_G,P_{data}$之间的divergence $Div(P_G,P_{data})$呢？</p>
<p>根据公式(1)，我们知道了JS divergence和$maxV(G,D)$之间的关系，是成正比的。那么现在我们找到D，使objective function取得最大值，这个最大值就是divergence。那么现在的式子就变成了，</p>
<script type="math/tex; mode=display">
G^*=arg\mathop{\rm  min}_{G}\mathop{\rm  max}_DV(G,D)</script><p>其中要最大化discriminator得出的分数，最小化generator生成的数据与$P_{data}$之间的差距。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708192757596.png" alt="image-20200708192757596" style="zoom:60%;"></p>
<p>假设现在只有三个generator $G_1,G_2,G_3$可供选择，对应的三个objective function变化如上图所示，横坐标表示选择了不同的discriminator，纵坐标表示$V(G_i,D)$的值。第一幅图表示选择固定G1，变化discriminator，V的值的变化曲线。</p>
<p>图中红色圆点所在的横坐标，表示$V(G_i,D)$值最大的位置，一共有三个。现在已经找到了使V最大的discriminator，接下来需要找使$max V(G,D)$最小的generator（$G_1,G_2,G_3$）。毫无疑问是第三幅图中的$G_3$，是可以使得$max V(G,D)$最小的generator。</p>
<p>红色圆点的纵坐标$V(G_i,D)$表示$P_{G_i},P_{data}$之间的divergence，也是第三幅图中的divergence最小。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708192923990.png" alt="image-20200708192923990" style="zoom:60%;"></p>
<p>其实GAN的两个训练步骤就是在解决这个最大最小化问题。</p>
<h4 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h4><p>我们把目标式子简化一下，现在D是一个给定的值，可以让$V(G,D)$的值最大化，$\mathop{\rm  max}_DV(G,D)$可以表示为$L(G)$，即找到$G^*$，</p>
<script type="math/tex; mode=display">
G^*=arg\mathop{\rm  min}_{G}L(G)</script><p>先计算出gradient  $\frac{\partial L(G)}{\partial \theta_G}$，再来更新$\theta_G$的参数。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708201616936.png" alt="image-20200708201616936" style="zoom:60%;"></p>
<p>Q：其中$L(G)=\mathop{\rm  max}_DV(G,D)$，我们可以对这个函数求微分吗 ？</p>
<p>A：可以。如果现在有一个函数$f(x)=max\{f_1(x),f_2(x),f_3(x)\}$，其对应的函数图像如上图所示，是个分段函数。我们假设$f_1(x)$的函数值是最大的，那么$\frac{df(x)}{dx}=\frac{df_1(x)}{dx}$，梯度对应的是在该区域内，函数值最大的那个梯度。更次参数更新都要注意自己在哪个region内，不同的region求导的函数不一样。</p>
<p>具体的算法流程如下：</p>
<ul>
<li>给定一个初始的generator $G_0$；</li>
<li>找到$D_0^*$，使得$V(G_0,D)$的值最大化；</li>
<li>得到$L(G)$之后，就可以对整个式子求微分，得出对应的梯度，更新generator的参数，就得到新的generator $G_1$；</li>
</ul>
<p>得到新的generator后，很可能已经进入了下一个region，因此还需要重新计算discriminator，</p>
<ul>
<li>此时objective function为$V(G_1,D)$，现在是$D_1^*$使V取得最大值；</li>
<li>$L(G)=V(G_1,D_1^*)$，再更新generator的参数。</li>
</ul>
<p>……</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708203440933.png" alt="image-20200708203440933" style="zoom:60%;"></p>
<p>更新参数这个过程到底是不是在减小JS divergence呢？</p>
<p><span style="color: blue">（此处公式渲染错误，可去<a href="https://scarleatt.gitee.io/ML-notes/ML-notes-html/22_theory_behind_GAN.html" target="_blank" rel="noopener">这个网站</a>查看完整文章。）</span></p>
<p>首先我们找到了$D_0^<em>$，使V取得最大值$V(G_0,D_0^</em>)$，也就是JS divergence取得最大值；在generator更新参数之后，objective function也发生了变化$V(G_0,D)\rightarrow V(G_1,D)$ ，$D_0^<em>$对应的$V(G_0,D_0^</em>)$并不是现在的最大值，而$D_1^<em>$对应的$V(G_0,D_1^</em>)$才是最大值。从图中可以看出，$V(G_0,D_1^<em>)&lt;V(G_0,D_0^</em>)$，对应的JS divergence反而减小了。</p>
<p>如果generator的参数变化不大，即$D_0^<em>\approx D_1^</em>$，我们就可以把这个过程看作是在减小divergence。在实际的操作中，我们应该使G迭代的次数减少， 使D迭代的次数增加。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708205829068.png" alt="image-20200708205829068" style="zoom:60%;"></p>
<h4 id="In-practice-…"><a href="#In-practice-…" class="headerlink" title="In practice …"></a>In practice …</h4><p>fix G的参数，得到G生成的图像$\{\tilde x^1,\tilde x^2,..,\tilde x^m\}$，再输入discriminator D，不断调整$\theta_d$，使得得到的分数越大越好，</p>
<script type="math/tex; mode=display">
\tilde V =\frac{1}{m}\sum_{i=1}^m logD(x^i)+\frac{1}{m}\sum_{i=1}^mlog(1-D(\tilde x^i))</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708212957397.png" alt="image-20200708212957397" style="zoom:60%;"></p>
<p>这个discriminator其实就是在做binary classifier；</p>
<p>这是整个算法的步骤：</p>
<p>Learning D：首先从数据库中取出m个真实图片，再根据一个分布随机产生m个vector作为输入$\{z^1,z^2,..,z^m\}$，此时fix G的参数，得到G生成的图像$\{\tilde x^1,\tilde x^2,..,\tilde x^m\}$，再输入discriminator D，不断调整$\theta_d$，使得得到的分数越大越好。V的值最大的时候，divergence的值才越小。</p>
<script type="math/tex; mode=display">
\tilde V =\frac{1}{m}\sum_{i=1}^m logD(x^i)+\frac{1}{m}\sum_{i=1}^mlog(1-D(\tilde x^i))</script><p>其中$D(x^i)$表示真实图像所得到的分数，D的目标就是使真实图像获得的分数越大越好；而$D(\tilde x^i)$表示G生成的图像所得到的分数，应该越小越好，所以前面加了负号。求出梯度$\Delta \tilde V(\theta_d)$，再更新$\theta_d$的值，</p>
<script type="math/tex; mode=display">
\theta_d\leftarrow \theta_d+\eta\Delta \tilde V(\theta_d)</script><p>Learning G：把D训练好之后，我们就可以fix D，来训练generator G的参数。首先也需要从分布中随机生成一些噪声z，再输入G，$G(z^i)$再输入D，得到相对应的分数，G的目标是想办法骗过D，不断调整参数$\theta_g$，使下面的objective function最小化，<u>generator不能train太多次，通常update一次就好</u>。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\tilde V &=\frac{1}{m}\sum_{i=1}^m log(1-D(G(z^i)))
\end{aligned}</script><p>求出梯度$\Delta \tilde V(\theta_g)$，再更新$\theta_g$的值，</p>
<script type="math/tex; mode=display">
\theta_g\leftarrow \theta_g-\eta\Delta \tilde V(\theta_g)</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708222236749.png" alt="image-20200708222236749" style="zoom:60%;"></p>
<h4 id="Objective-Function-for-Generator-in-Real-Implementation"><a href="#Objective-Function-for-Generator-in-Real-Implementation" class="headerlink" title="Objective Function for Generator in Real Implementation"></a>Objective Function for Generator in Real Implementation</h4><p>实际上，我们使用$V=E_{x\sim P_G}[-log(D(x))]$，可以更方便进行code。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708223556261.png" alt="image-20200708223556261" style="zoom:60%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>GAN</tag>
        <tag>Maximum Likelihood Estimation</tag>
        <tag>Minimize KL Divergence</tag>
        <tag>似然函数和KL散度的关系</tag>
      </tags>
  </entry>
  <entry>
    <title>Introduction of Generative Adversarial Network (GAN)</title>
    <url>/2020/07/04/GAN-intro/</url>
    <content><![CDATA[<p>有很多种不同类型GAN，可以在这里查看<a href="https://github.com/hindupuravinash/the-gan-zoo" target="_blank" rel="noopener">https://github.com/hindupuravinash/the-gan-zoo</a></p>
<h4 id="Generation"><a href="#Generation" class="headerlink" title="Generation"></a>Generation</h4><p>对于Image generation，要实现的是输入一个vector，输出一个image；而对于Sentence generation，则实现的是输入一个vector，输出为一个sentence。那么GAN就是用来实现这个中间的NN Generator。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704140831850.png" alt="image-20200704140831850" style="zoom:60%;"></p>
<h4 id="Basic-Idea-of-GAN"><a href="#Basic-Idea-of-GAN" class="headerlink" title="Basic Idea of GAN"></a>Basic Idea of GAN</h4><h5 id="Generator-vs-Discriminator"><a href="#Generator-vs-Discriminator" class="headerlink" title="Generator vs Discriminator"></a>Generator vs Discriminator</h5><p>这个generator可以是一个神经网络，也可以是一个函数f。</p>
<p>输入的vector表示我们要generate图像的某种特征，比如vector的第一维如果代表头发的长度，我们现在把这个值设得很大，那么就会generate一张头发很长的图像；如果我们改变了vector倒数第二维（头发为蓝色）的值，可以发现generate的新图头发变蓝了，由于此时只改变了头发的颜色，其他特征都是类似的，所以只有头发的颜色发生了变化。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704141328771.png" alt="image-20200704141328771" style="zoom:60%;"></p>
<p>Discriminator可以是一个神经网络，也可以是一个函数f。输入是一张image，输出为scalar，数值越大，表示这个image越接近真实图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704141804394.png" alt="image-20200704141804394" style="zoom:60%;"></p>
<p>在大自然中，某种鸟类以枯叶蝶为食。枯叶蝶必须不断地进化，使其看起来越来越像一个枯叶；它的天敌也在不断地进化，如果枯叶蝶看起来并不像叶子，那么它就会被捕食。其中枯叶蝶就相当于一个generator，天敌就相当于discriminator。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704142147503.png" alt="image-20200704142147503" style="zoom:60%;"></p>
<p>第一代的generator不知道怎样产生二次元的头像，所以会产生一些看起来很像是杂讯的图像，再把这个图像输入discriminator，来判断这是不是一张真实的图像，第一代的discriminator可以根据图像是否有颜色，来正确分辨真实图像和生成的图像；</p>
<p>那么第二代的generator的目标就是想办法骗过第一代的discriminator，生成了有颜色的图像，随之discriminator也会发生进化，学习了真实图像和生成图像之间的差异（真实图像是有嘴的）；</p>
<p>第三代的generator生成的图像可以骗过第二代的discriminator，然后discriminator也会继续进化，……</p>
<p>generator和discriminator都会不断地进化，因此generator会产生越来越像真实图片的图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704142643957.png" alt="image-20200704142643957" style="zoom:60%;"></p>
<p>这个过程可以看作是一个对抗的过程，也看一用另外一个和平的比喻来进行解释。generator相当于一个学生，discriminator相当于一个老师，学生并不知道真实的图像长什么样，但老师看过很多真实的图像，就知道真实的图像应该长什么样子。</p>
<p>第一代的generator相当于一年级的学生，重复着上述的过程。学生会画得越来越好，老师会越来越严格，那么学生最后就可以画出很想二次元人物的图像了。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704145218224.png" alt="image-20200704145218224" style="zoom:60%;"></p>
<h5 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h5><p>首先需要随机初始化G和D的参数，</p>
<p>Step 1：我们需要先调整D的参数，就必须先把G的参数固定。首先把随机产生的vector输入G（fix），生成新的图像之后与从database中sampled出来的图像进行比较。要实现D如果输入真实图像，就会得高分，与1越接近越好，如果输入生成的图像，就会得低分，与0越接近越好。有了这个标准，我们就可以来训练这个discriminator D；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704145820900.png" alt="image-20200704145820900" style="zoom:60%;"></p>
<p>Step2：训练好discriminator D之后，我们就可以fix D，来调整generator G。一个vector输入第一代G之后，会生成一张图像，再输入D（fix），就可以得到一个很低的分数（0.13）。那么G训练的目标就是使生成的图片可以“骗”过D，即生成的图片使D给出一个比较高的分数。由于D看过真实的图像，如果给出了很高的分数，就可以说明G生成的图像和真实图像是很接近的。</p>
<p>在真实的代码实现中，我们通常会把generator和discriminator当成是一个大的network，其中generator的输出就可以看作是一个hidden layer，discriminator所在的层参数是fix的，不用调整，只需要根据整个网络的输出来调整generator的参数。</p>
<p>由于我们希望使discriminator的输出分数值越大越好，因此这里使用了梯度上升算法 <strong>Gradient Ascent</strong>，也就是梯度下降算法前面多乘了一个负号。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704150612063.png" alt="image-20200704150612063" style="zoom:60%;"></p>
<p>现在来叙述一下总的算法流程，$\theta_d,\theta_g$分别表示discriminator和generator的参数。</p>
<p>Learning D：首先从数据库中取出m个真实图片，再根据一个分布随机产生m个vector作为输入$\{z^1,z^2,..,z^m\}$，此时fix G的参数，得到G生成的图像$\{\tilde x^1,\tilde x^2,..,\tilde x^m\}$，再输入discriminator D，不断调整$\theta_d$，使得得到的分数越大越好，</p>
<script type="math/tex; mode=display">
\tilde V =\frac{1}{m}\sum_{i=1}^m logD(x^i)+\frac{1}{m}\sum_{i=1}^mlog(1-D(\tilde x^i))</script><p>其中$D(x^i)$表示真实图像所得到的分数，D的目标就是使真实图像获得的分数越大越好；而$D(\tilde x^i)$表示G生成的图像所得到的分数，应该越小越好，所以前面加了负号。为了方便求梯度，在式子前面介入了log，求出梯度$\Delta \tilde V(\theta_d)$，再更新$\theta_d$的值，</p>
<script type="math/tex; mode=display">
\theta_d\leftarrow \theta_d+\eta\Delta \tilde V(\theta_d)</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704152035892.png" alt="image-20200704152035892" style="zoom: 67%;"></p>
<p>Learning G：把D训练好之后，我们就可以fix D，来训练generator G的参数。首先也需要从分布中随机生成一些噪声z，再输入G，$G(z^i)$再输入D，得到相对应的分数，G的目标是想办法骗过D，不断调整参数$\theta_g$，使生成的图像所得到的分数越高越好，</p>
<script type="math/tex; mode=display">
\tilde V =\frac{1}{m}\sum_{i=1}^m logD(G(z^i))</script><p>求出梯度$\Delta \tilde V(\theta_g)$，再更新$\theta_g$的值，</p>
<script type="math/tex; mode=display">
\theta_g\leftarrow \theta_g+\eta\Delta \tilde V(\theta_g)</script><p>在每个iteration里，都会进行这个步骤，先训练discriminator，再训练generator；这两个步骤会反复进行。</p>
<h5 id="Anime-Face-Generation"><a href="#Anime-Face-Generation" class="headerlink" title="Anime Face Generation"></a>Anime Face Generation</h5><p>结果展示</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704154439615.png" alt="image-20200704154439615" style="zoom:60%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704154503164.png" alt="image-20200704154503164" style="zoom:60%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704154528242.png" alt="image-20200704154528242" style="zoom:60%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704154544426.png" alt="image-20200704154544426" style="zoom:60%;"></p>
<h4 id="GAN-as-structured-learning"><a href="#GAN-as-structured-learning" class="headerlink" title="GAN as structured learning"></a>GAN as structured learning</h4><h5 id="Structed-learning"><a href="#Structed-learning" class="headerlink" title="Structed learning"></a>Structed learning</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704155106998.png" alt="image-20200704155106998" style="zoom:60%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704155202016.png" alt="image-20200704155202016" style="zoom:60%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704155216943.png" alt="image-20200704155216943" style="zoom:60%;"></p>
<h5 id="Why-Structured-Learning-Challenging"><a href="#Why-Structured-Learning-Challenging" class="headerlink" title="Why Structured Learning Challenging?"></a>Why Structured Learning Challenging?</h5><p>One-shot/Zero-shot Learning，如果有的类别都没有范例，或者只有很少一部分的范例。</p>
<p>而structured learning是一种极端的One-shot learning，由于output为一个structure，比如一个句子，可能这些句子在training data中从来没出现过，那么如何学习去输出一个从来没看到的structure，machine必须学会去创造。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704155430757.png" alt="image-20200704155430757" style="zoom:60%;"></p>
<p>machine还必须学会如何去planing，有全局观；比如sentence generation中，如果只看第一句话，会认为是负面的，但如果你把整句话都看完，就会发现这整句话在表达一个正面的意思。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704160320919.png" alt="image-20200704160320919" style="zoom:60%;"></p>
<h5 id="Structured-Learning-Approach"><a href="#Structured-Learning-Approach" class="headerlink" title="Structured Learning Approach"></a>Structured Learning Approach</h5><p>structured learning有两套方法：</p>
<ul>
<li>Bottom up，机器在生成一个部件时，会先生成多个component，这种方法一个很大的问题就是容易失去大局观；</li>
<li>Top down，产生一个完整的物件之后，再去从整体上看产生物件好不好。</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704160908741.png" alt="image-20200704160908741" style="zoom:60%;"></p>
<p>把这两种方法结合起来就是Generator。</p>
<h4 id="Can-Generator-learn-by-itself"><a href="#Can-Generator-learn-by-itself" class="headerlink" title="Can Generator learn by itself?"></a>Can Generator learn by itself?</h4><h5 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h5><p>对于Generator，首先输入不同的vector，就可以输出不同的图片。如果我们现在输入1对应的vetor，generator会生成一张image，目标是使image和真实的图像越接近越好，这个真实图像现在generator能看到，那么这不就和一般的supervised learning一摸一样了吗？</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704161403974.png" alt="image-20200704161403974" style="zoom:60%;"></p>
<p>那么我们怎么知道输入的那些vector的数值呢？</p>
<p>我们可以用一个Encoder来表示，把image输入这个NN Encoder，就会输出对应的特征，把图像的特征用vector来表示即可。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704162515395.png" alt="image-20200704162515395" style="zoom:60%;"></p>
<h5 id="Auto-encoder"><a href="#Auto-encoder" class="headerlink" title="Auto-encoder"></a>Auto-encoder</h5><p>Auto-encoder分为encoder和decoder。对于输入的$28\times 28$图像，先用encoder使得输入的图像变成code，decoder把这个code再恢复成原来的图像，这两者会一起进行学习。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704162914235.png" alt="image-20200704162914235" style="zoom:60%;"></p>
<p>目标是使得input和output越接近越好。这里的decoder就相当于一个generator，我们可以随机输入一个vector，使decoder（generator）生成一张对应的图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704163120815.png" alt="image-20200704163120815" style="zoom:60%;"></p>
<p>这里的code可以训练成二维的。如果code是$(-1.5\ \ 0)^T$，输入decoder会生成对应的图像，即图中的0；如果code是$(1.5\ \ 0)^T$，输入decoder会生成对应的图像，即图中的1.</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704163400098.png" alt="image-20200704163400098" style="zoom:60%;"></p>
<p>如果我们在这个范围内等距地sample，就有以下结果，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704163657239.png" alt="image-20200704163657239" style="zoom:60%;"></p>
<p>但使用auto-encoder也会产生一些问题，如果输入vector为a，那么会产生1的图像，如果输入vector为b，会产生斜着的1；</p>
<p>Q：那么0.5a+0.5b的vector会产生什么样的结果呢？</p>
<p>A：由于NN Generator是network，不是线性的，很可能产生不是数字的图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704163846888.png" alt="image-20200704163846888" style="zoom:60%;"></p>
<h5 id="VAE（Variational-Auto-encoder）"><a href="#VAE（Variational-Auto-encoder）" class="headerlink" title="VAE（Variational Auto-encoder）"></a>VAE（<strong>Variational Auto-encoder</strong>）</h5><p>我们可以用VAE来解决这个问题，输入vector a之后，generator不仅产生code m，还会产生每一维的方差$\sigma$，还有一个额外的vector e（noise），最后根据$c_i=exp(\sigma_i)\times e_i+m_i$得出需要的code；decoder需要根据这个带有noise的code，来还原出和input类似的图像。</p>
<p>那么现在machine不仅看到vector a会产生数字，看到vector b也会产生数字；看到vector a+noise也会产生数字，看到vector b+noise也会产生数字。</p>
<p>因此，<span style="color: red">对于现在的generator，如果input是在训练的时候从来没见过的vector，也可能output出合理的object。</span></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704164316549.png" alt="image-20200704164316549" style="zoom:60%;"></p>
<h5 id="What-do-we-miss"><a href="#What-do-we-miss" class="headerlink" title="What do we miss?"></a>What do we miss?</h5><p>现在有一张真实图像Target，通过generator生成的图像为generated image，目标是使这两者之间的差距越小越好。通常这两者之间的差距，可以通过每个像素点之间的差值来进行计算，比如可以将两张图像都表示成一个vector，再通过L1/L2计算两者之间的distance。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704165316903.png" alt="image-20200704165316903" style="zoom:60%;"></p>
<p>在一种理想的条件下，generator可以生成和Target完全一样的图像。但实际上，generator通常会犯一些错误，在生成的时候会有一些取舍，在一些地方会不得不做出妥协，选择在什么地方做妥协，对结果也会产生至关重要的影响。</p>
<p>在下图中，有一个Target，和4个Generated image。上面两张图像和target之间的差距只有一个pixel，下面两张图像和target之间的差距有6个pixel。但如果根据我们自己的观点，前两张图效果并不好，看起来并不像人写的数字，后两张图效果要好很多，因为后两张图只是对其中的一些笔画进行了延长。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704171245512.png" alt="image-20200704171245512" style="zoom:60%;"></p>
<p>因此，<span style="color: red">我们并不能单纯地只是让我们的output和target越像越好，否则就会产生上述的生成效果。</span></p>
<p>Structure  learning的输出是一个structure，由多个component组成，每个component之间的关系是非常重要的。对于下图中生成的数字图像，左边的图像有个pixel所在的位置很奇怪，不像是人手写的；但在右图中，我们添加了一些深色的pixel（和原来的pixel相邻）进去，就看起来很像是人手写的了。</p>
<p>但在neural network中，我们很难把component之间的关系放进去。在下图的网络结构中，layer L是output，表示颜色的深浅。如果layer L-1是给定的，那么输出的neural之间其实是independent的，值不会互相影响，也不会互相配合来产生一个相同的颜色。<span style="color: red">为了把不同component之间的关系也考虑进去，我们可以加入多个hidden layer</span>。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704172041505.png" alt="image-20200704172041505" style="zoom:60%;"></p>
<h5 id="GAN-vs-VAE"><a href="#GAN-vs-VAE" class="headerlink" title="GAN vs VAE"></a>GAN vs VAE</h5><p><span style="color: red">如果现在有相同的network，我们用GAN和VAE都可以用来生成图像，但VAE需要用更大的network（更多的hidden layer）才能达到和GAN接近的结果。</span></p>
<p>蓝色的点为实验结果，绿色的点表示target，VAE最好能训练的结果也就是图中的蓝色分布了，因为VAE很难考虑不同的dimension之间的关系。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704174238271.png" alt="image-20200704174238271" style="zoom:60%;"></p>
<h4 id="Can-Discriminator-generate"><a href="#Can-Discriminator-generate" class="headerlink" title="Can Discriminator generate?"></a>Can Discriminator generate?</h4><h5 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h5><p>我们再来回顾一下discriminator的概念，input为一张图像x，output为分数$D(X)$，数值越大，表示这个image越接近真实图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704174542805.png" alt="image-20200704174542805" style="zoom:60%;"></p>
<p>我们也可以用discriminator来生成图像。</p>
<p>我们之前提到用generator也可以用来生成图像，但考虑每个component之间的关系是非常困难的。对于discriminator，考虑每个component之间的关系，就相对而言比较容易了。</p>
<p>Q：为什么使用discriminator来生成图像就有优势？</p>
<p>A：<u>在生成的时候，来考虑每个component之间的关系是很麻烦的；但在整个图片都生成完的时候，再来判断图像每个component之间的关系就很容易了。</u></p>
<p>在下图中，如果用discriminator来生成图像，就可以根据分数来判断生成的图像到底好不好，比如左边的数字2，就可以给一个很低的分数，右边的数字2，就可以给一个很高的分数。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704174830772.png" alt="image-20200704174830772" style="zoom:60%;"></p>
<p>Q：那么我们怎么来实际来判断分数呢？</p>
<p>A：discriminator很可能是convolution neural network，对于其中的一个CNN filter（上图所示），可以来判断某个pixel周围是不是都是empty的，如果是的话，就给他低分。</p>
<p>如果我们现在有一个discriminator，我们可以通过以下方式来生成图像：穷举所有可能的输入x，看哪一个输入可以得到最高的分数。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704175856938.png" alt="image-20200704175856938" style="zoom:60%;"></p>
<h5 id="Discriminator-Training"><a href="#Discriminator-Training" class="headerlink" title="Discriminator - Training"></a>Discriminator - Training</h5><p>Discriminator的训练不仅需要真实的图像，也需要一些negative example。如果只有真实的图像，从来没有看过negative example，也不知道negative example长什么样子，D就有可能对所有的输入都输出一个很好的分数1.</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704180319560.png" alt="image-20200704180319560" style="zoom:60%;"></p>
<p>如何选择negative example也至关重要。在下图中，（左）如果negative example可以让机器很容易就分别出来（0，fake），那么如果我们输入一个相对比较清晰的图像，D就会得出一个很好的分数0.9，其实这个图像是fake的，这并不是我们想看到的结果；（右）如果输入的是非常真实的negative example，D才可以很好地鉴别real or fake image。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704180657495.png" alt="image-20200704180657495" style="zoom:60%;"></p>
<p>我们可以通过以下步骤来生成realistic negative examples，首先给出positive和negative examples。再进行不断地循环。</p>
<p>在循环中，D要给positive example很高的分数，给negative example很低的分数；学习出这个discriminator D之后，再用D来做generation（比如穷举找出$\tilde x$，使得分数取得最大值）；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704181511902.png" alt="image-20200704181511902" style="zoom:60%;"></p>
<p>找出这些D觉得还不错的图像$\tilde x$之后，进入下一次循环，将$\tilde x$与positive examples进行比较，学习出新的discriminator，……</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704181540562.png" alt="image-20200704181540562" style="zoom:60%;"></p>
<p>我们现在假设object分布在一维空间中，把object输入D，得到分数$D(x)$。在下图中，$D(x)$再real example区域取得了很不错的分数，在其他区域则分数相对较低。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704182329306.png" alt="image-20200704182329306" style="zoom:60%;"></p>
<p>但实际上，x所在的空间是非常高维的，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704183029426.png" alt="image-20200704183029426" style="zoom:60%;"></p>
<h5 id="Generator-v-s-Discriminator"><a href="#Generator-v-s-Discriminator" class="headerlink" title="Generator v.s. Discriminator"></a>Generator v.s. Discriminator</h5><p>Generator：</p>
<ul>
<li>优点：很容易做生成；</li>
<li>缺点：不容易考虑component之间的关系。</li>
</ul>
<p>Discriminator：</p>
<ul>
<li>优点：可以很容易考虑每个component之间的关系；</li>
<li>缺点：生成很慢。</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704205720871.png" alt="image-20200704205720871" style="zoom:60%;"></p>
<h5 id="Generator-Discriminator"><a href="#Generator-Discriminator" class="headerlink" title="Generator + Discriminator"></a>Generator + Discriminator</h5><p>现在我们将Generator和Discriminator结合起来，使用G来生成negative examples，现在D就不用那么费力去寻找对应的negative examples了。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704212740716.png" alt="image-20200704212740716" style="zoom:60%;"></p>
<h5 id="Benefit-of-GAN"><a href="#Benefit-of-GAN" class="headerlink" title="Benefit of GAN"></a>Benefit of GAN</h5><ol>
<li>从Discriminator的角度来看，现在我们只需要使用generator来生成negative examples，相比于之前的方法，可以更加高效；</li>
<li>从Generator的角度来看，虽然还是生成component-by-component的object，但此时G得到的feedback不再是L1或者L2 loss了，不用再去计算pixel之间的相似度，而是更具有全局观的discriminator给出的分数评价。</li>
</ol>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704213005168.png" alt="image-20200704213005168" style="zoom:60%;"></p>
<h5 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h5><p>这里的数据还使用的上文VAE的数据，只是训练方法调整为了GAN。蓝色点点是generator要学习的目标，前文的VAE没有discriminator，可以发现现在GAN比VAE的效果要好很多。在真实的环境中，VAE产生的人脸会比较模糊，GAN生成的人脸就没那么模糊。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704213601037.png" alt="image-20200704213601037" style="zoom:60%;"></p>
<h4 id="A-little-bit-theory"><a href="#A-little-bit-theory" class="headerlink" title="A little bit theory"></a>A little bit theory</h4><p><img src="/2020/07/04/GAN-intro/image-20200704220645444.png" alt="image-20200704220645444"></p>
<p><img src="/2020/07/04/GAN-intro/image-20200704220652727.png" alt="image-20200704220652727"></p>
<p><img src="/2020/07/04/GAN-intro/image-20200704220700604.png" alt="image-20200704220700604"></p>
<p><img src="/2020/07/04/GAN-intro/image-20200704220709098.png" alt="image-20200704220709098"></p>
<p><img src="/2020/07/04/GAN-intro/image-20200704220719432.png" alt="image-20200704220719432"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>GAN</tag>
        <tag>VAE</tag>
        <tag>Auto-Encoder</tag>
      </tags>
  </entry>
  <entry>
    <title>Logistic Regression</title>
    <url>/2020/06/06/Logistic-Regression/</url>
    <content><![CDATA[<h4 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h4><p>我们可以得出z的简易表达式$z=w\cdot x + b$，可得出</p>
<script type="math/tex; mode=display">
P(C_1|x)=\sigma(z)=\sigma(w\cdot x+b)</script><p>当得出$N_1,N_2,\mu^1,\mu^2,\sum$时，就可以计算出w和b的值。</p>
<h4 id="Three-Steps"><a href="#Three-Steps" class="headerlink" title="Three Steps"></a>Three Steps</h4><h5 id="Step1-Function-Set"><a href="#Step1-Function-Set" class="headerlink" title="Step1: Function Set"></a>Step1: Function Set</h5><p>把所有的w和b都要包括进来，这里使用的function set就是sigmoid函数，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606111122013.png" alt="image-20200606111122013" style="zoom:50%;"></p>
<h5 id="Step2-Goodness-of-a-Function"><a href="#Step2-Goodness-of-a-Function" class="headerlink" title="Step2: Goodness of a Function"></a>Step2: Goodness of a Function</h5><p>对于给定的一组w和b，得出似然函数L(w,b)的表达式，对于一个二分类问题，类别C1的概率为$f_{w,b}(x^i),\ i=1,2,4,…N$，而类别C2的概率则为$1-f_{w,b}(x^3)$。找出相对应的$w^{<em>},b^{</em>}$，使得L取得最大值。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606111627142.png" alt="image-20200606111627142" style="zoom:50%;"></p>
<p>对于训练数据集，我们设C1的$\hat y=1$，C2的$\hat y =0$，服从Bernoulli distribution。在函数前面加-号就可以使原来的最大化函数，转化为对目标的最小化。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606112102628.png" alt="image-20200606112102628" style="zoom:50%;"></p>
<p>这时原来的似然函数L转化为了一个新形式，把原来的乘法变成了ln项相加，可以方便后边对w的求导</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606112315406.png" alt="image-20200606112315406" style="zoom:50%;"></p>
<p>现在我们的目标就转化为了找出$w^<em>,b^</em>=argmin-lnL(w,b)$，交叉熵的形式为</p>
<script type="math/tex; mode=display">
-lnL(w,b)=\sum_n -[\hat y^nlnf_{w,b}(x^n)+(1-\hat y^n)ln(1-f_{w,b}(x^n))</script><h5 id="Step3-Find-the-best-function"><a href="#Step3-Find-the-best-function" class="headerlink" title="Step3: Find the best function"></a>Step3: Find the best function</h5><p>为了找出那组使得$-lnL(w,b)$最小化的参数$w^<em>,b^</em>$，这里我们使用了Gradient Descent方法</p>
<script type="math/tex; mode=display">
f_{w,b}(x)=\sigma (x)=\frac{1}{1+e^{-z}},\quad z=w\cdot x+b=\sum_i w_ix_i+b</script><p>对wi求导，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606113919943.png" alt="image-20200606113919943" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606113959065.png" alt="image-20200606113959065" style="zoom:50%;"></p>
<p>分别得出$\frac{\partial lnf_{w,b}(x)}{\partial w_i},\frac{\partial ln(1-f_{w,b}(x))}{\partial w_i}$，代入原式子，化简可得</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606114230182.png" alt="image-20200606114230182" style="zoom:50%;"></p>
<p>得出梯度$\frac{\partial (-ln L(w,b))}{\partial w_i}=\sum_n -\left(\hat y^n-f_{w,b}(x^n)\right)x_i^n$，代入每次的梯度更新公式，</p>
<script type="math/tex; mode=display">
w_i\leftarrow w_i -\eta \frac{\partial (-ln L(w,b))}{\partial w_i}=w_i -\eta \sum_n -\left(\hat y^n-f_{w,b}(x^n)\right)x_i^n</script><h5 id="Logistic-Regression-Square-error是否可行"><a href="#Logistic-Regression-Square-error是否可行" class="headerlink" title="Logistic Regression + Square error是否可行"></a>Logistic Regression + Square error是否可行</h5><p>按照之前的步骤，先得出$f_{w,b}(x),L(f)$的表达式，第三步再求导，可以发现一个问题，代入训练数据集的$\hat y$后，梯度总是为0，模型最后无法训练，所以这样的结合是不可行的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606115052273.png" alt="image-20200606115052273" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606115706601.png" alt="image-20200606115706601" style="zoom:50%;"></p>
<h5 id="Cross-Entropy-v-s-Square-Error"><a href="#Cross-Entropy-v-s-Square-Error" class="headerlink" title="Cross Entropy v.s. Square Error"></a>Cross Entropy v.s. Square Error</h5><p>下图我们将Cross entropy和square error进行了对比，黑色网格线表示cross entropy，红色表示square error</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606115544160.png" alt="image-20200606115544160" style="zoom:50%;"></p>
<p>对于cross entropy，loss变化较大，曲线比较sharp，相应的微分也较大，每次跨越的步长也较长</p>
<p>对于square error，loss曲线变化比较平缓，微分值很小，每次跨越的步长也小，当gradient接近于0的时候，参数就很有可能不再更新，训练也会停下来。就算将gradient设置为很小的值，使训练不那么容易停下来，但由于每次跨越的步长很小很小，也会出现训练非常缓慢的问题</p>
<h4 id="Logistic-vs-Linear-Regression"><a href="#Logistic-vs-Linear-Regression" class="headerlink" title="Logistic vs Linear Regression"></a>Logistic vs Linear Regression</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606121346151.png" alt="image-20200606121346151" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606121324734.png" alt="image-20200606121324734" style="zoom:50%;"></p>
<h4 id="Discriminative-v-s-Generative"><a href="#Discriminative-v-s-Generative" class="headerlink" title="Discriminative v.s. Generative"></a>Discriminative v.s. Generative</h4><p>logistic regression我们称之为Discriminative方法；而我们将gaussian来描述posterior probability，称之为Generative方法。虽然都使用了相同的函数表达式，但需要找到的参数却是不同的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606122040582.png" alt="image-20200606122040582" style="zoom:50%;"></p>
<p>logistic regression<strong>没有实质性的假设</strong>，要求直接找出对应的w和b。但generative model<strong>做出了假设</strong>，假设输入的数据是服从Gaussian分布的，需要先找出$\mu^1,\mu^2,\sum^{-1}$，再根据这些值得出相对应的w和b。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606143847987.png" alt="image-20200606143847987" style="zoom:50%;"></p>
<h5 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606143950210.png" alt="image-20200606143950210" style="zoom:50%;"></p>
<p>对于包含13个example 的训练数据，对于图中所示的测试数据，我们可以明显看出测试example属于Class1，那么通过Naive Bayes（朴素贝叶斯）计算的结果也是这样吗？下面我们将开始验证，</p>
<script type="math/tex; mode=display">
P(x|C1)=P(x_1=1|C_1)\times P(x_2=1|C_1)=1\times1\\
P(x|C2)=P(x_1=1|C_2)\times P(x_2=1|C_2)=\frac{1}{3}\times\frac{1}{3}</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606144006375.png" alt="image-20200606144006375" style="zoom:50%;"></p>
<p>根据这个计算结果可知，属于Class1的概率是小于0.5的，因此可以看出根据朴素贝叶斯算法算出，测试的example是属于Class2，和我们的直觉是相反的。==这是由于训练数据集中属于Class1的数量太少了，==比例只有1/13。在实际生活中的模型训练中，我们也必须要避免数据集的差异对实验结果造成的影响，数据集中每个类别所占的比例应该是差别不大的。</p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Logistic Regression</tag>
      </tags>
  </entry>
  <entry>
    <title>Improving Sequence Generation by GAN</title>
    <url>/2020/07/16/GANSeqNew/</url>
    <content><![CDATA[<p>本文首先会叙述GAN如何来改善Sequence Generation的问题，由于text generation在generator的输出是不连续的，会导致模型不能求微分，本文也提供了三种解决方案：Gumbel-softmax、Continuous Input for Discriminator、Reinforcement Learning，重点讲解了其中第二种方法；再来讲述Unsupervised Conditional Sequence Generation，使用CGAN来进行摘要提取、文字翻译、语音识别等。</p>
<h3 id="Conditional-Sequence-Generation"><a href="#Conditional-Sequence-Generation" class="headerlink" title="Conditional Sequence Generation"></a>Conditional Sequence Generation</h3><p>只要是sequence generation，我们都可以看作是conditional sequence generation，是有条件的。比如语音辨识系统，需要输入一段语音再进行识别；翻译任务也需要输入原文再进行翻译；聊天机器人也需要上文，才能知道下一句要说什么。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200715111557427.png" alt="image-20200715111557427" style="zoom:67%;"></p>
<h4 id="Review-Sequence-to-sequence"><a href="#Review-Sequence-to-sequence" class="headerlink" title="Review: Sequence-to-sequence"></a>Review: Sequence-to-sequence</h4><p>再回顾下聊天机器人的大概流程，首先会输入一段文字“How are you？”，输入encoder之后，再把code输入generator（decoder），我们希望输出序列是“I’m good”的概率最大。</p>
<p>现在有两个chatbot，一个输出“Not bad”，另一个输出“I’m John”。如果从人的观点来看，“Not bad”是一个更好的回答；但对于机器而言，“I’m John”则是一个更好的回答。因为机器的期望输出是“I’m good”，后者和正确答案至少有“I’m“是相同的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200715112137706.png" alt="image-20200715112137706" style="zoom:67%;"></p>
<p>首先我们会讲怎么用reinforce learning来Improving Supervised Seq-to-seq Model，再讲GAN。</p>
<h4 id="RL-human-feedback"><a href="#RL-human-feedback" class="headerlink" title="RL (human feedback)"></a>RL (human feedback)</h4><h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>如果现在我们想做chatbot，用reinforce learning的思想怎么来做呢？</p>
<p>chatbot在和人互动的过程中，会获得reward。如果人问“How are you？”，chatbot回答“ByeBye”，就给一个很低的分数（-10）；如果人说“Hello”，chatbot回复”Hi“，就给reward（+3）.</p>
<p>那么chatbot就希望自己可以得到最高的奖励，来maximize the expected reward。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200715113239286.png" alt="image-20200715113239286" style="zoom:67%;"></p>
<h5 id="Maximizing-Expected-Reward"><a href="#Maximizing-Expected-Reward" class="headerlink" title="Maximizing Expected Reward"></a>Maximizing Expected Reward</h5><p>对于input sequence c，chatbot会输出一个对应的response sentence x，Human再来比较这个c和x，给予一个评价$R(c,x)$。</p>
<p>这个结构和conditional generation很像，主要差别在于Human这个结构，和原来的discriminator有些不一样，现在的Human要把generator的input和output都考虑进来，才能得出评价分数。</p>
<p>现在chatbot就希望Human给出的评价分数越高越好，即maximize expected reward，可以用policy gradient来完成这件事。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200715113709825.png" alt="image-20200715113709825" style="zoom:67%;"></p>
<p>下图中的encoder和decoder合起来就是sequence to sequence模型，我们希望不断更新这个model的参数，来使Human给出的reward越高越好。这个公式表示在现有的参数下，Human给出的reward有多大，</p>
<script type="math/tex; mode=display">
\overline R_\theta=\sum_hP(h)\sum_xR(h,x)P_\theta (x|h)</script><p>其中$P(h)$表示输入的每个sequence所可能出现的概率，比如人输入”How are you”给chatbot很多次，那么对应的概率就要大；$P_\theta (x|h)$表示对于给定的h，chatbot给出的每个回复的概率；$R(h,x)$表示所有的回复。</p>
<p>我们的目标就是找到$\theta^*$，来使reward越大越好，即</p>
<script type="math/tex; mode=display">
\theta^*=arg\mathop{\rm max}_\theta \overline R_\theta</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200715114634632.png" alt="image-20200715114634632" style="zoom:67%;"></p>
<p>我们可以对期望的reward式子进行变化，</p>
<script type="math/tex; mode=display">
\begin{aligned}
\overline R_\theta &=\sum_hP(h)\sum_xR(h,x)P_\theta (x|h)\\
&=E_{h\sim P(h)}[E_{x\sim P_\theta(x|h)}[R(h,x)]]\\
&=E_{h\sim P(h),x\sim P_\theta(x|h)}[R(h,x)]\\
\end{aligned}</script><p>要计算这个期望值，就必须要计算所有c和x，但实际上，我们无法穷举出所有的input和output；</p>
<p>因此，我们只sample部分数据，从分布$P(h)$中sample出N个h，从$P_\theta(x|h)$中sample出N个x，即sample：$(h^1,x^1),(h^2,x^2),…,(h^N,x^N)$，这时我们的reward公式也发生了变化，即</p>
<script type="math/tex; mode=display">
\overline R_\theta \approx \frac{1}{N}\sum_{i=1}^NR(h^i,x^i)</script><p>现在$\theta$从reward的公式里面消失了，那么我们要怎么算出gradient来更新参数呢？</p>
<h5 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h5><p>我们可以先对有$\theta$的项求gradient，再使用最后的式子来做approximate，</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Delta\overline R_\theta &=\sum_hP(h)\sum_xR(h,x)\Delta P_\theta (x|h)\\
&\approx\frac{1}{N}\sum_{i=1}^NR(h^i,x^i)\Delta logP_\theta(x|h)
\end{aligned}</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200715223055794.png" alt="image-20200715223055794" style="zoom:67%;"></p>
<p>计算出对应的gradient之后，就可以使用gradient ascent来更新参数，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200715223851147.png" alt="image-20200715223851147" style="zoom:67%;"></p>
<p>那么gradient $\Delta \overline R_\theta$的具体含义是什么呢？</p>
<ul>
<li>如果$R(h^i,x^i)&gt;0$，就更新$\theta$来增加$P_\theta(x^i|h^i)$，即增加在给定的输入$h^i$的情况下，输出$x^i$的几率；</li>
<li>如果$R(h^i,x^i)&lt;0$，就更新$\theta$来减小$P_\theta(x^i|h^i)$。</li>
</ul>
<h5 id="Policy-Gradient-Implemenation"><a href="#Policy-Gradient-Implemenation" class="headerlink" title="Policy Gradient - Implemenation"></a>Policy Gradient - Implemenation</h5><p>那么我们如何使用policy gradient技术，让chatbot可以在reinforce learning的情景中，来学习如何与人进行对话呢？</p>
<p>在下图中，chatbot的参数是$\theta^t$，让chatbot与人进行对话，做一个sampling的process，如果人说的话是$c^i$，chatbot的回答是$x^i$，会得到对应的一个reward $R(c^i,x^i)$，一共有N个reward。</p>
<p>再计算出对应的gradient，更新参数$\theta$；每更新一次参数，都要重新收集charbot和人的对话。这点要和常规的gradient ascent进行区别，常规的gradient ascent算法可以很快进行下一次的参数更新，并不需要重新收集训练资料。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716095124819.png" alt="image-20200716095124819" style="zoom:67%;"></p>
<h5 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h5><p>下面我们对maximum likelihood和reinforcement learning进行比较。</p>
<p>首先是training data的区别，maximum likelihood的$\hat x^i$是人为标注的，是正确答案；但RL是人输入$c^i$，然后chatbot输出$x^i$，这里的$x^i$却不是人为标注的，并不一定是正确答案，有可能是正确答案，也有可能不是。</p>
<p>我们再对比下两者要最大化的objective function，RL多了一项$R(c^i,x^i)$，相当于为training data中的每个pair都加上了不同的weight $R(c^i,x^i)$；而Maximum Likelihood的training data中每个pair对应的weight都是1.</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716100802304.png" alt="image-20200716100802304" style="zoom:67%;"></p>
<h5 id="Alpha-GO-style-training"><a href="#Alpha-GO-style-training" class="headerlink" title="Alpha GO style training !"></a>Alpha GO style training !</h5><p>但reinforce learning训练起来是很麻烦的，人并没有那么多精力来和机器一直互动。因此，有人就提出了训练两个chatbot，让这两个chatbot互相问问题，有时候会出现一些正常的对话（右），但有时候很可能进入一个死循序（左）。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716112947010.png" alt="image-20200716112947010" style="zoom:67%;"></p>
<p>我们可以使用一个evaluate function来计算这个模型的训练结果，来观察这个对话到底好不好，如果是左边的两个chatbot，就给低分，如果是右边的两个chatbot，就给高分。</p>
<p>但这个evaluation function是人定的，复杂度是有限的。为了解决这个问题，我们可以引入GAN的概念。</p>
<h4 id="GAN-discriminator-feedback"><a href="#GAN-discriminator-feedback" class="headerlink" title="GAN (discriminator feedback)"></a>GAN (discriminator feedback)</h4><h5 id="Conditional-GAN"><a href="#Conditional-GAN" class="headerlink" title="Conditional GAN"></a>Conditional GAN</h5><p>现在有一个chatbot，输入和响应的sentence分别是c、x，我们可以用discriminator来替代人的角色，来评估c和x的匹配程度，来评价这是不是一个正常的人类对话。</p>
<p>为了能辨别到底是不是正常的人类对话，discriminator还需要输入大量的人类对话进行学习。chatbot的目标就是生成能骗过discriminator的对话。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716133621333.png" alt="image-20200716133621333" style="zoom:67%;"></p>
<p>这个过程就相当于是conditional GAN，input sequence c就相当于是一个condition，generator要生成既满足condition，还要能骗过discriminator的sequence。</p>
<p>下面我们将简要介绍一下chatbot和GAN结合的算法流程。首先需要初始化G和D的参数，再进入循环，</p>
<ul>
<li>先从training data中sample出input c和对应的response x；</li>
<li>再从input中sample出$c’$，输入generator，产生对应的response $\tilde x=G(c’)$；</li>
<li>同时将generator的input和output都输入D，更新D的参数，使其看到正确的$c,x$，就给高分；看到错误的$c’,\tilde x$，就给低分；训练这个discriminator既可以用传统的JS divergence，也可以用WGAN的思想；</li>
<li>还需要更新generator的参数，使其生成的sequence可以骗过discriminator，使D输出的分数越大越好。</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716133716259.png" alt="image-20200716133716259" style="zoom:67%;"></p>
<p>chatbot现在是一个<a href="https://scarleatt.github.io/2020/06/28/seq2seq/" target="_blank" rel="noopener">sequence to sequence</a>的model（RNN），首先要给一个condition作为输入，network输出词汇的distribution（这里我们假设只有两个词A，B），根据distribution再去做sample，就得出此次生成的词汇是B；把这个word和之前的condition再当作下一个时间点的输入，得出对应的distribution，……</p>
<p>这里的condition相当于是输入sequence的特征集合，必须在每次生成一个新词的时候都输入network，不然network可能会忘记输入sequence的一些关键信息。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716145251089.png" alt="image-20200716145251089" style="zoom: 40%;"></p>
<p>discriminator现在的输入就包括两部分，chatbot的input和output，输出一个评价分数scalar。现在我们要做的事是更新generator的参数，使其生成的sequence可以骗过discriminator，使discriminator的输出评价分数越大越好。</p>
<p>但现在会出现一个新的问题，这个network没有办法计算微分。如果这里是图像的生成，generator生成的图像全部直接丢给discriminator，没有sampling这个过程；但现在是文字生成问题，generator最后的结果会有一个sampling的过程，选择可能性最大的那个词作为输出，就不能计算微分；</p>
<p><strong>Q</strong>：为什么做了sampling过程就不能计算微分了？</p>
<p><strong>A</strong>：我们可以从微分的本质来回答这个问题，我们可以观察其输入的一点小小的变化，对输出会造成什么影响，这两者的变化值相除，就是对应的微分。现在回到我们chatbot的generator，如果我们对输入进行了小小的变化，由于进行了sampling，这个输出的变化是不确定的；很可能你现在sample了输入x出来，对x进行变化，本来y会产生相应的变化，但由于y没有被sample到，这个变化就没有表现出来。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716142825838.png" alt="image-20200716142825838" style="zoom:67%;"></p>
<h5 id="Three-Categories-of-Solutions"><a href="#Three-Categories-of-Solutions" class="headerlink" title="Three Categories of Solutions"></a>Three Categories of Solutions</h5><p>有三个方法可以解决这个问题</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716151953526.png" alt="image-20200716151953526" style="zoom:67%;"></p>
<h5 id="Solution-1-Gumbel-softmax"><a href="#Solution-1-Gumbel-softmax" class="headerlink" title="Solution 1: Gumbel-softmax"></a>Solution 1: Gumbel-softmax</h5><p>首先我们介绍一下Gumbel-softmax，是如何来解决不能求微分这个问题的。其核心思想就是把原来不能微分的变成了可以微分的东西。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716152122053.png" alt="image-20200716152122053" style="zoom: 67%;"></p>
<h5 id="Solution-2-Continuous-Input-for-Discriminator"><a href="#Solution-2-Continuous-Input-for-Discriminator" class="headerlink" title="Solution 2: Continuous Input for Discriminator"></a>Solution 2: Continuous Input for Discriminator</h5><p>现在我们介绍另外一个方法，把连续的输入给discriminator。</p>
<p>原来由于sampling这个过程，使得我们不能求微分，现在我们可以避开这个过程，直接把distribution给discriminator，就可以求微分了。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716152253777.png" alt="image-20200716152253777" style="zoom:67%;"></p>
<p>但这种做法会出现一个新的问题。对于real sentence，是one hot编码；而对于generated data，generator输出的是一个distribution，并不是one hot编码，discriminator现在可以根据是不是one hot编码，不考虑句子本身的语义，就可以轻易分辨出哪一个是真实的数据。</p>
<p>generator只要尽快生成one-hot编码，就可以轻易骗过discriminator。但这时generator生成的sequence是没有任何意义的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716152713801.png" alt="image-20200716152713801" style="zoom:67%;"></p>
<p>在condition generation中，如果是要进行sequence generation，要用的方法是把连续的输入给discriminator，我们可以使用<a href="https://scarleatt.github.io/2020/07/14/WGAN/" target="_blank" rel="noopener">WGAN</a>来完成这个训练。在使用WGAN时，要有一个额外的限制，discriminator是一个1-Lipschitz函数，要足够smooth才行。discriminator被限制后，就没那么容易分辨出到底哪一个是real sentence。</p>
<h5 id="Solution-3-Reinforcement-Learning"><a href="#Solution-3-Reinforcement-Learning" class="headerlink" title="Solution 3: Reinforcement Learning?"></a>Solution 3: Reinforcement Learning?</h5><p>现在我们使用reinforce learning的思想，来解决sampling之后不能求微分这个问题。</p>
<p>在之前的章节RL (human feedback)中，我们提到可以把discriminator当作人，人就知道怎么来调整chatbot的参数，使评价分数scalar越高越好，这里的评价分数也就相当于是reward。把人换成机器的discriminator，更新generator的参数，来提高discriminator的分数，也就等价于使reward最大化。我们可以用discriminator的输出$D(c,x)$，来替代这个reward $R(c,x)$。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716155238308.png" alt="image-20200716155238308" style="zoom:67%;"></p>
<p><strong>g-step</strong>：首先要训练generator，和typical RL相比，这里我们把Human换成了机器（discriminator），把所有的$R(c,x)$ 用$D(c,x)$来代替。之前的人来做discriminator，就非常花时间；但现在变成了机器来做discriminator，耗时就相对较小；</p>
<p><strong>d-step</strong>：再来训练discriminator。我们需要把真实的人类对话输入给D，还需要把generator生成的对话也给D，discriminator就可以学习来分辨这两种对话。</p>
<p>训练好discriminator之后，对应的$D(c^i,x^i)$也发生了变化，因此要重新训练generator，……，再重新训练discriminator，……</p>
<p>这个过程会不断反复地进行。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716161034464.png" alt="image-20200716161034464" style="zoom:67%;"></p>
<p>现在对于输入的sequence为$c^i=$“What is your name？”，chatbot给出的输出$x^i=$“I don’t know”，是不太好的对话，discriminator给出的分数$D(c^i,x^i)$是负的，我们就需要更新参数$\theta$来减小$P_\theta(x^i|c^i)$。</p>
<p>我们把输出$x^i=$“I don’t know”分成三个部分，用$x^i_1,x^i_2,x^i_3$分别表示“I”，“don’t”，“know”；那么我们现在要减小的目标就是，</p>
<script type="math/tex; mode=display">
logP_\theta(x^i|c^i)=logP(x_1^i|c^i)+logP(x_2^i|c^i,x_1^i)+logP(x_3^i|c^i,x_{1:2}^i)</script><p>要来减小$logP_\theta(x^i|c^i)$也就等价于，把$P(x_1^i|c^i),P(x_2^i|c^i,x_1^i),P(x_3^i|c^i,x_{1:2}^i)$减小；</p>
<p>如果现在$P(x_1^i|c^i)=P(‘’I’’|c^i)$，含义在给定的$c^i=$“What is your name？”条件下，输出”I”的几率。如果“I”当作正确答案的开头，是非常可行的，正确答案可以是“I am xx”等。但现在在训练过程中，chatbot看到“I”的可能性却是下降的，这是不合理的；如果产生了“dont”，“know”，这是合理的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716161943800.png" alt="image-20200716161943800" style="zoom:67%;"></p>
<p>如果现在chatbot的回答$x^i$是“I am John”，这是一个好的对话，discriminator就会给一个正的分数，再更新参数$\theta$来增加$P_\theta(x^i|c^i)$。</p>
<p>我们把输出$x^i=$“I am John”分成三个部分，用$x^i_1,x^i_2,x^i_3$分别表示“I”，“am“，“John”；那么我们现在要增加的目标就是，</p>
<script type="math/tex; mode=display">
logP_\theta(x^i|c^i)=logP(x_1^i|c^i)+logP(x_2^i|c^i,x_1^i)+logP(x_3^i|c^i,x_{1:2}^i)</script><p>要来增加$logP_\theta(x^i|c^i)$也就等价于，把$P(x_1^i|c^i),P(x_2^i|c^i,x_1^i),P(x_3^i|c^i,x_{1:2}^i)$增加；现在$P(x_1^i|c^i)=P(‘’I’’|c^i)$，这个几率是在上升的。</p>
<p>现在出现了一个新问题。对于同一个词”I“的概率，一个增加一个减小，如果sampleing的次数够多，是恰恰可以相互抵消的；但实际上，我们往往并不能sample到这么多的次数。</p>
<h5 id="Reward-for-Every-Generation-Step"><a href="#Reward-for-Every-Generation-Step" class="headerlink" title="Reward for Every Generation Step"></a>Reward for Every Generation Step</h5><p>输入为$c^i=$“What is your name？”，输出$x^i=$“I don’t know”，是不好的对话。但造成这个不好的原因并不是因为在开头sample除了“I”，而是后面的“dont”，“know”。因此，我们希望机器可以学习到对话为什么不好，需要对原来的式子进行改写，</p>
<script type="math/tex; mode=display">
\Delta\overline R_\theta\approx\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T (Q(c^i,x_{1:t}^i)-b)\Delta logP_\theta(x^i_t|c^i,x_{1:t-1}^i)</script><p>其中$P_\theta(x^i_t|c^i,x_{1:t-1}^i)$表示在输入的sequence $c^i$，以及产生了t-1个word的情况下，产生$x^i_t$的概率；计算reward的方式也发生了变化，变成了$Q(c^i,x_{1:t}^i)-b)$，会对每一个时间点新生成的word进行evaluation，而不是对整个输出的句子做evaluation。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716170735579.png" alt="image-20200716170735579" style="zoom:67%;"></p>
<h5 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a><strong>Experimental Results</strong></h5><p>下面是实验结果的展示。</p>
<p>对于MLE（Maximum Likelihood Evaluation）的生成结果，会频繁出现“I’m sorry”，“I dont know”。如果对应到图像的问题，这些频繁出现的句子就很像我们之前用VAE生成的模糊影像，由于有很多种火车，因此VAE选择了平均值，就会变的很模糊；再回到这里的文本生成问题，chatbot会生成很多个不同的答案 ，如果要同时maximum这些不同答案的likelihood，就会出现一些很奇怪的句子。</p>
<p>对于GAN训练出来的chatbot，就会产生一些很长、有内容的句子。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716172626169.png" alt="image-20200716172626169" style="zoom:67%;"></p>
<h5 id="More-Applications"><a href="#More-Applications" class="headerlink" title="More Applications"></a>More Applications</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716173533935.png" alt="image-20200716173533935" style="zoom:67%;"></p>
<p><span style="color: red">如果在训练seq2seq的模型，我们可以考虑用GAN来改善模型生成的结果。</span></p>
<h3 id="Unsupervised-Conditional-Sequence-Generation"><a href="#Unsupervised-Conditional-Sequence-Generation" class="headerlink" title="Unsupervised Conditional Sequence Generation"></a>Unsupervised Conditional Sequence Generation</h3><h4 id="Text-Style-Transfer"><a href="#Text-Style-Transfer" class="headerlink" title="Text Style Transfer"></a>Text Style Transfer</h4><p><a href="https://scarleatt.github.io/2020/07/06/CycleGAN/" target="_blank" rel="noopener">在前面的文章中，我们已经提到过如何用GAN来进行image style transformation，</a>比如把风景照转成梵谷的画，把male audio转成female audio等。那么我们现在也可以对文字进行style transformation，把正面的句子看作是一种style，把负面的句子看作是另外一种style。</p>
<p>如果我们要进行text style transfer，只需要一堆正面的句子，一堆负面的句子，分别当作是两个domain的数据，应用CycleGAN，就可以训练这个network。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716173811779.png" alt="image-20200716173811779" style="zoom:67%;"></p>
<h5 id="Direct-Transformation"><a href="#Direct-Transformation" class="headerlink" title="Direct Transformation"></a>Direct Transformation</h5><p>这是图像进行风格转换的结构图，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716175248999.png" alt="image-20200716175248999" style="zoom:67%;"></p>
<p>如果我们要对文本进行风格转换，把上图中的图像换成文字，即把positive sentence算成一个domain，把negative sentence算成是另外一个domain，用cycleGAN算法来进行训练即可。</p>
<p>这样做就会产生一个问题，generator的输出是discrete的，是进行sampling的结果，当generator $G_{X\rightarrow Y}$和discriminator $D_Y$连起来的时候，就不能进行微分，也就不能训练。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716175344334.png" alt="image-20200716175344334" style="zoom:67%;"></p>
<p>上文已经针对这种问题提出了三种解法，这里我们选择Solution 2: Continuous Input for Discriminator，把每个word都进行word embedding，都用一个vector来进行替代，整个句子就相当于是vector的sequence，是连续的；而且word embedding的结果并不是one-hot编码，并不会对结果造成其他的影响。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716180409232.png" style="zoom:67%;"></p>
<p>这里是实验结果的展示，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716180441236.png" alt="image-20200716180441236" style="zoom:67%;"></p>
<h5 id="Projection-to-Common-Space"><a href="#Projection-to-Common-Space" class="headerlink" title="Projection to Common Space"></a><strong>Projection to Common Space</strong></h5><p>现在讲另外一种transform的方式，输入真实人物的图像，网络输出动漫人物的图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716180633945.png" alt="image-20200716180633945" style="zoom:67%;"></p>
<p>我们也可以进行这种风格的文本转换，同样会出现结果是discrete的问题，我们也可以选择三种方式的其中一种来进行改进。这里我们简要叙述两种解决办法：</p>
<ol>
<li>由于decoder的结果是discrete的，但decoder的hidden layer的结果却不是discrete的，是Continuous的；</li>
<li>通过两个encoder把两个不同domain的sentence，都映射到同一个space，还需要加一个domain discriminator，可以对domain X和Y的encoder所输出的vector进行判断，看到底是属于哪一个domain的图像。如果这个domain discriminator不能进行判断，那么我们就可以认为这两个encoder所生成的vector其distribution都是一样的，从而这两个distribution中相同的维度表示相同的意思。</li>
</ol>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716180900039.png" alt="image-20200716180900039" style="zoom:67%;"></p>
<h4 id="Unsupervised-Abstractive-Summarization"><a href="#Unsupervised-Abstractive-Summarization" class="headerlink" title="Unsupervised Abstractive Summarization"></a>Unsupervised Abstractive Summarization</h4><h5 id="Abstractive-Summarization"><a href="#Abstractive-Summarization" class="headerlink" title="Abstractive Summarization"></a>Abstractive Summarization</h5><p>我们可以训练一个提取摘要的模型，输入一篇文章，输出为对应的摘要。以前的技术是这样来操作的，先给机器一篇文章，机器来判断每个句子重不重要，重要的句子就会被extract出来，然后被拼接成全文的摘要。</p>
<p>但这样生成的摘要不太好，我们应该在理解文章之后，用自己的话把摘要写出来。我们可以训练一个sequence to sequence的模型，先给machine一堆文章及其对应的标注摘要，模型训练完成后，就可以总结出一篇新文章的摘要。</p>
<p>但这个模型的训练需要大量的资料，差不多要收集100万个example，机器才能训练出来这个模型。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716200238426.png" alt="image-20200716200238426" style="zoom:67%;"></p>
<h5 id="Review-Unsupervised-Conditional-Generation"><a href="#Review-Unsupervised-Conditional-Generation" class="headerlink" title="Review: Unsupervised Conditional Generation"></a>Review: Unsupervised Conditional Generation</h5><p>我们可以把文章看成一种domain，把摘要看成是另外一种domain，使用GAN的技术，就可以用unsupervised的方法，不需要收集两个domain之间的pair &lt;文章，摘要&gt;。使用GAN的方法后，我们只需要收集domain X的一堆数据（文章），domain Y的一堆数据（摘要），训练完成后，机器就可以自动实现domain X和Y之间的互转。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716201652026.png" alt="image-20200716201652026" style="zoom:67%;"></p>
<h5 id="Unsupervised-Abstractive-Summarization-1"><a href="#Unsupervised-Abstractive-Summarization-1" class="headerlink" title="Unsupervised Abstractive Summarization"></a><strong>Unsupervised Abstractive Summarization</strong></h5><p>这个技术和cycleGAN很像。</p>
<p>首先训练一个seq2seq的model，输入一篇文章，输出一段sequence；我们还需要一个discriminator，看过很多其他人写的摘要，可以对输出的sequence进行检测，看这个sequence到底是不是人写的摘要。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716202222856.png" alt="image-20200716202222856" style="zoom:67%;"></p>
<p>现在会出现一个新问题，这个输出的sequence可以很像人写的摘要，但这个摘要和输入的document没有关系。</p>
<p>我们可以再加一个seq2seq模型，可以将第一个seq2seq模型生成的摘要在进行reconstruct，看新生成的document是不是和最开始的input接近，即最小化reconstruction error。那么现在第一个seq2seq模型的目标就有两个：产生能骗过discriminator的摘要；产生的摘要必须是和原文接近的内容。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716203005609.png" alt="image-20200716203005609" style="zoom:67%;"></p>
<p>我们也可以从另外一个角度来理解这个模型，</p>
<p>先输入一个document，生成摘要后，再reconstruct回原来的document，这可以看作是一个<strong>seq2seq2seq auto-encoder</strong>，中间的摘要就可以看作是encoder之后的code；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716203431808.png" alt="image-20200716203431808" style="zoom:67%;"></p>
<p>但这个code并不一定是可读的。为了让model产生人类可以阅读的摘要，我们需要一个discriminator，这个discriminator学习了大量人类写的摘要，可以来判断生成的摘要是不是readable。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716203947531.png" alt="image-20200716203947531" style="zoom:67%;"></p>
<p>这里展示了部分实验结果，<span style="color: red">unsupervised</span>表示Unsupervised Abstractive Summarization，可以发现生成的摘要比较保守，由于generator会很想生成能骗过discriminator的摘要，通常generator会去原文中提取一段话。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716204312828.png" alt="image-20200716204312828" style="zoom:67%;"></p>
<p>也有一些失败的例子，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716204635651.png" alt="image-20200716204635651" style="zoom:67%;"></p>
<p>这里是几种对比实验的展示，分别是WGAN，RL，Supervised。纵轴表示生成摘要的评价指标，越高越好，横轴表示训练资料的数量。黑色表示使用了380万个pair作为训练资料的结果。</p>
<p>如果我们先用unsupervised的方法来训练我们的模型，把模型训练得很强，再用少量的label data来进行fine-tune，进步就会很快。在下图中，我们只使用了50万个有label的数据，就可以达到和supervise的方法一样好的效果。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716204750146.png" alt="image-20200716204750146" style="zoom:67%;"></p>
<h4 id="Unsupervised-Translation"><a href="#Unsupervised-Translation" class="headerlink" title="Unsupervised Translation"></a>Unsupervised Translation</h4><h5 id="Unsupervised-Machine-Translation"><a href="#Unsupervised-Machine-Translation" class="headerlink" title="Unsupervised Machine Translation"></a>Unsupervised Machine Translation</h5><p>我们也可以用unsupervised的方法来进行机器翻译，比如把法语当作是一种domain X，把中文当作是另外一种domain Y；我们就可以使用这种cycleGAN的思想，来进行domain之间的转换。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716205936509.png" alt="image-20200716205936509" style="zoom:67%;"></p>
<p>facebook新发的论文也证明这种思想是非常可行的。纵轴表示摘要的好坏，横轴表示训练资料的数量。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716210148668.png" alt="image-20200716210148668" style="zoom:67%;"></p>
<h5 id="Unsupervised-Speech-Recognition"><a href="#Unsupervised-Speech-Recognition" class="headerlink" title="Unsupervised Speech Recognition"></a>Unsupervised Speech Recognition</h5><p>既然语音信号之间可以互相转换，语言之间也可以互相转换，那么语音信号和语音之间能进行转换吗？</p>
<p>其实这也是可以办到的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716210619685.png" alt="image-20200716210619685" style="zoom:67%;"></p>
<p>这里是具体的实验结果，红色和蓝色直线表示使用unsupervised的方法，发现只有30%的准确率；而对于supervised的方法，训练资料越多，模型的准确率越高。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200716210738016.png" alt="image-20200716210738016" style="zoom:67%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>CGAN</tag>
        <tag>GAN</tag>
        <tag>Sequence-to-sequence</tag>
        <tag>Reinforce Learning</tag>
        <tag>Policy Gradient</tag>
        <tag>Text Style Transfer</tag>
        <tag>Abstractive Summarization</tag>
      </tags>
  </entry>
  <entry>
    <title>Mac安装Pytorch</title>
    <url>/2020/03/29/Mac%E5%AE%89%E8%A3%85Pytorch/</url>
    <content><![CDATA[<p>先创建一个名为<code>pytorch</code>的虚拟环境，</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda create -n pytorch python=3.7 numpy matplotlib pandas jupyter notebook</span><br></pre></td></tr></table></figure>
<p>激活虚拟环境</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda activate pytorch</span><br></pre></td></tr></table></figure>
<p>去官网选择合适的pytorch版本的安装命令<a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener">https://pytorch.org/get-started/locally/</a></p>
<p><img src="/2020/03/29/Mac安装Pytorch/image-20200329101220290.png" alt="image-20200329101220290"></p>
<p>复制到命令行下，回车运行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda install pytorch torchvision -c pytorch</span><br></pre></td></tr></table></figure>
<p>会出现以下报错信息：</p>
<p><strong>CondaHTTPError: HTTP 000 CONNECTION FAILED for url</strong></p>
<p><strong>解决方法：添加镜像站到Anaconda</strong></p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda config --add channels http://mirror.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --add channels http://mirror.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --set show_channel_urls yes</span><br><span class="line"></span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ </span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/ </span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/ </span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/ </span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br></pre></td></tr></table></figure>
<p>再把官网安装命令的<code>-c pytorch</code>去掉，运行以下命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda install pytorch torchvision</span><br></pre></td></tr></table></figure>
<p>检测pytorch是否安装成功，命令行运行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">jupyter notebook</span><br></pre></td></tr></table></figure>
<p>运行测试代码，出现以下结果，则说明安装成功</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.FloatTensor([<span class="number">5</span>, <span class="number">3</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/29/Mac安装Pytorch/image-20200329102655979.png" alt="image-20200329102655979"></p>
]]></content>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>RDF+SPARQL对比</title>
    <url>/2020/07/15/RDF-SPARQL/</url>
    <content><![CDATA[<h5 id="RDF语义模型-–-关系显式定义"><a href="#RDF语义模型-–-关系显式定义" class="headerlink" title="RDF语义模型 – 关系显式定义"></a>RDF语义模型 – 关系显式定义</h5><p>RDF模型关系是显式定义的，比如我们现在有以下信息，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Tencent located_in Shengzhen </span><br><span class="line">Shengzhen located_in China</span><br></pre></td></tr></table></figure>
<p>还需要对$located_in$这个属性进行定义，我们将其定义为可传递的，即</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">located_in type transitiveProperty</span><br></pre></td></tr></table></figure>
<p>我们可以从这些信息中进行推理，推理出Tencent和China的关系，即</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Tencent located_in China</span><br></pre></td></tr></table></figure>
<p>如果现在问一个问题“In which country is Tencent located?”，RDF语义模型可以很快给出答案，“In China”。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200715171338405.png" alt="image-20200715171338405" style="zoom:100%;"></p>
<h5 id="关系模型-–-关系隐式声明"><a href="#关系模型-–-关系隐式声明" class="headerlink" title="关系模型 – 关系隐式声明"></a>关系模型 – 关系隐式声明</h5><p>但对于关系模型，关系则是隐式声明的。如果我们要回答问题“In which country is Tencent located?”，则需要用三个数据库（Company，City，Company_city），在查询时则需要将三个表连接起来，最后才能得出答案“In China”。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200715171833261.png" alt="image-20200715171833261" style="zoom:100%;"></p>
<h5 id="当数据发生了变更-–-语义模型如何应对"><a href="#当数据发生了变更-–-语义模型如何应对" class="headerlink" title="当数据发生了变更 – 语义模型如何应对"></a>当数据发生了变更 – 语义模型如何应对</h5><p>如果数据发生了变化，比如加入了新城市Guangzhou与Shengzhen、China的关系，现在的数据就变成了</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Tencent located_in Shengzhen </span><br><span class="line">Shengzhen loacted_in Guangdong </span><br><span class="line">Guangdong located_in China</span><br></pre></td></tr></table></figure>
<p>对于RDF语义模型，我们可以推理出新的信息，即</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Shengzhen located_in China </span><br><span class="line">Tencent located_in China</span><br></pre></td></tr></table></figure>
<p>回答问题“In which country is Tencent located?”，也可以很轻松地得出来答案“In China”</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200715172137411.png" alt="image-20200715172137411"></p>
<p>对于关系模型，我们就会发现之前的三张表不能用了，需要换成4张表，即Company，State，Company_City，City_State，这时原来的sql查询语句也不能继续使用了。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200715173517113.png" alt="image-20200715173517113"></p>
<h5 id="数据的智能性如何体现"><a href="#数据的智能性如何体现" class="headerlink" title="数据的智能性如何体现?"></a>数据的智能性如何体现?</h5><p>左图表示关系数据，智能性更多体现在应用的代码（sql code）上；</p>
<p>右图为加入了知识图谱的推理，关系显式地被表现出来，对图加一些点和边也很容易；在我们使用RDFS时，也会加入一些新的词汇（本体、属性等），这些词汇本身就有一定的推理能力，也就形成了更加智能的数据。对这些智能的数据也可以再进一步推理。</p>
<p>知识图谱的查询是蕴含在推理过程中的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200715173710175.png" alt="image-20200715173710175"></p>
]]></content>
      <categories>
        <category>Knowledge Graph</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Knowledge Graph</tag>
        <tag>RDF</tag>
      </tags>
  </entry>
  <entry>
    <title>Recurrent Neural Network</title>
    <url>/2020/06/10/RNN/</url>
    <content><![CDATA[<h4 id="Example-Application"><a href="#Example-Application" class="headerlink" title="Example Application"></a>Example Application</h4><p>将Taipei作为向量的输入，由于RNN网络是有记忆的，因此可以根据输入来预测目的地dest，以及离开的时间</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609165306993.png" alt="image-20200609165306993" style="zoom:50%;"></p>
<h4 id="RNN-Example"><a href="#RNN-Example" class="headerlink" title="RNN Example"></a>RNN Example</h4><p>RNN是一个有记忆的网络。对于输入的向量$(x_1,x_2)$，hidden neural的输入就包含store和input的neural，本次hidden neural的输出又作为下一次hidden neural的部分输入</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609165404347.png" alt="image-20200609165404347" style="zoom:50%;"></p>
<p>在下图中，对于给定的input sequence为(1,1),(1,1),(2,2)，所有的weights都出实话为1，没有bias项，所有的activation function都是linear的</p>
<p>如果input sequence的第一个输入为(1,1)，store neural的初始值为0，则hidden neural的输出为2，把hidden neural的再放到store neural里（红色箭头），output neural的输出为$y_1=4,y_2=4$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609170004564.png" alt="image-20200609170004564" style="zoom:50%;"></p>
<p>第二个input为(1,1)，store neural的值为(2,2)，那么hidden layer的输出为$2+2+1+1=6$，再把hidden neural 的输出作为store neural的值，对应的$y_1=2,y_2=12$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609170504030.png" alt="image-20200609170504030" style="zoom:50%;"></p>
<p>第三个input为(2,2)，store neural的值为(6,6)，那么hidden neural的输出为$6+6+2+2=16$，对应的$y_1=32,y_2=32$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609170819485.png" alt="image-20200609170819485" style="zoom:50%;"></p>
<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><h5 id="Basic-Concept"><a href="#Basic-Concept" class="headerlink" title="Basic Concept"></a>Basic Concept</h5><p>下面将在具体叙述RNN网络的原理。</p>
<p>如果input sequence为”arrive Taipei on November 2nd”，那么第一个input记为$x^1=arrive$，再通过hidden neural计算出$a^1$，这里需要把$a^1$存入到store neural（作为下一次hidden neural计算的部分输入），再通过output neural计算出$y^1$;</p>
<p>那么第二个input记为$x^2=Taipei$，其中$x^2,a^1$都是hidden neural的输入，再通过hidden neural计算出$a^2$，这里需要把$a^2$存入到store neural（作为下一次hidden neural计算的部分输入），再通过output neural计算出$y^2$;</p>
<p>…</p>
<p>RNN的训练并不是多个网络的叠加，即在下图中，RNN不是有三个不同的network，只有一个network，每次更新相对应的input和其他neural的值即可</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609171227640.png" alt="image-20200609171227640" style="zoom:50%;"></p>
<p>这里还有一个需要注意的点，即使是input sequence中相同的单词输入(Taipei)，由于input sequence的顺序不同，store neural内存的值也不同，因此output neural的输出值也不同</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609171916224.png" alt="image-20200609171916224" style="zoom:50%;"></p>
<h5 id="Of-course-it-can-be-deep-…"><a href="#Of-course-it-can-be-deep-…" class="headerlink" title="Of course it can be deep …"></a>Of course it can be deep …</h5><p>和其他的神经网络一样，RNN也可以加深他的网络层次</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609172301903.png" alt="image-20200609172301903" style="zoom:50%;"></p>
<h5 id="Elman-Network-amp-Jordan-Network"><a href="#Elman-Network-amp-Jordan-Network" class="headerlink" title="Elman Network &amp; Jordan Network"></a>Elman Network &amp; Jordan Network</h5><p>下面将叙述两种不同的RNN网络。</p>
<p>Elman Network类似于前文讲的rnn network，将hidden neural的输出作为store neural的值；而Jordan Network则是将output neural的值存入store neural；</p>
<p>其中Jordan Network的performance比较好，因为Jordan是直接将更清晰的值target存入store neural，而Elman是将学习到的值（hidden neural的输出）存入store neural</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609172444143.png" alt="image-20200609172444143" style="zoom:50%;"></p>
<h5 id="Bidirectional-RNN"><a href="#Bidirectional-RNN" class="headerlink" title="Bidirectional RNN"></a>Bidirectional RNN</h5><p>RNN不仅可以是单向的，也可以是双向的，可以train一个正向的network，也同时是train一个逆向的network。如下图所示，input sequence为$…,x^t,x^{t+1},x^{t+2},…$，正向和逆向hidden neural的输出同时作为output neural的输入，从而输出target的值$…,y^t,y^{t+1},y^{t+2},…$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609173058539.png" alt="image-20200609173058539" style="zoom:50%;"></p>
<p>这种双向的RNN对我们的训练是有好处的。对于图中的output $y^{t+1}$，如果只是单向RNN，那么$y^{t+1}$只可以看到$x^1,…,x^{t+1}$的值；但现在是双向的RNN，$y^{t+1}$不仅可以看到$x^1,…,x^{t+1}$的值，还可以看到$x^{t+2},…$到句尾的值，<u>即整个input sequence的值都可以被双向的RNN看到，从而得出更好的训练结果</u></p>
<h4 id="The-Problem-of-Long-Term-Dependencies"><a href="#The-Problem-of-Long-Term-Dependencies" class="headerlink" title="The Problem of Long-Term Dependencies"></a>The Problem of Long-Term Dependencies</h4><p>在某些情况下，某些词比如$h^3$只和最近的某些输入$x_0,x_1$有关。比如我们要给予当前输入的序列来预测下一个可能的输出，输入序列是“the clouds are in the ”，我们想要预测下一个词是“sky”，即“the clouds are in the _sky_”，我们根据最近的信息“cloud”就可以推测出来，并不需要其他的上下文信息；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200717221753851.png" alt="image-20200717221753851"></p>
<p>但现实生活中还有很多情况，根据最近的信息无法进行预测，还要结合更早输入的信息。比如输入的序列是“I grew up in France… I speak fluent ”，我希望预测出“<em>French</em>”，即“I  grew up in France… I speak fluent <em>French</em>”。由于France和French之间有很多其他的句子，RNN无法处理这个问题。</p>
<p>Q：为什么RNN无法处理这个问题？</p>
<p>A：序列后面的梯度很难反向传播到前面的序列，就产生了梯度消失问题。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200717222433437.png" alt="image-20200717222433437"></p>
<h4 id="Long-Short-term-Memory-LSTM"><a href="#Long-Short-term-Memory-LSTM" class="headerlink" title="Long Short-term Memory (LSTM)"></a>Long Short-term Memory (LSTM)</h4><h5 id="three-gates"><a href="#three-gates" class="headerlink" title="three gates"></a>three gates</h5><p>下图中的neural有4个input和1个output，一共有三个gate：</p>
<ul>
<li>Input Gate：决定其他neural的值是否可以输入这个neural；是否可以输入由neural的学习情况而定</li>
<li>Forget Gate：什么时候要把memory里面的东西forget，<u>打开表示memory，关闭表示forget</u>；具体什么时候要自己学习</li>
<li>Output Gate：其他的neural可不可以到这个neural把值取出来，打开的时候才可以取出来，关闭的时候则不可以；打不打开可以自己学习，自己决定</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609175328153.png" alt="image-20200609175328153" style="zoom:50%;"></p>
<h5 id="formula"><a href="#formula" class="headerlink" title="formula"></a>formula</h5><p>上面只是对LSTM做了一个简要的叙述，下面将开始详细叙述。下图中的activation function f为sigmoid function，黑色圆点表示multiply；$z_i,z_o,z_f$分别操控input、output、forget gate；z表示外界要存入的值，</p>
<p><u>如果$f(z_f)$的输出值为1，表示打开forget gate（memory）；输出值为0，表示关闭forget gate（forget）</u></p>
<p>对于输入的z值，经过activation function g运算后为$g(z)$，相应的$z_i$经过运算得$f(z^i)$，c为Memory的初始值，当forget gate开启的时候，c’表示经过一次forget gate打开时进行计算后更新的值</p>
<script type="math/tex; mode=display">
c'=g(z)f(z_i)+cf(z_f)</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609181527674.png" alt="image-20200609181527674" style="zoom:50%;"></p>
<p>memory cell的输出再输入activation function h，输出$h(c’)$，即可得出该neural的output为a</p>
<script type="math/tex; mode=display">
a=h(c')f(z_0)</script><h5 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609223053105.png" alt="image-20200609223053105" style="zoom:50%;"></p>
<p>如下图所示，箭头上的数字表示weight，activation function g和h都是linear function，LSTM初始状态如下</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609225029699.png" alt="image-20200609225029699" style="zoom:50%;"></p>
<p>第一个input vector为$z=(x_1,x_2,x_3,bias)^T=(3,1,0,1)^T$，则$g(z)=3$，Input Gate为$z_i=100\rightarrow f(z_i)\approx1$，Forget Gate为$z_f=110\rightarrow f(z_f)\approx 1$，Memory Cell内的$c=0$，那么memory cell更新后的值为c’</p>
<script type="math/tex; mode=display">
\begin{aligned}
c'&=g(z)f(z_i)+cf(z_f) \\
&=3*1+0*1\\
&=3
\end{aligned}</script><p>memory cell的输出再输入activation function h (linear)，输出$h(c’)=3$，Output Gate为$z_o=0\rightarrow f(z_0)\approx0$，即可得出该neural的output为a</p>
<script type="math/tex; mode=display">
a=h(c')f(z_0)=0</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609224201819.png" alt="image-20200609224201819" style="zoom:50%;"></p>
<p>第二个input vector为$z=(x_1,x_2,x_3,bias)^T=(4,1,0,1)^T$，…..,其中Memory保存了上一次更新的值3，同理可得a=0</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609225345356.png" alt="image-20200609225345356" style="zoom:50%;"></p>
<p>第三个input vector为$z=(x_1,x_2,x_3,bias)^T=(2,0,0,1)^T$，…..,同理可得a=0</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609225442395.png" alt="image-20200609225442395" style="zoom:50%;"></p>
<p>第四个input vector为$z=(x_1,x_2,x_3,bias)^T=(1,0,1,1)^T$，则$g(z)=1$，Input Gate为$z_i=0\rightarrow f(z_i)\approx0$，Forget Gate为$z_f=10\rightarrow f(z_f)\approx 1$，Memory Cell内的$c=7$，那么memory cell更新后的值为c’</p>
<script type="math/tex; mode=display">
\begin{aligned}
c'&=g(z)f(z_i)+cf(z_f) \\
&=1*0+7*1\\
&=7
\end{aligned}</script><p>memory cell的输出再输入activation function h (linear)，输出$h(c’)=7$，Output Gate为$z_o=90\rightarrow f(z_0)\approx1$，即可得出该neural的output为a=7</p>
<script type="math/tex; mode=display">
a=h(c')f(z_0)=7</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609225546396.png" alt="image-20200609225546396" style="zoom:50%;"></p>
<h5 id="Original-Network-vs-LSTM"><a href="#Original-Network-vs-LSTM" class="headerlink" title="Original Network vs LSTM"></a>Original Network vs LSTM</h5><p>LSTM就是把原来的hidden layer中的neural替换为LSTM cell</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610095121581.png" alt="image-20200610095121581" style="zoom:50%;"></p>
<p>将现在的 $(x_1,x_2)$乘以不同的weight，从而可以作为4个不同输入，即input、input gate、forgte gate、output gate</p>
<p>一般的neural network只需要操控$(x_1,x_2)$，但LSTM需要操控4个不同的input，因此参数量多了4倍</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610095605339.png" alt="image-20200610095605339" style="zoom:50%;"></p>
<h5 id="LSTM-with-neural-network"><a href="#LSTM-with-neural-network" class="headerlink" title="LSTM with neural network"></a>LSTM with neural network</h5><p>z的维数则表示LSTM中memory cell（LSTM中红色方框）的个数，z的第一维表示第一个LSTM cell的input，第二维表示第二个LSTM cell的input，……</p>
<p>因此一共有4个不同的vector $z_f,z_iz,z_o$，作为network的input，操控整个LSTM的运转</p>
<p>输入到第一个LSTM cell的值是vector $z_f,z_iz,z_o$中的第一维数据，每个cell的输入值都是不一样的</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610101406355.png" alt="image-20200610101406355" style="zoom:50%;"></p>
<p>$x^t$进行不同的transform，可以得出4个不同的input $z_f,z_iz,z_o$</p>
<p>首先对于输入的z值，经过activation function g运算后为$g(z)$，相应的$z_i$经过运算得$f(z_i)$，$g(z)$再和 $f(z_i)$相乘（element-wise），memory cell的初始值为$c^{t-1}$，经过运算可得当前的值$c^t=g(z)f(z_i)+c^{t-1}f(z_f)$，经过activation function h计算出$h(c^t)$，再和output gate的输出值$f(z_o)$结合，得出当前LSTM cell的输出值$y^t=h(c^t)f(z_0)$，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610102846836.png" alt="image-20200610102846836" style="zoom:50%;"></p>
<h5 id="LSTM-Extension"><a href="#LSTM-Extension" class="headerlink" title="LSTM Extension"></a>LSTM Extension</h5><p>下图展示了两个LSTM cell的extension，在时间点t进行的计算，不仅要包括 $x^{t+1}$，还要包括上个时间点中lstm cell算出的值，即memory cell的输出$c^t$，以及$c^t$再输入activation function的输出值$h(c^t)$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610103840723.png" alt="image-20200610103840723" style="zoom:50%;"></p>
<h4 id="RNN-vs-LSTM"><a href="#RNN-vs-LSTM" class="headerlink" title="RNN vs LSTM"></a>RNN vs LSTM</h4><p><span style="color: red">传统的RNN和LSTM的区别：</span></p>
<ul>
<li>传统的RNN通常是直接覆盖的形式，当前时间点的计算结果直接覆盖上一个时间节点的计算结果，可以处理短期依赖问题，无法处理长期依赖问题；</li>
<li>LSTM通常采用了“累加”的形式，上一个时间节点的计算结果会影响之后的计算结果，并不是直接覆盖；可以处理短期和长期依赖问题。</li>
<li>对比两者的参数更新公式，</li>
</ul>
<p>首先是<strong>传统的RNN</strong>，网络单元结构示意图，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200717221422068.png" alt="image-20200717221422068" style="zoom:67%;"></p>
<p>参数更新公式如下，RNN本时刻的隐藏层信息只来源于当前输入和上一时刻的隐藏层信息，没有记忆功能。</p>
<script type="math/tex; mode=display">
h_t=tanh(W_h\cdot [h_{t-1},x_t]+b_h)</script><p>对于<strong>LSTM</strong>而言，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200717221403866.png" alt="image-20200717221403866" style="zoom:67%;"></p>
<p>参数更新公式如下，</p>
<script type="math/tex; mode=display">
c'=g(z)f(z_i)+cf(z_f)\\
a=h(c')f(z_0)</script><h4 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h4><p>Training Sentences为”arrive Taipei on November 2nd”，当Taipei输入时，必须要先计算出前一层hidden layer的输出值</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610112127379.png" alt="image-20200610112127379" style="zoom:50%;"></p>
<p>RNN的训练也是前向和逆向传播，其中逆向传播算法为Backpropagation through time（BPTT），也有一个参数更新公式</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610112423397.png" alt="image-20200610112423397" style="zoom:50%;"></p>
<p>但基于RNN的network并不是很容易训练，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610112627265.png" alt="image-20200610112627265" style="zoom:50%;"></p>
<h5 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h5><p>下图中的纵轴表示total loss，横轴表示两个weight w1和w2。error surface非常的rough，图像要么很flat，要么非常steep</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610112709861.png" alt="image-20200610112709861" style="zoom:50%;"></p>
<p>假设图中最右方的橙色点表示初始值，由于刚开始error surface比较flat，gradient也比较小，learning rate比较大；经过多次的训练后，如果这时候刚好到达悬崖和平原的交接处，这一次的gradient就超级大，再加上之前很大的learning rate，loss可能就会直接飞出去了</p>
<h5 id="Clipping"><a href="#Clipping" class="headerlink" title="Clipping"></a>Clipping</h5><p>改进措施是clipping，当gradient大于15的时候，就看做是15，不再继续增加，这时gradient的移动方向就是图中的蓝色虚线部分，就不会再飞出去了，仍然可以继续做RNN training </p>
<h5 id="Why？"><a href="#Why？" class="headerlink" title="Why？"></a>Why？</h5><p>Q：由于之前的文章中提到sigmoid function会产生gradient vanshing问题，那么造成RNN训练困难的原因是因为activation function吗 ？</p>
<p>A：有学者将sigmoid function 替换为ReLU，发现RNN网络给出了更差的performance，因此这个问题的产生并不是因为activation function的问题</p>
<p>最直观的解决方式是：将某一个参数的gradient进行变化，观察这个变化会对output产生的影响</p>
<p>对于下图中的toy example，input sequence为1,0,0,0,…，hidden layer中的neural都是线性的，在上一个时间点的hidden layer的输出也会作为当前时间点的部分input，设其权重为w，那么如果输入了1000次，那么整个network的output $y^{1000}=w^{999}$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610115048161.png" alt="image-20200610115048161" style="zoom:50%;"></p>
<p>如果$w=1$，那么对应的输出$y^{1000}=1$；如果$w=1.01$，那么对应的输出$y^{1000}\approx20000$；w虽然只有小小的变化，但由于蝴蝶效应，对output的影响却很大，因此有很large的gradient $\frac{\partial L}{\partial w}$，这时就需要把learning rate设置小一点</p>
<p>如果$w=0.99$，那么对应的输出$y^{1000}\approx0$；如果$w=0.01$，那么对应的输出$y^{1000}\approx0$；w虽然有很大的变化，但对output的影响却很小，因此有很small的gradient $\frac{\partial L}{\partial w}$，这时就需要把learning rate设置大一点</p>
<p>在很小的区域内，gradient就会有很大的变化，learning rate的变化也很大</p>
<p>因此，RNN不好训练的原因并不是因为activation function的影响，而是由于weight在high frequency地被使用，weight在不同的位置、不同的时间点是会被反复地使用的</p>
<h5 id="Helpful-Techniques"><a href="#Helpful-Techniques" class="headerlink" title="Helpful Techniques"></a>Helpful Techniques</h5><p><strong>LSTM</strong>可以让error surface不那么崎岖，可以把一些很平坦的地方拿掉，使error surface不再有那么平坦的地方；<u>可以解决gradient vanishing问题，但并不能解决gradient exploding的问题，有的地方还是会特别崎岖，因此在使用LSTM时，需要设置很小的learning rate。</u></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610120736515.png" alt="image-20200610120736515" style="zoom:50%;"></p>
<p><span style="color: red">LSTM可以解决gradient vanishing的原因：</span></p>
<ul>
<li>LSTM当前的input和memory里的值是“相加”的，每次都会有新的东西加进来；但RNN却是相乘的，只要weight有一点小小的变化，其output都会产生巨大的变化，如果相加就不会有这么大的变化；</li>
<li>如果forget gate是打开的状态，一旦weight可以影响memory里面的值，这个影响就会永远存在，每次都会有新的东西加进来，不会被直接覆盖掉，除非forget gate打算把memory cell的值忘掉；但RNN是直接覆盖，前面的影响就消失了。</li>
</ul>
<p>Q：既然forget gate也有可能选择覆盖掉memory cell里面的值，是不是可以认为之前的影响也会突然消失？</p>
<p>A：LSTM的第一个版本是为了解决gradient vanishing问题，forget gate是后面才加上去的。因此在LSTM的训练中，要给forget gate非常大的bias，要保证forget gate大部分情况下都是forget的；即大部分情况都不会覆盖memory cell里面的值，只有极少数的情况会被覆盖掉。</p>
<p><strong>Gated Recurrent Unit（GRU）</strong>：比LSTM更简单，gate的数量只有俩，即forget gate和input gate，这两者有一个联动：</p>
<ul>
<li>当input gate打开时，forget gate关闭，forget掉menory里的值；</li>
<li>当forget gate打开时，记住memory里的值，input gate关闭，不再进行输入</li>
</ul>
<p>还有一些其他的技术可以解决这个问题，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610123018202.png" alt="image-20200610123018202" style="zoom:50%;"></p>
<h4 id="More-Applications-……"><a href="#More-Applications-……" class="headerlink" title="More Applications ……"></a>More Applications ……</h4><h5 id="Many-to-one"><a href="#Many-to-one" class="headerlink" title="Many to one"></a>Many to one</h5><p>input是一些vector的sequence，output可以只是一个vector</p>
<p>比如sentiment analysis，输入电影评价的sequence，输出为电影评论的类别</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610123509761.png" alt="image-20200610123509761" style="zoom: 50%;"></p>
<h5 id="Many-to-Many-Output-is-shorter"><a href="#Many-to-Many-Output-is-shorter" class="headerlink" title="Many to Many (Output is shorter)"></a>Many to Many (Output is shorter)</h5><p>input和output也可以都是一个sequence，比如语音识别speech recognition</p>
<p>对于原来的RNN，每个vector对应到某一个character，input的vector通常是很短的，比如0.01s，因此通常是很多个vector对应到同一个character</p>
<p>有一种解决方式是trimming，去掉重复的汉字，但这时就识别不出“好棒棒”，只能识别出“好棒”</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610123754697.png" alt="image-20200610123754697" style="zoom:50%;"></p>
<p>有学者提出了CTC，添加了表示null的符号进来</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610154820892.png" alt="image-20200610154820892" style="zoom:50%;"></p>
<p>当输入“机”字，根据上一个时间点的memory和“机”，RNN就可以学习到输出为“器”，…..</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610155051371.png" alt="image-20200610155051371" style="zoom:50%;"></p>
<p>如果不停止的话，rnn可能会一直生成下去，因此要加入一个“断“字，停止生成</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610155210452.png" alt="image-20200610155210452" style="zoom:50%;"></p>
<h5 id="Sequence-to-sequence-Auto-encoder-Text"><a href="#Sequence-to-sequence-Auto-encoder-Text" class="headerlink" title="Sequence-to-sequence Auto-encoder - Text"></a>Sequence-to-sequence Auto-encoder - Text</h5><p>虽然这两句话有相同的单词，但这两句话中的单词却有不同的顺序，因此表示的含义也不一样，一个是positive，一个是negative</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610155820922.png" alt="image-20200610155820922" style="zoom:50%;"></p>
<p>在考虑单词原有顺序的情况下，我们可以通过sequence2sequence auto-encoder来将一个document转化为vector，那么具体怎么做呢？</p>
<p>现在有一个input sequence，“mary was hungry. She didn’t find any food”，先通过encoder找到一个vector，再把这个vector输入decoder，找出对应的sequence</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610160241278.png" alt="image-20200610160241278" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610162641873.png" alt="image-20200610162641873" style="zoom:50%;"></p>
<h5 id="Sequence-to-sequence-Auto-encoder-Speech"><a href="#Sequence-to-sequence-Auto-encoder-Speech" class="headerlink" title="Sequence-to-sequence Auto-encoder - Speech"></a>Sequence-to-sequence Auto-encoder - Speech</h5><p>比如有一段演讲语音，现在我想搜索audio中提到了白宫的部分，这时我们就需要做如下工作</p>
<p>把输入的audio segment转化成vector，再把spoken query的audio segment转化成vector，再计算这两者的相似程度，就可以得到搜寻的结果</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610163540450.png" alt="image-20200610163540450" style="zoom:50%;"></p>
<p><strong>audio-&gt;vector</strong></p>
<p>把输入的audio segment分成4部分$x_1,x_2,x_3,x_4$，输入RNN，这里的RNN充当了一个encoder的角色，最后一个时间点在memory里存的值就是整段audio的information</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610163053020.png" alt="image-20200610163053020" style="zoom:50%;"></p>
<p>但是这个RNN的encoder是没办法train的，所以这里加入了RNN的decoder。encoder的memory里存的值作为decoder的第一个input，产生一个sequence，这个y1和x1越接近越好，再根据y1产生y2，y3，y4。这个训练的target是$y_1,y_2,y_3,y_4$与$x_1,x_2,x_3,x_4$越接近越好</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610163303283.png" alt="image-20200610163303283" style="zoom:50%;"></p>
<p>这个RNN的encoder和decoder是一起 train，其中任何一个都是没办法单独训练的，一起train就有一个target</p>
<p>将speech转化为vector之后，再进行可视化</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610164950240.png" alt="image-20200610164950240" style="zoom:50%;"></p>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title>Self-Attention与Transformer模型</title>
    <url>/2020/07/13/Transformer/</url>
    <content><![CDATA[<p>本文是对论文《Attention is all you need》的理解。首先介绍了标准的attention机制，再介绍了transformer的encoder和decoder，以及self-attention。</p>
<h3 id="标准attention机制"><a href="#标准attention机制" class="headerlink" title="标准attention机制"></a>标准attention机制</h3><p>下图中就表示了标准的attention机制。也可见<a href="https://scarleatt.github.io/2020/06/28/seq2seq/" target="_blank" rel="noopener">sequence to sequence</a>中有关attention机制的相关叙述。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200713163711310.png" alt="image-20200713163711310"></p>
<p>根据下面这个公式，先计算Query和每个Key之间的相似度，再乘对应的value，就可以来计算出attention的值。</p>
<script type="math/tex; mode=display">
\rm Attention(Query,Source)=\sum_{i=1}^{L_x}Similarity(Query,Key_i)*Value_i</script><p>其中$L_x$表示source的长度，Query和每个Key之间的相似度也有不同的计算方法，</p>
<p>点积：</p>
<script type="math/tex; mode=display">
\rm Similarity(Query,Key_i)=Query*Key_i</script><p>Cosine相似性:</p>
<script type="math/tex; mode=display">
\rm Similarity(Query,Key_i)=\frac{Query*Key_i}{||Query||\cdot ||Key_i||}</script><p>MLP网络：</p>
<script type="math/tex; mode=display">
\rm Similarity(Query,Key_i)=MLP(Query,Key_i)</script><p>下面我们将从三个阶段来详细介绍attention机制。</p>
<p><strong>阶段1</strong>：根据Query和Key计算相似度Similarity，可以用上文介绍的三种方法，即求两者的点乘、计算两者的cosin相似度、根据神经网络来计算；我们把这一阶段算出来的$Key_i$与Query之间的相似度记为$\alpha_i$；</p>
<p><strong>阶段2</strong>：把$\alpha_i$输入softmax函数，进行归一化，得到$\hat \alpha_i$；</p>
<p><strong>阶段3</strong>：根据$\hat \alpha_i$和Value计算attention的值，即</p>
<script type="math/tex; mode=display">
\rm Attention(Query,Key,Value)=\sum_{i=1}^{L_x}\alpha_i\cdot Value_i</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200713164825271.png" alt="image-20200713164825271"></p>
<h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>transform模型由多个encoder和decoder构成，如下图所示</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200712185243525.png" alt="image-20200712185243525" style="zoom:80%;"></p>
<h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>Encoder由N=6个相同的layer组成，layer就是上图中左边的方框部分，左边的英文字母“Nx”表示layer的数量，这里N=6。每个layer都有两个sub-layer，即multi-head attention和position-wise feed-forward network。每个sub-layer还加入了residual connection和layer normalization，因此每个sub-layer的输出为</p>
<script type="math/tex; mode=display">
\rm sub\_layer_{output}=LayerNorm(x+Sublayer(x))</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200712214746642.png" alt="image-20200712214746642" style="zoom:67%;"></p>
<h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>Decoder也由6个相同的layer组成，layer就是上图的右边的方框部分，此时N=6。layer由3个sub-layer组成，除了encoder提到的两个sub-layer，还有Masked Multi-Head Attention。</p>
<p>新加入的masked multi-head attention，在进行softmax之前对未来的position进行mask（设为-inf），可以保证对位置i的预测只依赖位置小于i的已知输出，不会接触到未来位置的信息。</p>
<ul>
<li>输出：对应i位置的输出词的概率分布，如果词库中有一万个单词，输出的vector就是一万维；</li>
<li>输入：encoder的输出 &amp; 对应i-1位置decoder的输出。中间的attention（masked）并不是self-attention，K，V来自encoder，Q来自上一位置decoder的输出；</li>
<li>解码：编码可以并行计算，一次性全部encoding出来。但解码不是一次把所有的序列解出来，由于要用到上一个位置encoder的出书作为这次的query，因此解码是像rnn一样一个一个解出来的。</li>
</ul>
<p>动图演示，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/transformer_decoding_2.gif" alt="transformer_decoding_2"></p>
<h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h4><p>attention有三个输入$Q,K,V$。<span style="color: red">对于self-attention来讲，Q，K，V三者都来自同一个输入向量X，只是将X进行了不同的线性变换（乘上不同的权重矩阵）。</span></p>
<p>attention可以通过以下式子计算，先计算Q和K的点乘，再进行scale（除以$\sqrt{d_k}$），输入softmax函数，再与V进行点乘，即可得到输出。</p>
<script type="math/tex; mode=display">
{\rm Attention}(Q,K,V)={\rm softmax}(\frac{QK^T}{\sqrt{d_k}})V</script><p>我们把输入的embedding全都放到一个矩阵$X$内，矩阵的每一行表示输入序列的一个word，$W^Q,W^K,W^V$表示Q，K，V对应的权重矩阵，可以通过网络进行学习。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200712215525200.png" alt="image-20200712215525200" style="zoom: 50%;"></p>
<p>得到Q，K，V之后，就可以进行self-attention的计算，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200712220405217.png" alt="image-20200712220405217" style="zoom: 50%;"></p>
<p>Q1：为什么选择dot-product attention，而不是additive attention？</p>
<p>A：dot-product attention可以使用高度优化过后的矩阵运算代码。</p>
<p>Q2：为什么要除以$\sqrt{d_k}$这一项？</p>
<p>A：（1）在$d_k$很小的时候，如果不进行scaling，dot-product attention和additive attention的performance相差不大；如果$d_k$值很大，那么点乘的值就会很大，不做scaling的话，效果就没有additive attention好。（2）如果$d_k$的值很大，点乘的结果也很大，输入softmax函数之后的梯度就会变得很小，不利于back propagation。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200712192858975-20200713101139647.png" alt="image-20200712192858975"></p>
<h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><p>multi-head attention则是将Q，K，V分别进行h次不同的<strong>线性变换</strong>，再把这些不同的attention结果连接起来，即</p>
<script type="math/tex; mode=display">
\rm MultiHead(Q,K,V)=Concat(head_1,..,head_h)W^O\\
\quad head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)</script><p>multi-head attention的流程图如下，1) input sequence为<span style="color: green">Thinking Machines</span>；2) embedding每个word，转化成vector；3) 分别计算8个attention head的Q，K，V；4) ；使用上一步的结果Q，K，V来计算attention的值；5) 将$Z^0,…,Z^7$的结果连接去来，形成一个大矩阵，再用这个大矩阵来乘矩阵$W^O$，得到最后的结果Z。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200713120312773.png" alt="image-20200713120312773"></p>
<h4 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h4><p>由于这个模型并没有包含RNN和CNN，并没有考虑输入序列的order，我们必须在embedding之后把序列的order也考虑进去，把位置信息positional encoding加上embedding，就得到了带位置信息的embedding。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200713100210115.png" alt="image-20200713100210115"></p>
<p>作者使用了不同频率的正弦和余弦函数来完成这项工作，</p>
<script type="math/tex; mode=display">
PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\\
PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})</script><p>其中pos就表示position，i表示dimension。positional encoding的每个维度都对应着一个正弦曲线。</p>
<p>NLP任务还需要考虑到单词的相对位置，根据下列正余弦的变化公式，</p>
<script type="math/tex; mode=display">
\rm sin(\alpha+\beta)=sin\alpha cos\beta+cos\alpha sin\beta\\
cos(\alpha+\beta)=cos\alpha cos\beta-sin\alpha sin\beta</script><p>可得出：现在对于固定的offset k，$PE_{pos+k}$可以表示成$PE_{pos}$的线性变换，</p>
<h4 id="Residual-Connection"><a href="#Residual-Connection" class="headerlink" title="Residual Connection"></a>Residual Connection</h4><p>在每个encoder的sub-layer中，都对应一个residual connection和layer normalization。</p>
<p>下图中Add表示residual connection，是为了解决多层神经网络训练困难的问题，通过将前一层的信息无差地传递到下一层，可以有效地仅关注差异部分。</p>
<p>Norm则表示Layer Normalization，通过对层的激活值的归一化，可以加速模型的训练过程，使其更快地收敛。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200713172314745.png" alt="image-20200713172314745" style="zoom:67%;"></p>
<h4 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h4><h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>输入x有m个batch，即$B=\{x_1,x_2,..,x_m\}$，每个$x_i$都有K个element，对每个batch进行layer Normalization，得到$B’=\{y_1,y_2,..,y_m\}$。</p>
<p>对于其中的某个batch $x_i=\{x_{i,1},x_{i,2},…,x_{i,K}\}$，我们先求出其对应的均值和方差，即</p>
<script type="math/tex; mode=display">
\mu_i=\frac{1}{K}\sum_{k=1}^Kx_{i,k}\\
\sigma_i^2=\frac{1}{K}\sum_{k=1}^K(x_{i,k}-\mu)^2</script><p>再进行归一化，参数$\epsilon$是为了解决分母突然为0的情况。</p>
<script type="math/tex; mode=display">
\hat x_{i,k}=\frac{x_{i,k}-\mu_i}{\sqrt{\sigma_i^2+\epsilon}}</script><p>经过缩放和转移的步骤，可得到最后的结果</p>
<script type="math/tex; mode=display">
y_i=\gamma \hat x_i+\beta=LN_{\gamma,\beta}(x_i)</script><p>对于输入的tensor的形状为$[m,H,W,C]$，m表示batch的数量，H和W表示输入数据的高度和宽度，C表示channel的数量。Layer Normalization是对每个channel进行归一化。</p>
<h5 id="BN-vs-LN"><a href="#BN-vs-LN" class="headerlink" title="BN vs LN"></a>BN vs LN</h5><p>Batch Norm和Layer Norm的主要区别在于归一化的维度不一样，</p>
<ul>
<li>batchNorm是在batch上，对mHW做归一化；</li>
<li>layerNorm是在channel方向上，对CHW做归一化。</li>
</ul>
<p>BN的具体做法就是对每一小批数据，在batch这个方向上做归一化。如下图所示：</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200713220311349.png" alt="image-20200713220311349" style="zoom:100%;"></p>
<p>而对于Layer Norm，则是在channel上</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200713220419384.png" alt="image-20200713220419384"></p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p><strong>标准的attention和self-attention的主要区别</strong>在于：self-attention的Q，K，V都是由输入序列X经过线性变换所提取到的。</p>
<p><strong>Transformer模型：</strong></p>
<p>优点：transformer模型不仅可以应用到NLP的机器翻译领域，还可以应用到其他非NLP领域，是非常有科研潜力的一个方向。</p>
<p>缺点：（1）没有使用RNN和CNN，使模型并不能捕捉一些局部特征，使用Transform+RNN+CNN效果可能会更好；（2）Transform丢失的位置信息在NLP其实是非常重要的，加入position encoding其实只是一个权宜之计，并不能从根本上改变transform的结构缺陷。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] <a href="https://zhuanlan.zhihu.com/p/48508221" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/48508221</a></p>
<p>[2] <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">https://jalammar.github.io/illustrated-transformer/</a></p>
<p>[3] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need [C]//Advances in Neural Information Processing Systems. 2017: 5998-6008.</p>
<p>[4] <a href="https://mp.weixin.qq.com/s?__biz=MzI5NDMzMjY1MA==&amp;mid=2247485591&amp;idx=1&amp;sn=ab8b2204f81e0a8de6d1882df9a9c2c0&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzI5NDMzMjY1MA==&amp;mid=2247485591&amp;idx=1&amp;sn=ab8b2204f81e0a8de6d1882df9a9c2c0&amp;scene=21#wechat_redirect</a></p>
<p>[5] <a href="https://leimao.github.io/blog/Layer-Normalization/" target="_blank" rel="noopener">Layer Normalization</a></p>
]]></content>
      <categories>
        <category>Knowledge Graph</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Transformer</tag>
        <tag>Self-Attention</tag>
        <tag>Sequence to Sequence</tag>
      </tags>
  </entry>
  <entry>
    <title>Word Embedding</title>
    <url>/2020/06/11/Word-Embedding/</url>
    <content><![CDATA[<h4 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h4><p>如果使用1-of-N encoding，每个单词用一个vector表示，那么5个单词就需要5个vector；如果我们把同一个类别的单词都放到那个类里，即属于class1的单词有dog、cat、bird，属于动物类的单词，同理可以得出class2，class3；</p>
<p>但只做classify是不够的，这些class之间也有一些其他的联系；比如class1属于动物，class3属于植物，他们都是生物，只做classify并不能体现这种联系；</p>
<p>现在我们把每个word都project到一个两个dimension上， 水平的dimension可以是表示生物（class1，class3）和其他类别class2之间的差距，竖直的dimension可以是会动（class2，class3）的和不会动class1之间的差距；</p>
<p>如果现在有10w个单词，1-of-N encoding就需要10w个vector，但word embedding可能只需要50维左右，就可以表示这些所有的word。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611094055907.png" alt="image-20200611094055907" style="zoom:50%;"></p>
<p>word2vec是一个无监督学习问题，如果network的input为“Apple”，要输出其对应的vector </p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611095916810.png" alt="image-20200611095916810" style="zoom:50%;"></p>
<p>下面我们将叙述生成词向量的两种主要手段。</p>
<h4 id="Count-based"><a href="#Count-based" class="headerlink" title="Count based"></a>Count based</h4><p>基于计数的方法，记录文本中词的出现次数。如果两个单词$w_i,w_j$常常一起出现，那么我们就认为其对应的vector $V(w_i),V(w_j)$之间就是非常接近的。</p>
<p>用$N_{i,j}$表示$w_i,w_j$在同一个document中出现的次数，那么我们希望找到对应的$V(w_i),V(w_j)$，其做inner product的值和这个次数越接近越好。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611102225747.png" alt="image-20200611102225747" style="zoom:50%;"></p>
<h4 id="Prediction-based"><a href="#Prediction-based" class="headerlink" title="Prediction-based"></a>Prediction-based</h4><p>基于预测的方法，即可以通过上下文预测中心词，也可以通过中心词预测上下文。中心词即我们要预测的词。在下文中，$w_i$是我们要预测的值，$…,w_{i-2},w_{i-1}$就是其对应的上下文；</p>
<p>现在我们把$w_{i-1}$的one-hot encoding作为网络的输入，网络的输出为每个词作为下一个词$w_i$输出的概率，如果词袋中有10w个词，那么输出的维度就对应为10w维。</p>
<p>把网络中第一个hidden layer的input拿出来，即$z=(z_1,z_2,…)^T$；如果输入为不同1-of-N encoding，那么对应的z也是不一样的。我们就可以用z来代表一个word，即z就是我们要寻找的word vector $V(w)$。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611102755128.png" alt="image-20200611102755128" style="zoom: 40%;"></p>
<p>如果现在有两个training text，“蔡英文 宣誓就职”、“马英九 宣誓就职”，如果输入的1-of-N encoding是“蔡英文”或“马英九”，那么我们希望在网络的输出概率中，“宣誓就职”的概率是最大的。同理我们也可以把网络的第一个hidden layer的输入作为z，就是我们要寻找的word vector $V(w)$。</p>
<p>这时我们就需要中间的hidden layer来做这样一件事，如果输入为不同的词汇（“蔡英文”，“马英九”），那么我们希望中间的hidden layer可以把不同的词汇project到相同的空间，这样网络的输出才可能都是“宣誓就职”对应的概率最大。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705122246038.png" alt="image-20200705122246038" style="zoom:50%;"></p>
<h5 id="Sharing-weight"><a href="#Sharing-weight" class="headerlink" title="Sharing weight"></a><strong>Sharing weight</strong></h5><p>只考虑前面的一个词汇，来预测下一个词汇会很难，我们可以考虑前面的几个词汇。现在我们来叙述考虑前两个词汇的结果。</p>
<p>现在我们并不能像之间的neural network那样，所有的输入都连成一个vector作为输入。但实际上，我们希望不同的one-hot vector的同一维度之间是tie在一起的。即$w_{i-1},w_{i-2}$的第一维对应$z_1$，其对应的weight是一样的；同理$w_{i-1},w_{i-2}$的第二维对应$z_1$，其对应的weight是一样的，……</p>
<p>Q：为什么不同的one-hot vector的同一维度之间是tie在一起的呢？</p>
<p>A：我们现在把同一个word放到$w_{i-1},w_{i-2}$的位置， 如果每一维对应的weight都不一样，那么实际上就输入了两个vector。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611103812991.png" alt="image-20200611103812991" style="zoom:50%;"></p>
<p>如果我们设置$x_{i-1},x_{i-2}$的长度都是$|V|$，$z$的长度是$|Z|$，那么</p>
<script type="math/tex; mode=display">
z=W_1x_{i-2}+W_2x_{i-1}</script><p>其中$W_1=W_2=W$，那么$z=W(x_{i-2}+x_{i-1})$，也就得到了word vector $V(w)$，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611104833054.png" alt="image-20200611104833054" style="zoom:50%;"></p>
<p>那么在实际的网络训练中，我们如何保证$W_1=W_2$呢？</p>
<p>首先需要将$w_1,w_2$初始化为相同的值，那么</p>
<script type="math/tex; mode=display">
w_i\leftarrow w_i-\eta \frac{\partial C}{\partial w_i}\\
w_j\leftarrow w_j-\eta \frac{\partial C}{\partial w_j}</script><p>由于gradient的值不同，$w_1,w_2$在更新一次参数之后就不相等了，必须保证每次更新之后的值还是一样的，因此</p>
<script type="math/tex; mode=display">
w_i\leftarrow w_i-\eta \frac{\partial C}{\partial w_i}-\eta \frac{\partial C}{\partial w_j}\\
w_j\leftarrow w_j-\eta \frac{\partial C}{\partial w_j}-\eta \frac{\partial C}{\partial w_i}</script><p>这样每次更新的值都一样的，也就保证了$w_1=w_2$</p>
<h5 id="Training"><a href="#Training" class="headerlink" title="Training"></a><strong>Training</strong></h5><p>对于输入“潮水、退了”，我们希望network的输出和“就”越接近越好，即最小化cross entropy</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611103701528.png" alt="image-20200611103701528" style="zoom:50%;"></p>
<h5 id="Various-Architectures"><a href="#Various-Architectures" class="headerlink" title="Various Architectures"></a>Various Architectures</h5><p>CBOW：根据上下文的词汇$w_{i-1},w_{i+1}$来预测中心词$w_i$;</p>
<p>Skip-gram：根据中心词$w_i$来预测上下文$w_{i-1},w_{i+1}$。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611110004840.png" alt="image-20200611110004840" style="zoom:50%;"></p>
<h5 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h5><p>vec(Rome) - vec(Italy) $\approx$ vec(Berlin) - vec(Germany)，Italy和Rome之间有is-capital-of的关系，这种关系也恰好在Madrid和Spain之间出现。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611110430525.png" alt="image-20200611110430525" style="zoom:50%;"></p>
<p>如果现在有人问机器一个问题，Rome和Italy之间的关系就像是Berlin和什么的关系？我们就可以通过计算vec(Berlin) $\approx$ vec(Rome) - vec(Italy)  + vec(Germany)得出结果。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705125922175.png" alt="image-20200705125922175" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>word embedding</tag>
      </tags>
  </entry>
  <entry>
    <title>Wasserstein GAN (WGAN)</title>
    <url>/2020/07/14/WGAN/</url>
    <content><![CDATA[<p>本文主要介绍了WGAN的核心思想。由于JS divergence自身的限制，我们先改进了classifier的输出分数的分布，从sigmoid改成了linear，即LSGAN；还有另外一种改进方式，使用Wasserstein distance来衡量两个分布之间的差异，即WGAN。</p>
<h4 id="JS-divergence-is-not-suitable"><a href="#JS-divergence-is-not-suitable" class="headerlink" title="JS divergence is not suitable"></a>JS divergence is not suitable</h4><p>在之前的文章中，我们使用了JS divergence来衡量$P_G,P_{data}$之间的差距。在大多数的情况中，我们学习出来的分布$P_G$和真实数据的分布$P_{data}$之间其实并没有任何重叠，原因如下：</p>
<ol>
<li>the nature of data，$P_G,P_{data}$都是高维空间的图像数据在低维空间的manifold，这两个分布在低维的manifold上是有一部分重叠的，但在高维空间中，这两个分布不一定有重叠，因此在manifold上的重叠是可以被忽略的。</li>
<li>由于我们是从$P_G,P_{data}$中sample出部分数据来进行divergence，如果sample的数据不够多，虽然这两个分布是有重叠的，但我们sample出来的数据并不一定有重叠。</li>
</ol>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714095937032.png" alt="image-20200714095937032" style="zoom:67%;"></p>
<p>Q：什么是manifold？</p>
<p>A：manifold其实是一个空间。可以看作是一个d维的空间，而这个空间是被m维的空间扭曲之后的结果（m&gt;d）。在下图中，就表示了一个二维的manifold被折起来，变成了三维空间的数据，其实我们用二维空间就可以表示这个三维空间的数据。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714103558383.png" alt="image-20200714103558383" style="zoom: 67%;"></p>
<p><strong>What is the problem of JS divergence?</strong></p>
<p>当两个分布完全没有任何重叠的话，JS divergence的值就是$log2$，在下图中只有第三种情况两种分布$P_{G_{100},P_{data}}$有重叠，JS divergence的值为0，其他情况下都是$log2$。</p>
<p>那么这样会造成什么弊端呢？</p>
<p>对于下图中的前两种情况，$P_{G_0},P_{data}$之间的差距和$P_{G_1},P_{data}$是完全不一样的，很明显$P_{G_0},P_{data}$之间的差距要大很多，但JS divergence计算这两者之间的差距输出都是$log2$，是<span style="color: red">equally bad</span>。因此$P_{G_0}$不会update到$P_{G_1}$，更不会update到$P_{G_{100}}$，训练到此就被卡住了。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714104629506.png" alt="image-20200714104629506" style="zoom:67%;"></p>
<h4 id="Least-Square-GAN-LSGAN"><a href="#Least-Square-GAN-LSGAN" class="headerlink" title="Least Square GAN (LSGAN)"></a>Least Square GAN (LSGAN)</h4><p>在下图中，我们用蓝色和红色圆点表示数据不同的类别，横向坐标轴表示原始的数据分布。现在我们来训练一个binary classifier，这个classifier会给蓝色的点0分，给绿色的点1分，输出接近与sigmoid函数，输出在接近0的位置特别平。本来我们的目标是训练一个classifier，generator会根据discriminator给出的gradient去移动，希望generator生成的点会往右边不断移动，来接近真实数据的分布。</p>
<p>但实际上，如果我们训练一个classifier使其输出接近sigmoid函数，generator生成的数据所的分数几乎都是0，gradient非常小几乎为0，没办法继续训练下去。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714110849955.png" alt="image-20200714110849955" style="zoom:67%;"></p>
<p>我们可以让这个classifier训练出来没那么好，那么到底什么叫不要train得太好呢？</p>
<p>后面就有学者提出了<strong>Least Square GAN</strong>，把sigmoid换成linear，这样就不会出现在某些地方特别平坦的问题，也就变成了一个regression问题。</p>
<h4 id="Wasserstein-GAN-WGAN"><a href="#Wasserstein-GAN-WGAN" class="headerlink" title="Wasserstein GAN (WGAN)"></a>Wasserstein GAN (WGAN)</h4><h5 id="Earth-Mover’s-Distance"><a href="#Earth-Mover’s-Distance" class="headerlink" title="Earth Mover’s Distance"></a>Earth Mover’s Distance</h5><p>现在有一台挖土机，我们把分布P看作是一堆土，分布Q就使挖土机要把土挖过去的地方，挖土机从P到Q移动的距离就叫做<strong>Earth Mover’s Distance</strong>，也称作Wasserstein distance。</p>
<p>如果现在把分布P和Q分别看作是在一维空间上的两个点，这两个点之间的距离就可以看作是Wasserstein distance，即$W(P,Q)=d$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714112901592.png" alt="image-20200714112901592" style="zoom:67%;"></p>
<p>如果我们要衡量下图中两个分布的Wasserstein distance，挖土机有很多种铲土的方案，有可以直接到达的，也有舍近求远的，那么Wasserstein distance到底是smaller distance还是larger distance呢？</p>
<p>现在我们可以穷举所有的moving plan，把推土机走的平均距离都分别算出来，看哪一个距离最小，最小的那个距离就是Wasserstein distance。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714113653211.png" alt="image-20200714113653211" style="zoom:67%;"></p>
<p>下图中展示了最小的distance，同样颜色的方块代表同样的土，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714114230924.png" alt="image-20200714114230924" style="zoom:67%;"></p>
<p>现在来讲一个更加真实的例子。要把P的土挪到Q所在的地方，首先要确定一个moving plan，这个moving plan可以看作是一个矩阵。$x_p,x_q$分别表示P和Q中土的对应位置，这两者的交界处就表示P要挪多少土给Q，颜色越亮挪的土越多。矩阵一行内所有的土和起来，就表示分布P在对应位置bar的高度。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714114523500.png" alt="image-20200714114523500" style="zoom:67%;"></p>
<p>那么现在有了这个moving plan的矩阵，我们就可以来计算要挪土的距离了。$\gamma(x_p,x_q)$表示要从$x_p$挪多少土到$x_q$去，$||x_p-x_q||$则表示这两者之间的距离。</p>
<script type="math/tex; mode=display">
B(\gamma)=\sum_{x_p,x_q}\gamma(x_p,x_q)||x_p-x_q||</script><p>对于我们要找的Wasserstein distance，我们则需要穷举所有可能的$\gamma$矩阵，找到对应的moving plan，使distance算出来最小，即</p>
<script type="math/tex; mode=display">
W(P,Q)=\mathop{\rm min}_{\gamma\in\pi}B(\gamma)</script><h5 id="Why-Earth-Mover’s-Distance"><a href="#Why-Earth-Mover’s-Distance" class="headerlink" title="Why Earth Mover’s Distance?"></a><strong>Why Earth Mover’s Distance?</strong></h5><p>可以看出计算最优的moving plan，算出Wasserstein distance是非常麻烦的，那么我们为什么还要使用这个Wasserstein distance呢？</p>
<p>回顾下我们之前使用的JS divergence，$P_{G_0},P_{data}$与$P_{G_{50}},P_{data}$这两者之间的divergence都是log2，$P_{G_{0}},P_{G_{50}}$的表现是一样的，没办法进行改进，也没办法改进到$P_{G_{100}}$。</p>
<p>但Wasserstein distance并不会出现这种问题，我们可以用Wasserstein distance来衡量两个不同分布之间的差异，在下图中，$d_0$明显比$d_{50}$大，那么模型就可以不断地进行优化，来找到最后的$P_{G_{100}}$，使得对应的Wasserstein distance为0，即$W(P_{G_{100}},P_{data})=0$。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714115941870.png" alt="image-20200714115941870" style="zoom:67%;"></p>
<h5 id="Evaluate-wasserstein-distance"><a href="#Evaluate-wasserstein-distance" class="headerlink" title="Evaluate wasserstein distance"></a>Evaluate wasserstein distance</h5><p>那么我们现在怎么来修改discriminator，使其可以评估Wasserstein distance呢？</p>
<p>推导过程过于复杂，在此直接得出结论</p>
<script type="math/tex; mode=display">
V(G,D)=\mathop{\rm max}_{D\in1-Lipschitz}\{E_{x\sim P_{data}}[D(x)]-E_{x\sim P_{G}}[D(x)]\}</script><p>如果x是从$P_{data}$中sample出来的，应该越大越好，如果是从$P_G$中sample出来的，应该越小越好。除此之外，还需要有一个额外的限制，discriminator是一个1-Lipschitz函数，要足够smooth才行。</p>
<p>如果现在没有这个constrain，紫色圆点是从$P_{G}$中sample出来的，绿色圆点是从$P_{data}$中sample出来的。现在只考虑让$E_{x\sim P_{data}}[D(x)]$分数越大越好，让$E_{x\sim P_{G}}[D(x)]$越小越好，这两个分布很有可能没有重叠的地方，那么$E_{x\sim P_{data}}[D(x)]$的分数会变得无限大，$E_{x\sim P_{G}}[D(x)]$会变得无限小，训练永远不会停止，模型也永远不会收敛。</p>
<p>因此必须要有额外的限制，训练出来的discriminator得出的分数必须足够平滑，这样就不会出现$E_{x\sim P_{data}}[D(x)]$的分数会变得无限大、$E_{x\sim P_{G}}[D(x)]$会变得无限小这种情况，模型也最终会收敛。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714122923490.png" alt="image-20200714122923490" style="zoom:67%;"></p>
<p>那么<strong>Lipschitz Function</strong>到底是什么呢？</p>
<p>我们用$||x_1-x_2||$来表示input change，$||f(x_1)-f(x_2)||$来表示output change，input change乘上倍数K，必须要大于等于output change，即output不能变化太大，要在满足这个限制的条件下进行变化。output的变化永远比input的变化要小，即</p>
<script type="math/tex; mode=display">
||f(x_1)-f(x_2)||\leq K||x_1-x_2||</script><p>如果K=1，那么就是1-Lipschitz函数。</p>
<p>在下图中，有两个函数的曲线，蓝色曲线的变化太多剧烈，output的变化超过了input的变化，并不是1-Lipschitz；但绿色曲线的变化就相对平缓很多，是一个1-Lipschitz函数。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714150013383.png" alt="image-20200714150013383" style="zoom:67%;"></p>
<p>那么我们要给discriminator加一个constrain呢？</p>
<p>如果是没有constrain的discriminator，直接使用gradient ascent就好；但现在是加入了constrain的discriminator，有学者就提出了<strong>weight clipping</strong>这种方法；</p>
<p>weight clipping的具体做法如下，模型还是照常训练，照常更新参数，但现在我们给weight多了一个额外的限制，如果$w&gt;c,w=c; w&lt;-c,w=-c$；不会出现weight突然很大的情况，因此output的变化也不会很剧烈。</p>
<p>Q：那么weight clipping处理过后就可以变成1-Lipschitz函数吗？</p>
<p>A：并不能，这只是一个非常原始的解法。</p>
<h4 id="Improved-WGAN-WGAN-GP"><a href="#Improved-WGAN-WGAN-GP" class="headerlink" title="Improved WGAN (WGAN-GP)"></a><strong>Improved WGAN (WGAN-GP)</strong></h4><p>如果不能直接找出直接限制discriminator的解法，我们可以找出等价的解法。我们要求discriminator是属于1-Lipschitz函数的，这个做法就等价于，将discriminator对x求梯度之后的norm，这个norm的值必须小于等于1，即等价于</p>
<script type="math/tex; mode=display">
||\Delta_xD(x)||\leq1 \rm\ for\ all\ x</script><p>那么我们怎么把这个限制条件加进去呢？</p>
<p>我们可以在原来的式子后面加入额外的一项，即</p>
<script type="math/tex; mode=display">
V(G,D)\approx\mathop{\rm max}_{D}\{E_{x\sim P_{data}}[D(x)]-E_{x\sim P_{G}}[D(x)]
\\\quad\quad\quad-\lambda \int_x[max(0,||\Delta_xD(x)||-1)]dx\}</script><p>相当于加入一个正则项，如果$||\Delta_xD(x)||$的值大于1，就相当于有penalty；也就是希望$||\Delta_xD(x)||$这一项的值小于等于1，如果满足这个条件，就不会有penalty。</p>
<p>但在实际的做法中，这种做法很不可取。加入正则项这个办法有一个额外的条件，就是必须要对<span style="color: red">所有的x</span>求积分，所有的x都需要满足这个条件，但实际上我们只sample出部分数据，并不能保证sample出全部的数据。</p>
<p>现在我们要进行进一步的改进，我们事先有一个确定好的分布$P_{penalty}$，这个分布中的x满足$||\Delta_xD(x)||\leq1$即可，</p>
<script type="math/tex; mode=display">
V(G,D)\approx\mathop{\rm max}_{D}\{E_{x\sim P_{data}}[D(x)]-E_{x\sim P_{G}}[D(x)]
\\\quad\quad\quad\quad\quad\quad\quad\quad-\lambda E_{x\sim P_{penalty}}[max(0,||\Delta_xD(x)||-1)]{\rm d}x\}</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714151055791.png" alt="image-20200714151055791" style="zoom:67%;"></p>
<p>那这个$P_{penalty}$到底是什么样的呢？</p>
<p>我们先在$P_{data},P_G$中分别取一个点，把这两个点的直线连接起来，从这两个直线中间的某个点随机sample一个点，我们就把这个点x当作是从分布$P_{penalty}$中sample出来的。</p>
<p>为什么这样取出来的点x就可以当作是从分布$P_{penalty}$中sample出来的呢？</p>
<p>在原始的论文中是这样解释的，如果要对所有的x都满足这个限制条件是不可能的，像图中这种画直线的方式得到了很好的performance；我们也可以从直觉上这样理解，我们的目标是让generator生成的数据分布$P_G$，沿着discrimiantor给出的梯度方向，慢慢挪到真实数据分布$P_{data}$那边去，而$P_{penalty}$正是这中间的过渡形态，只要使从$P_{penalty}$中sample出来的x满足条件$||\Delta_xD(x)||\leq1$即可。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714153133502.png" alt="image-20200714153133502" style="zoom:67%;"></p>
<p>$max(0,||\Delta_xD(x)||-1)]$对小于1的值也不算在内，但其实$||\Delta_xD(x)||$小于1对结果也是会有影响的，我们并不能直接忽略这个值，gradient大于1有惩罚，小于1也会有惩罚，因此，进一步改进得到了，</p>
<script type="math/tex; mode=display">
V(G,D)\approx\mathop{\rm max}_{D}\{E_{x\sim P_{data}}[D(x)]-E_{x\sim P_{G}}[D(x)]
\\\quad\quad\quad\quad\quad\quad\quad\quad-\lambda E_{x\sim P_{penalty}}(||\Delta_xD(x)||-1)^2\}</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714154701222.png" alt="image-20200714154701222" style="zoom:67%;"></p>
<h4 id="Spectrum-Norm"><a href="#Spectrum-Norm" class="headerlink" title="Spectrum Norm"></a>Spectrum Norm</h4><p>spectrum这个方法可以对discriminator进行限制，使所有的x对应的梯度都小于1.</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714155831975.png" alt="image-20200714155831975" style="zoom:67%;"></p>
<h4 id="Algorithm-of-original-GAN"><a href="#Algorithm-of-original-GAN" class="headerlink" title="Algorithm of original GAN"></a>Algorithm of original GAN</h4><p>那么怎么把原始的GAN改成WGAN呢？如下图所示</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714161350733.png" alt="image-20200714161350733"></p>
<h4 id="Energy-based-GAN-EBGAN"><a href="#Energy-based-GAN-EBGAN" class="headerlink" title="Energy-based GAN (EBGAN)"></a>Energy-based GAN (EBGAN)</h4><p>本来discriminator是一个binary classifier的架构，现在我们把它改成autoencoder，generator还是维持原来的架构。现在把generator生成的图像经过encoder和decoder，来计算reconstruction error，再乘上一个负号，就变成了现在discriminator的output。</p>
<p>这样做就有一个好处。这个autoencoder可以是pre-training，提前训练好的，我们可以提前用很多真实的例子来训练这个autoencoder，并不需要使用到generator，就可以训练好discriminator。因此EBGAN的generator就可以跟快生成很清晰的图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714161906413.png" alt="image-20200714161906413" style="zoom:67%;"></p>
<p>在训练EBGAN的时候，我们要让real data的discriminator得分越高越好，也就是reconstruction error要越小越好；让generated data得分越小越好，reconstruction error要越大越好。由于Hard to reconstruct，easy to destroy，那么对于generated data，如果generator直接生成noise，这时的error是最大的，得分也是最低的，但这并不是我们想要的结果。</p>
<p>我们加入了一个额外的限制，对于generator生成的数据所对应的reconstruction error，设置一个margin，让这个error小于某个threshold就好，不用更小，这个margin是需要自己去调整的参数。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200714163253851.png" alt="image-20200714163253851" style="zoom:67%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>GAN</tag>
        <tag>WGAN</tag>
        <tag>Earth Mover’s Distance</tag>
        <tag>Lipschitz Function</tag>
        <tag>EBGAN</tag>
      </tags>
  </entry>
  <entry>
    <title>PCA</title>
    <url>/2020/06/28/dimension-reduction/</url>
    <content><![CDATA[<h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><p>unsupervised learning可以分为两大类，dimension reduction和generation</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628104819015.png" alt="image-20200628104819015" style="zoom:50%;"></p>
<h4 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628105521936.png" alt="image-20200628105521936" style="zoom:50%;"></p>
<p>先根据input之间的相似度建立一颗树，再用一条线切一刀，如果使用红色的线切，那么可以分为2个大类</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628110013187.png" alt="image-20200628110013187" style="zoom:50%;"></p>
<h4 id="Distributed-Representation"><a href="#Distributed-Representation" class="headerlink" title="Distributed Representation"></a>Distributed Representation</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628110332232.png" alt="image-20200628110332232" style="zoom:50%;"></p>
<h4 id="Dimension-Reduction"><a href="#Dimension-Reduction" class="headerlink" title="Dimension Reduction"></a>Dimension Reduction</h4><p>用2维就可以表示这些特征，并不需要用到3D</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628110540545.png" alt="image-20200628110540545" style="zoom:50%;"></p>
<p>这些3只有倾斜的角度不一样，用1D即可表示</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628110631658.png" alt="image-20200628110631658" style="zoom:50%;"></p>
<p>主要有两种方法，feature selection和PCA</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628110752052.png" alt="image-20200628110752052" style="zoom:50%;"></p>
<h4 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h4><p>如果reduce to 1D，我们使用$z_1=w^1 \cdot  x$，使得$x$投影到$w_1$上，即达到了降维的目的，那么我们如何来评价降维的好坏呢？</p>
<p>我们可以使用降维之后数据的variance来评价，variance越大越好</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628112108807.png" alt="image-20200628112108807" style="zoom:50%;"></p>
<p>如果reduce to 2D，那么现在就需要投影到两个不同的方向$(w^1,w^2)$上，再来与$x$做inner product，得到$z_1,z_2$，再分别计算这两者的variance；其中$w^1,w^2$要满足一定的条件，即$w^1\cdot w^2=0$，两者是垂直的，可以保证是不同的方向</p>
<p>那么W就是一个正交矩阵，向量之间相互正交，且向量模长都是1</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628112628933.png" alt="image-20200628112628933" style="zoom:50%;"></p>
<h5 id="Formula"><a href="#Formula" class="headerlink" title="Formula"></a>Formula</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628114149626.png" alt="image-20200628114149626" style="zoom:50%;"></p>
<p>由于$a^Tb$是一个scalar，所以可以直接加上转置符号</p>
<script type="math/tex; mode=display">
\begin{aligned}
(a\cdot b)^2&=a^Tba^Tb\\
&=a^Tb(a^Tb)^T
\end{aligned}</script><p>其中$Ex=\bar x$，协方差$Cov(x)$为</p>
<script type="math/tex; mode=display">
\begin{aligned}
Cov(x)&=\frac{1}{N}\sum(x-Ex)(x-Ex)^T \\
&=\frac{1}{N}\sum(x-\bar x)(x-\bar x)^T
\end{aligned}</script><p>令协方差为S，那么我们现在的问题是 maximizing $(w^1)^TSw^1$，限制条件是</p>
<script type="math/tex; mode=display">
||w^1||_2=(w^1)^Tw^1=1</script><p>先找到$w^1$，可以maximizing $(w^1)^TSw^1$，这里使用了拉格朗日乘数法，再对$w^1$求偏微分，得</p>
<script type="math/tex; mode=display">
Sw^1=\alpha w^1</script><p>即$w^1$为特征向量 eigenvector</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628115707351.png" alt="image-20200628115707351" style="zoom:50%;"></p>
<p>$w^1$为矩阵S的特征向量，对应的特征值$\lambda_1$是最大的</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628115903683.png" alt="image-20200628115903683" style="zoom:50%;"></p>
<p>再找到$w^2$，对$w^2$求偏微分，得</p>
<script type="math/tex; mode=display">
Sw^2-\alpha w^2-\beta w^1=0</script><p>两边同时乘上$(w^1)^T$，得</p>
<script type="math/tex; mode=display">
(w^1)^TSw^2-\alpha (w^1)^Tw^2-\beta(w^1)^T w^1=0</script><p>代入$(w^1)^Tw^1=1,(w^2)^Tw^1=0$，</p>
<script type="math/tex; mode=display">
\begin{aligned}
&(w^1)^TSw^2-\alpha (w^1)^Tw^2-\beta(w^1)^T w^1\\
&=(w^1)^TSw^2-\beta
\end{aligned}</script><p>代入$Sw^1=\lambda w^1$,</p>
<script type="math/tex; mode=display">
\begin{aligned}
&(w^1)^TSw^2\\
&=((w^1)^TSw^2)^T=(w^2)^TS^Tw^1\\
&=(w^2)^TSw^1=\lambda (w^2)^Tw^1=0
\end{aligned}</script><p>那么</p>
<script type="math/tex; mode=display">
0-\alpha\ 0-\beta\ 1=0\quad\rightarrow \beta=0</script><p>代入$Sw^2-\alpha w^2-\beta w^1=0$，可得</p>
<script type="math/tex; mode=display">
Sw^2-\alpha w^2=0\quad \rightarrow Sw^2=\alpha w^2</script><p>可得出$w^2$为矩阵S的特征向量，对应的特征值$\lambda_2$是第二大的</p>
<h5 id="Decorrelation"><a href="#Decorrelation" class="headerlink" title="Decorrelation"></a><strong>Decorrelation</strong></h5><script type="math/tex; mode=display">
W=\begin{pmatrix} 
(w_1)^T\\
(w_2)^T \\
\vdots
\\
(w_K)^T
\end{pmatrix}</script><p>$(w^1)^T$表示W的第一行，且$(w^1)^Tw^1=1,(w^2)^Tw^1=0$，因此$Ww^1=e_1,…,Ww^K=e_K$，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628123018232.png" alt="image-20200628123018232" style="zoom:50%;"></p>
<p>可得出$Cov(z)$是一个对角矩阵，只有正对角线上有元素</p>
<h5 id="Another-Point-of-View"><a href="#Another-Point-of-View" class="headerlink" title="Another Point of View"></a>Another Point of View</h5><p>下图中的7可以由三个部分组成，即$u^1,u^3,u^5$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628123840304.png" alt="image-20200628123840304" style="zoom:50%;"></p>
<p>那么我们目标就是找到这K个component，使得$||(x-\bar x)-\hat x||_2$达到最小值</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628124332233.png" alt="image-20200628124332233" style="zoom:50%;"></p>
<p>x可以分为$x^1,x^2,…$，对应的$c_1$也可以分为$c_1^1,c_1^2,…$，这样就形成了三个矩阵</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628125033081.png" alt="image-20200628125033081" style="zoom:50%;"></p>
<p>那么我们怎么来最小化矩阵之间的最小差值呢？</p>
<p>下图中的U对应PCA中的权重矩阵W，为前文求出来的K个特征向量，为K个component，$\sum,V$表示C矩阵</p>
<p>对于矩阵$XX^T$的K个最大的特征值，矩阵U的每一列表示就表示这些特征值所对应的K个特征向量</p>
<p>做SVD求解出来的U矩阵，就是协方差矩阵$Cov(z)$所对应的特征向量，也就是PCA得出来的解</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628125515261.png" alt="image-20200628125515261" style="zoom:50%;"></p>
<p>其中$c_k$可以用另外一种形式表达出来，$c_k=(x-\bar x)\cdot w^k$，$\cdot$表示做inner product</p>
<p>如果此时K=2，那么$c_1=\sum_{i=1}^3(x-\bar x)\cdot w^k_i$，可以用neural network的形式表达出来，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628155642780.png" alt="image-20200628155642780" style="zoom:50%;"></p>
<p>那么$c_1$乘上$w_i^1$，就可以得到output为$\hat x_i$，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628160122130.png" alt="image-20200628160122130" style="zoom:50%;"></p>
<p>对于$c_2$也有类似的结果</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628160212513.png" alt="image-20200628160212513" style="zoom:50%;"></p>
<p>对于network的output为$\hat x_i$，应与$x-\bar x$之间的error最小化</p>
<p>那么我们就可以把这个结构看成是具有一个hidden layer的network，其output和input应该越接近越好，这就可以叫做<strong>Autoencoder</strong></p>
<p>Q：既然是neural network，那么我们可以用gradient descent来得到和PCA一样的最优解吗？</p>
<p>A：用PCA求解出来的w是相互正交的，可以让reconstruction error最小化，但gradient descent求解出来的w并不能保证这一点，而且并不能使这个reconstruction error比PCA方法更小</p>
<p><span style="color: red">如果是在linear的情况下，使用PCA比较好，用network就会很麻烦；但network可以是deep的，可以中间有很多个hidden layer，这被称为<strong>Deep Autoencoder</strong></span></p>
<h5 id="Weakness-of-PCA"><a href="#Weakness-of-PCA" class="headerlink" title="Weakness of PCA"></a>Weakness of PCA</h5><ul>
<li>unsupervised，输入的data是没有label的，PCA会找一种方式使得降维之后data的variance最大，就可能出现左上的结果，这时两者的class是不一样的，如果还是继续投影到红线上，就会出现两个class的data相互交错的局面；</li>
<li>Linear，对于立体的data，如果还是继续pca，得到的结果也会非常不理想</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628161709298.png" alt="image-20200628161709298" style="zoom:50%;"></p>
<h5 id="Pokemon"><a href="#Pokemon" class="headerlink" title="Pokémon"></a>Pokémon</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628162244781.png" alt="image-20200628162244781" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628162630167.png" alt="image-20200628162630167" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628162636915.png" alt="image-20200628162636915" style="zoom:50%;"></p>
<h5 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628162743780.png" alt="image-20200628162743780" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Unsupervised Learning</tag>
        <tag>Dimension Reduction</tag>
        <tag>PCA</tag>
      </tags>
  </entry>
  <entry>
    <title>基于知识图谱和GCN的应用和开发</title>
    <url>/2020/07/03/knowledge-graph/</url>
    <content><![CDATA[<h4 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h4><p>深度学习的主要思想有三个：</p>
<ul>
<li><p>权重分配；</p>
</li>
<li><p>层级结构：每一层训练的结果都依赖于上一层；</p>
</li>
<li><p>欧几里得空间数据：比如图像数据，可以通过二维坐标(x,y)进行表示，语音信号是一个一维的数据，围棋的话也是一个二维数据，目前的大多数数据都是可以用欧几里得空间数据进行表示的，即可以投影到坐标轴上</p>
</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702101528758-20200706215024224.png" alt="image-20200702101528758"></p>
<p>但在实际生活中，有很多数据并不能用欧几里得数据来进行表示，比如在微信上，每个人都相当于一个节点，朋友之间的关系相当于是一条边，相当于图，没有任何的空间信息，是<u>非欧几里得的空间结构</u>（eg. 社交网络、科学网络等） </p>
<p>那要怎么对这类信息进行训练呢？传统的深度学习方法（CNN和RNN等）并没有办法训练这类数据，因此就产生了图卷积（GCN）</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702103428339.png" alt="image-20200702103428339"></p>
<h4 id="图卷积背景和基本框架"><a href="#图卷积背景和基本框架" class="headerlink" title="图卷积背景和基本框架"></a>图卷积背景和基本框架</h4><h5 id="图卷积发展历史"><a href="#图卷积发展历史" class="headerlink" title="图卷积发展历史"></a>图卷积发展历史</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702104325966.png" alt="image-20200702104325966"></p>
<h5 id="邻接矩阵"><a href="#邻接矩阵" class="headerlink" title="邻接矩阵"></a>邻接矩阵</h5><p>无向图的邻接矩阵是对角矩阵</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702104743073.png" alt="image-20200702104743073"></p>
<h5 id="图卷积基本框架"><a href="#图卷积基本框架" class="headerlink" title="图卷积基本框架"></a>图卷积基本框架</h5><p>在下图中，A为邻接矩阵，H为特征矩阵（$N\times F$），，N为节点个数，F表示每个节点的特征数；比如我们每个人在社交网络中都是一个节点，也有一些对应的特征，名字、年龄、出生日期，这些特征就构成了这个特征矩阵$N\times F$；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702104938456.png" alt="image-20200702104938456"></p>
<p>在这个基本框架中，input为邻接矩阵A，第一个hidden layer把特征矩阵点乘到每一个节点上进行训练，再进入下一层，再进行点乘，…，一直不断重复这个过程。每次训练更新的参数是特征矩阵，最后的output和input是一样的，图结构没有发生变化，但特征矩阵达到了收敛（基本上不会发生变化），最后再对图中的每个节点进行分类；</p>
<p><span style="color: red">把节点和特征矩阵进行点乘，就实现了信息在节点和社区间进行传递</span></p>
<h4 id="图卷积"><a href="#图卷积" class="headerlink" title="图卷积"></a>图卷积</h4><p>那么下面我们将具体介绍节点和特征矩阵是怎样进行点乘的，怎样一步步进行训练。</p>
<h5 id="回顾CNN"><a href="#回顾CNN" class="headerlink" title="回顾CNN"></a>回顾CNN</h5><p>我们先回顾一下CNN是如何做卷积操作的，如下图所示，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702110939157.png" alt="image-20200702110939157"></p>
<h5 id="图卷积-1"><a href="#图卷积-1" class="headerlink" title="图卷积"></a>图卷积</h5><p>这是卷积操作的参数更新公式，</p>
<script type="math/tex; mode=display">
h_i^{l+1}=\sigma(W_0^{(l)}h_0^{(l)}+W_1^{(l)}h_1^{(l)}+\cdot\cdot\cdot+W_8^{(l)}h_8^{(l)})</script><p>图卷积相比较于卷积，在$h_0^{(l)}W_0^{(l)}$的基础上，多了邻接矩阵的信息$\frac{1}{c_{ij}}h_j^{(l)}$，<span style="color: red">权重矩阵W也没有卷积操作那么重要</span>，其参数更新公式为，</p>
<script type="math/tex; mode=display">
h_i^{l+1}=\sigma(h_0^{(l)}W_0^{(l)} + 
\sum_{j\in N_i} \frac{1}{c_{ij}}h_j^{(l)}W_1^{(l)})</script><p>更新参数的时候，通过周围的节点来更新该节点（下图中红色节点），与该节点不直接相连的节点无关</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702111915112.png" alt="image-20200702111915112"></p>
<p>那么我们就可以通过以下公式来更新特征矩阵，</p>
<script type="math/tex; mode=display">
H^{(l+1)}=\sigma(\tilde AH^{(l)}W^{(l)})</script><p>其中A为邻接矩阵，H为特征矩阵，W为权重矩阵，本次的特征矩阵$H^{(l+1)}$根据上一次的特征矩阵$H^{(l)}$生成；</p>
<p>如果只根据邻接矩阵来更新H，会出现一些问题：</p>
<ul>
<li>无法提取自身信息，邻接矩阵对角线上的元素始终是0（如果没有自环），自身的信息在训练过程中是无法考虑进去的；</li>
<li>$\tilde A$没有归一化</li>
</ul>
<p>我们需要把参数更新公式进行一些变化，首先解决邻接矩阵对角线元素为0的问题，</p>
<script type="math/tex; mode=display">
H^{(l+1)}=\sigma((\tilde D-\tilde A)H^{(l)}W^{(l)})</script><p><span style="color: red">其中</span>矩阵$\tilde D$是一个对角矩阵（degree matrix），对角线上的数值表示了每个节点所相连的边数。</p>
<p>归一化处理主要列出两种方式，要根据不同的输入数据来进行选择，其中第一种归一化方式比较常用</p>
<script type="math/tex; mode=display">
H^{(l+1)}=\sigma(\tilde D^{-\frac{1}{2}}(\tilde D-\tilde A)\tilde D^{-\frac{1}{2}}H^{(l)}W^{(l)}) \quad\quad (1)\\
H^{(l+1)}=\sigma(\tilde D^{-1}(\tilde D-\tilde A)\tilde H^{(l)}W^{(l)})</script><h5 id="图卷积的计算"><a href="#图卷积的计算" class="headerlink" title="图卷积的计算"></a>图卷积的计算</h5><p>下面我们将一个具体的例子来讲述公式(1),</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702114057283.png" alt="image-20200702114057283"></p>
<p>在上图中，degree matrix表示了每个节点所相连的边数，比如2表示1号节点相连的边有2条，该对角矩阵也就是前文提到的$\tilde D$，$(\tilde D-\tilde A)$就是laplacian matrix；</p>
<p>我们将特征矩阵$H_0$和权重矩阵W进行随机初始化，再代入公式(1)，即可得出训练完成的结果$H_N$，再求出每一列的最大值，即MaxValue$(H(i))=[1,1,4,1,1,4]$，即分类结果，其中第1，2，4，5个节点分类为1，第3，6个节点分类为4，把整个图的节点分成了两类；</p>
<p>那么为什么可以这样分类呢？我们可以回到原图labeled graph中，节点1，2，5相当于是一个全连接图，可以当作一个类别，而节点4，5都和三个节点相连，有三条边，也可以当作是一个类别；节点3，6就没有前几个节点那样关系比较紧密，因此被分成了第二个类别</p>
<p>我们可以得出以下两点：</p>
<ul>
<li>对于随机产生的特征矩阵，依靠图自身的结构，就可以得到一个很好的分类效果；</li>
<li>在这个例子中，进行10次运算，特征矩阵就收敛了，收敛速度非常快</li>
</ul>
<p>再来回顾下这个核心公式，所有的图卷积的变化和应用都离不开这个公式。W不那么重要，特征矩阵$H^{(l)}$表示每个人根据自己的实际特征来进行特征提取；邻接矩阵$\tilde A$表示每个人（节点）之间的关系，关系不一样就会构建出不同的图</p>
<script type="math/tex; mode=display">
H^{(l+1)}=\sigma(\tilde D^{-\frac{1}{2}}(\tilde D-\tilde A)\tilde D^{-\frac{1}{2}}H^{(l)}W^{(l)}) \quad\quad (1)</script><p><span style="color: red">那么我们要在图卷积进行创新的话，就只有改变邻接矩阵A和特征矩阵H，即如何新建这个图、如何提取这个特征；由于机器学习和深度学习技术的不断发展，特征提取的方法已经非常成熟，<u>关键的创新点还是如何新建这个图</u></span></p>
<h5 id="边信息嵌入"><a href="#边信息嵌入" class="headerlink" title="边信息嵌入"></a>边信息嵌入</h5><p>图中的边信息有时候是包含权重的，我们也应该把边的信息值也考虑进去，即把原图转化为一个线图；在下图中，原图中节点为i，j，m，边为a，b，c，转化为线图后，节点变成a，b，c，边为i，j，m，把边变成了点，把点变成了边</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702122014466.png" alt="image-20200702122014466"></p>
<p>经过两次不同的输入$h_{(i,j)}^l,h_j^{l+1}$，分别对应原图和线图，这样训练就可以把边的信息也整合进去</p>
<h5 id="Example-1-半监督图分类"><a href="#Example-1-半监督图分类" class="headerlink" title="Example 1: 半监督图分类"></a>Example 1: 半监督图分类</h5><p>在下图中，黑色圈点表示已经知道了分类的类别，其他的没有被黑色线条包围的点表示没有进行分类，目标是预测未分类的节点的类别，这里我们还定义了一个损失函数L，Z表示GCN的输出结果（上文的公式(1)）</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702123720321.png" alt="image-20200702123720321"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702124159182.png" alt="image-20200702124159182"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702124253302.png" alt="image-20200702124253302"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702124330022.png" alt="image-20200702124330022"></p>
<p>得到实验结果，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702124418966.png" alt="image-20200702124418966"></p>
<h5 id="Example-2-科学家网络"><a href="#Example-2-科学家网络" class="headerlink" title="Example 2: 科学家网络"></a>Example 2: 科学家网络</h5><p>每篇论文都会有一个题目，互相引用这篇论文则会形成一个关系，我们的任务是根据论文的题目来预测论文的类别（统计、物理、数学）</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702124600168.png" alt="image-20200702124600168"></p>
<p>这个例子只用了2层的GCN，其中$\hat AXW^{(0)}$表示第一层，$\hat A \ {\rm ReLU}(\hat AXW^{(0)})W^{(1)}$表示第二层，再输入softmax函数，整个训练过程更新的参数是X，从表格中的数据也可以看出，GCN取得了非常不错的效果</p>
<h4 id="基于知识图谱的图卷积"><a href="#基于知识图谱的图卷积" class="headerlink" title="基于知识图谱的图卷积"></a>基于知识图谱的图卷积</h4><h5 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h5><blockquote>
<p>知识图谱是Google用于增强其搜索引擎功能的知识库。本质上, 知识图谱旨在描述真实世界中存在的各种实体或概念及其关系,其构成一张巨大的语义网络图，节点表示实体或概念，边则由属性或关系构成。现在的知识图谱已被用来泛指各种大规模的知识库。 </p>
</blockquote>
<p>如果你输入搜索引擎中国这个词，相应的会出来中国的一些特征，比如中国的城市、国土面积、人口等；中国和国家都可以看作是图中的一个节点，中国和国家的关系就可以看作是一条边</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702145803375.png" alt="image-20200702145803375"></p>
<h5 id="图卷积：知识图谱"><a href="#图卷积：知识图谱" class="headerlink" title="图卷积：知识图谱"></a>图卷积：知识图谱</h5><p>先把数据用图表示出来，用户为U，实体为V，$y_{uv}=1$则表示用户u和实体v之间有联系，比如我对电影很感兴趣；每个实体u之间会有关系r，比如电影和教父；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702150629999.png" alt="image-20200702150629999"></p>
<p>将图卷积应用到知识图谱，可以用以下公式来进行更新，不断更新特征矩阵H直至其收敛即可，</p>
<script type="math/tex; mode=display">
H^{(l+1)}=\sigma(\tilde D_u^{-\frac{1}{2}}\tilde A_u\tilde D_u^{-\frac{1}{2}}H^{(l)}W^{(l)}) \quad\quad</script><p>那么我们怎么把实体推荐给用户呢？我们又怎么知道用户是不是对这个实体感兴趣呢？</p>
<p>（1）我们可以针对每个用户再新建一个图，先引入一个函数$S_u(\cdot)$，表示这条边（关系）对用户的重要性；</p>
<p>（2）再把原来的实体矩阵变成一个有权重的矩阵，比如边$r_1$输入函数$S_u(\cdot)$，得到$S_u(r_1)$，就是新的关系值。<u>每个用户都有一个邻接矩阵，再对每个用户的邻接矩阵进行训练，就可以得出用户到底对哪个电影感兴趣</u>；</p>
<p>（3）图卷积；</p>
<p>（4）<strong>标签顺滑</strong>，比如对于一个动物，机器可以很自信地确定这个动物到底是猫还是狗，输出值只有0或者1；由于机器过于自信，可能会导致很多额外的误差，我们现在就可以把这个值改成0.1或者0.9，就可以得到更好的结果。</p>
<p>总结：先新建一个图（knowledge graph，KG），再把用户对感兴趣的实体新建一个权重矩阵，再进行图卷积和标签顺滑。那么现在我们的损失函数就有了一些小变化，结合了图卷积和标签顺滑，即</p>
<script type="math/tex; mode=display">
L=J(\hat y_{uv})+\lambda R(A_u)</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702153035542.png" alt="image-20200702153035542"></p>
<p>下面我们将简要叙述这个训练的过程，图中的水平虚线表示decision boundary：</p>
<p>（a）其中蓝色的点表示用户感兴趣的点（observed）；在其下方有一些其他的点（unobserved），我们希望把黄色的点（被黄色方框围住）就推荐给用户，让用户去选择，而剩下蓝色的点，则是用户不感兴趣的，不推荐给用户；</p>
<p>（b）一共要进行两层的训练，对于第一层的训练结果，我们现在把其中两个黄色的点就推荐给用户了；</p>
<p>（c）对于第二层的训练结果，就把其中的三个点推荐给用户了，但最左端的点也在decision boundary之上，也会被推荐给用户，模型在训练过程中会存在一定的误差，</p>
<p>（d）这里就开始对第二个用户的模型进行训练；</p>
<p>（e）这是加入了标签顺滑（label smoothing）的结果，推荐会更加精准，在图中，我们可以看到用户不感兴趣的点进行下移，用户感兴趣的点（a图中的黄色方框部分）则上移，尽量移到decision boundary之上</p>
<h5 id="Label-Smoothing-Regularization"><a href="#Label-Smoothing-Regularization" class="headerlink" title="Label Smoothing Regularization"></a>Label Smoothing Regularization</h5><p>（1）在下图的能量方程中，$l_u(e_i)$表示用户u是否对实体$e_i$感兴趣（值为0或1），能量方程可以把相同的label保存下来，把不同的label就去掉了；（2）再找到使得能量方程取得最小值的$l_u^<em>$；（3）再通过KL距离函数求得$y_{uv}$和$\hat l_u^</em>(v)$之间的相似度，也是求最小值，可得$R(A)$，再代入损失函数</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702155919603.png" alt="image-20200702155919603"></p>
<p>$\lambda$可以不断做调整，来找到损失函数的最小值</p>
<h5 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h5><p>输入为知识图谱G，新建一个邻接矩阵M后；（2）再把用户对感兴趣的实体新建一个权重矩阵（输入mapping函数S），再进行图卷积更新特征矩阵，即$e_u^{(i)}\leftarrow \sigma (\cdot)$；（3）标签顺滑，更新$l_u^{(i)}(e)$，再代入损失函数中的 J；不断地进行这个过程，只到损失函数收敛为止</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702162309077.png" alt="image-20200702162309077"></p>
<h5 id="结果演示"><a href="#结果演示" class="headerlink" title="结果演示"></a>结果演示</h5><p>下图是几种推荐算法的比较，推荐给用户Top-K的实体，看最后的准确率如何，可以发现GCN的结构取得了非常不错的效果。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702163653325.png" alt="image-20200702163653325"></p>
<p>下面是一些可视化的对比，图1是推荐食物，可以看到GCN的结构取得了很好的效果；图2表示取不同的$\lambda$，得到的$R(A_u)$的值；图三对比了不同算法的运行速度，可以看到KGCN用时最少，速度最快</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702163938245.png" alt="image-20200702163938245"></p>
<h4 id="研究方向"><a href="#研究方向" class="headerlink" title="研究方向"></a>研究方向</h4><h5 id="人体姿势识别"><a href="#人体姿势识别" class="headerlink" title="人体姿势识别"></a>人体姿势识别</h5><p> 人体的关节就可以看作是一个节点，关节之间的骨骼就可以看作是边，结合起来就是一张图；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702165243019.png" alt="image-20200702165243019"></p>
<p>核心框架：输入为一个视频，视频的每个时间序列都是一张图像，每一帧的视频取出来就是一张图像；把每一帧的图像先构建成一个骨骼和关节点图，再通过时间的顺序把每一帧对应的关节点都连接起来，形成一张大图，再输入GCN进行训练，通过分类器识别出这是一个跑步的动作。</p>
<p>在上图中的左下角中，横轴表示每一帧的骨骼图，纵轴表示时间，把相应的骨骼节点连接起来，变成一张大图，再输入GCN进行训练。</p>
<p>下图为实验结果，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702170313601.png" alt="image-20200702170313601"></p>
<h5 id="超图"><a href="#超图" class="headerlink" title="超图"></a>超图</h5><p>超图是图的一个基本概念，下图中的图有8个点，选择不同的点，可以有多种排列组合。如果选一个点，一共有个点可以选择，那么就有8种（$C_n^1$）组合。如果我选两个点，比如$n_3,n_8$，这两个点构成一个子图，就有6种（$C_n^2$）组合。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702170734267.png" alt="image-20200702170734267"></p>
<p>选出不同的点后，得到不同的邻接矩阵，再进行训练。每一层选出不同组合的邻接矩阵再输入GCN进行图卷积。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703093426889.png" alt="image-20200703093426889"></p>
<p>超图卷积的核心公式就是下图中的公式(11)</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703093638629.png" alt="image-20200703093638629"></p>
<h5 id="图构建"><a href="#图构建" class="headerlink" title="图构建"></a>图构建</h5><p>对于给定的数据，新建图之后再输入GCN进行分类，我们要怎么画成一张图呢？</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703093921828.png" alt="image-20200703093921828"></p>
<p>在上图中，observed dynamics表示5个分子的运动轨迹图，我们的目标是构建新图，并预测分子的下一步轨迹；对于人体的动作，我们也得通过学习才能知道，到底是以左手还是右手为重心。</p>
<p>在下图中，展示了图构建的大概过程，编码器和解码器都是GCNs。对于编码器部分，先进行了从边到节点($v\rightarrow e$)的过程，再从节点到边($e\rightarrow v$)，再从边到节点($v\rightarrow e$)；对于解码器，先从边到点，再从点到边，进行图卷积，最后再进行分类，得到下一步的运动轨迹。</p>
<p>其中x表示节点，z表示x这个点与i、j相连的概率，z是一个概率分布。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703095234400.png" alt="image-20200703095234400"></p>
<p>预测下一步运动轨迹：在下图中，我们可以看出预测图和真实值之间差距并不大，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703100157533.png" alt="image-20200703100157533"></p>
<h5 id="子图嵌入"><a href="#子图嵌入" class="headerlink" title="子图嵌入"></a>子图嵌入</h5><p>在原来的GCN中，我们每次只会考虑一个点；如果我们现在把多个点（关联性很大的点）表示成一个点，再输入GCN，效果会不会更好呢？这也是一个很火的研究方向。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703101250920.png" alt="image-20200703101250920"></p>
<h4 id="场景应用"><a href="#场景应用" class="headerlink" title="场景应用"></a>场景应用</h4><h5 id="用户推荐系统"><a href="#用户推荐系统" class="headerlink" title="用户推荐系统"></a>用户推荐系统</h5><p>在搜索中推荐相关联的朋友给你。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703102812106.png" alt="image-20200703102812106"></p>
<h5 id="舆情监控和控制"><a href="#舆情监控和控制" class="headerlink" title="舆情监控和控制"></a>舆情监控和控制</h5><p>每个国家都需要舆情监控，比如关于假消息和真消息的分类或者监控，在网络传播过程中找到假消息源，进行封锁等，用GCN都可以起到很好的作用。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703102736481.png" alt="image-20200703102736481"></p>
<h5 id="自动驾驶"><a href="#自动驾驶" class="headerlink" title="自动驾驶"></a>自动驾驶</h5><p>所有的车都有一个车载系统，每个车载系统之间进行联网，就可以构成非常大的图。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703102850234.png" alt="image-20200703102850234"></p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703103300945.png" alt="image-20200703103300945"></p>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p>本文是观看杨栋老师在B上讲解的直播课程《基于知识图谱和图卷积神经网络的应用和开发》的笔记，直播地址：<a href="https://live.bilibili.com/11869202" target="_blank" rel="noopener">https://live.bilibili.com/11869202</a> ，贪心科技官网录播回放：<a href="https://www.greedyai.com/course/139" target="_blank" rel="noopener">https://www.greedyai.com/course/139</a></p>
]]></content>
      <categories>
        <category>Knowledge Graph</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>知识图谱</tag>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>fGAN</title>
    <url>/2020/07/10/fGAN/</url>
    <content><![CDATA[<h4 id="f-divergence"><a href="#f-divergence" class="headerlink" title="f-divergence"></a>f-divergence</h4><p>$p(x),q(x)$分别表示x从分布P，Q中sample出来的概率。f可以是不同的函数，必须是凸函数，且$f(1)=0$。f-divergence的式子如下，可以表示P和Q之间的差异，</p>
<script type="math/tex; mode=display">
D_f(P||Q)=\int_x q(x)f\left(\frac{p(x)}{q(x)}\right)dx</script><p>那么为什么这个式子可以表示P和Q之间的差异呢？</p>
<p>如果现在$q(x)=p(x)$，那么$D_f(P||Q)=0$，表示q和p之间的距离为0.</p>
<p>如果q和p有一些很小的差距，算出来的divergence就大于0.</p>
<script type="math/tex; mode=display">
\int_x q(x)f\left(\frac{p(x)}{q(x)}\right)dx=0\geq  f\left(\int_x q(x)\frac{p(x)}{q(x)}dx\right)=f(1)=0</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710155622091.png" alt="image-20200710155622091" style="zoom:60%;"></p>
<p>如果$f(x)=xlogx$，那么$D_f(P||Q)$就是KL divergence；</p>
<p>如果$f(x)=-logx$，那么$D_f(P||Q)$就是Reverse divergence；</p>
<p>如果$f(x)=(x-1)^2$，那么$D_f(P||Q)$就是Chi Square；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710162446585.png" alt="image-20200710162446585" style="zoom:60%;"></p>
<h4 id="Fenchel-Conjugate"><a href="#Fenchel-Conjugate" class="headerlink" title="Fenchel Conjugate"></a>Fenchel Conjugate</h4><p>每个凸函数都有一个Conjugate function $f^*$，是由x和$f(x)$导出来的。对于值的计算，我们可以通过穷举所有t的值代入，看到底哪个t可以使$xt-f(x)$的值最大。即</p>
<script type="math/tex; mode=display">
f^*(t)=\mathop{\rm max}_{x\in dom(f)}\{xt-f(x\}</script><p>对于值的计算来举一个例子，当$t=t_1$时，</p>
<script type="math/tex; mode=display">
f^*(t)=\mathop{\rm max}_{x\in dom(f)}\{xt_1-f(x\}</script><p>这时$x$的取值范围为$x=\{x_1,x_2,x_3\}$，代入x的值，计算$x_1t_1-f(x_1),x_2t_1-f(x_2),x_3t_1-f(x_3)$的值，$f^*(t)$即为三者最大；</p>
<p>代入$t_2$的值来计算$f^(t_2)$的值；</p>
<p>……</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710162725028.png" alt="image-20200710162725028" style="zoom:60%;"></p>
<p>这样每个t值都带进去算很麻烦，因此出现了第二种方案，把$xt-f(x)$用图形描述出来。找出这些直线的upper bound，可以发现$f^*(t)$是凸函数。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710164729970.png" alt="image-20200710164729970" style="zoom:60%;"></p>
<p>现在假设$f(x)=xlogx$，代入$xt-f(x)$计算，画出$x=\{0.1,1,10,…\}$时的函数图像，并找出这些直线的upper bound，如下图所示，如果进行了很多次运算，这些直线的upper bound和$f^*(t)=exp(t-1)$的图像很接近。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710165445558.png" alt="image-20200710165445558" style="zoom:60%;"></p>
<p>下图是这个过程具体的证明，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710165854738.png" alt="image-20200710165854738" style="zoom:60%;"></p>
<h4 id="Connection-with-GAN"><a href="#Connection-with-GAN" class="headerlink" title="Connection with GAN"></a><strong>Connection with GAN</strong></h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710170013115.png" alt="image-20200710170013115" style="zoom:60%;"></p>
<p>$f(x)$与$f^*(x)$互为Conjugate function。把$x=\frac{p(x)}{q(x)}$代入，得</p>
<script type="math/tex; mode=display">
D_f(P||Q)=\int_x q(x)f\left\{\mathop{\rm max}_{x\in dom(f)}\{\frac{p(x)}{q(x)}t-f^*(t)\}\right\}dx</script><p>我们现在可以用一个discriminator D，来帮助我们求解这个max的问题，输入为x，输出就是满足条件的t，就不用穷举所有的t才能找到我们的最优解。如果用$D(x)$来替代x，就可以表示$D_f(P||Q)$的lower bound，即</p>
<script type="math/tex; mode=display">
\begin{aligned}
D_f(P||Q)&\geq\int_x q(x)f\left(\frac{p(x)}{q(x)}D(x)-f^*(D(x))\right)dx\\
&=\int_xp(x)D(x)dx-\int_xq(x)f^*(D(x))dx
\end{aligned}</script><p>如果随便找一个D，最后得出来的值肯定比divergence的值小；但如果是找一个最优（max）的D，预测出来的t就是最准的，就可以使结果逼近divergence，即</p>
<script type="math/tex; mode=display">
D_f(P||Q)\approx \mathop{\rm max}_D\int_xp(x)D(x)dx-\int_xq(x)f^*(D(x))dx</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710173339038.png" alt="image-20200710173339038" style="zoom:60%;"></p>
<p>$V(G,D)$可以有不同的形式，不同的divergence就有不同的$V(G,D)$。</p>
<p>下图中列出了不同的divergence和generator。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710173859040.png" alt="image-20200710173859040"></p>
<p>可以用不同的divergence又有什么优点呢？</p>
<h4 id="Mode-Collapse"><a href="#Mode-Collapse" class="headerlink" title="Mode Collapse"></a>Mode Collapse</h4><p>当我们在训练GAN的时候，可能会遇到mode collapse，real data的distribution是非常宽泛的，但generated data的distribution可能会非常小。比如我们在生成二次元人物的时候，可能会出现下图中的结果，某张特定的人脸开始蔓延，变得到处都是，同一张人脸会不断反复地出现。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710174639258.png" alt="image-20200710174639258" style="zoom: 50%;"></p>
<h4 id="Mode-Dropping"><a href="#Mode-Dropping" class="headerlink" title="Mode Dropping"></a>Mode Dropping</h4><p>mode dropping的情况比mode collapse要稍微简单一点，现在real data有两种不同的distribution，而generator只会产生一种distribution的数据。</p>
<p>Generator第一次会先产生一些白皮肤的人，再进行一次generator，会产生一些黄皮肤的人，再进行一次generator，会产生一些黑皮肤的人。每次只产生一种分布的数据。</p>
<p><img src="/Users/liufang/Library/Application Support/typora-user-images/image-20200710175725435.png" alt="image-20200710175725435" style="zoom: 50%;"></p>
<p>会出现这个问题，一个很可能的原因就是divergence选得不好。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710175856174.png" alt="image-20200710175856174" style="zoom: 50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>GAN</tag>
        <tag>fGAN</tag>
        <tag>f-divergence</tag>
        <tag>Fenchel Conjugate</tag>
      </tags>
  </entry>
  <entry>
    <title>Generative Models</title>
    <url>/2020/06/29/generative-models/</url>
    <content><![CDATA[<blockquote>
<p>本文主要叙述了集中generative models，包括PixelRNN、VAE，GAN（生成对抗网络）；还叙述了auto-encoder和VAE的区别，以及VAE进行改进的地方，通过高斯混合模型来对VAE的原理进行了详细介绍。</p>
</blockquote>
<h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><p>Generative Models可以分为三大类：pixelRNN、autoencoder、GAN；</p>
<p>现在machine可以对猫狗进行分类，在将来machine就可以通过自己的学习画出一只猫来。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629141407487.png" alt="image-20200629141407487" style="zoom: 67%;"></p>
<h4 id="PixelRNN"><a href="#PixelRNN" class="headerlink" title="PixelRNN"></a>PixelRNN</h4><h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>如果要生成一张图像，我们可以先生成这张图像的第一个红色pixel，再根据这个pixel生成下一个蓝色pixel，每个pixel可以用一个三维的vector来进行表示</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629141521860.png" alt="image-20200629141521860" style="zoom: 67%;"></p>
<p>这是一个无监督学习的过程，并不需要对data进行标注</p>
<p>如果我们现在把下图中的狗下半身遮住，可以让machine学习出下半身的图片</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629141945697.png" alt="image-20200629141945697" style="zoom: 67%;"></p>
<p>也可以用到声讯号</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629142331286.png" alt="image-20200629142331286" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629142353897.png" alt="image-20200629142353897" style="zoom:50%;"></p>
<h5 id="Practicing-Generation-Models-Pokemon-Creation"><a href="#Practicing-Generation-Models-Pokemon-Creation" class="headerlink" title="Practicing Generation Models: Pokémon Creation"></a>Practicing Generation Models: Pokémon Creation</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629142516414.png" alt="image-20200629142516414" style="zoom: 67%;"></p>
<p>做这个实验时，有一些tips；</p>
<p>由于每个pixel都使用三维的vector来表示（RGB），只有三个channel的值相差特别大时，才会有颜色特别鲜明的图片，但学习结果并不能保证这一点，因此图片会会比较灰蒙蒙的，最后得出来的结果会不太好；</p>
<p>因此，每个pixel最好使用1-of-N encoding来表示，输入一个绿色的方块，vector中只有绿色方块对应的dimensions为1，其他都是0；但有$256\times256$种颜色，颜色种类非常繁多，可以先对类似的颜色做一个<strong>clustering</strong>，类似的颜色都用同一种颜色来表示，可以把颜色数量缩小到167种</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629142535096.png" alt="image-20200629142535096" style="zoom: 67%;"></p>
<p>下面开始正式做实验</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629143804656.png" alt="image-20200629143804656" style="zoom: 67%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629143843226.png" alt="image-20200629143843226" style="zoom: 33%;"></p>
<h4 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h4><h5 id="Auto-encoder"><a href="#Auto-encoder" class="headerlink" title="Auto-encoder"></a>Auto-encoder</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629144022672.png" alt="image-20200629144022672" style="zoom: 67%;"></p>
<h5 id="VAE-1"><a href="#VAE-1" class="headerlink" title="VAE"></a>VAE</h5><p>VAE不像autoencoder那样，直接得出code，经过了一些变换，即$c_i=exp(\sigma_i)\times e_i+m_i$，也是来最小化reconstruction error</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629144250780.png" alt="image-20200629144250780" style="zoom: 67%;"></p>
<p>minimize的目标还有</p>
<script type="math/tex; mode=display">
\sum_{i=1}^3(exp(\sigma_i)-(1+\sigma_i)+(m_i)^2)</script><h5 id="Pokemon-Creation"><a href="#Pokemon-Creation" class="headerlink" title="Pokémon Creation"></a>Pokémon Creation</h5><p>input一个宝可梦图像，进行encoder、decoder，得到reconstruct之后的图像，其中code是10-dim的；</p>
<p>现在我们只选择其中的2个纬度出来，其他纬度的值都固定不变，对这个二维的坐标轴，放入不同的点，再输入对应的Decoder，观察其合成出来的image；如果使用不同维度的点，就可以观察到不同维度生成的image；根据不同的维度，我们就可以根据自己的需要，调整这些维度的值，从而得出我们想要的结果</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629150656140.png" alt="image-20200629150656140" style="zoom: 50%;"></p>
<p>部分结果展示</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629151406932.png" alt="image-20200629151406932" style="zoom:50%;"></p>
<h5 id="Why-VAE"><a href="#Why-VAE" class="headerlink" title="Why VAE?"></a>Why VAE?</h5><p>对于一张满月的图像，先进行encode，再进行decode，使得input和output之间的差值最小化；对半月的图像输入也如此</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629152107223.png" alt="image-20200629152107223" style="zoom: 67%;"></p>
<p>对于encoder的其中一个输出$\sigma_i$，表示noise的variance，需要再输入exp函数进行计算，保证其值是大于0的，是machine自己学习的</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629153400892.png" alt="image-20200629153400892" style="zoom: 67%;"></p>
<p>但只有noise是不够的，还需要加一些限制</p>
<script type="math/tex; mode=display">
\sum_{i=1}^3(exp(\sigma_i)-(1+\sigma_i)+(m_i)^2)</script><p>在下图中，蓝色的线表示$exp(\sigma_i)$，红色的线表示$(1+\sigma_i)$，绿色的线表示两者的差值，可以发现在原点处差值是最小的，$\sigma _i$的值接近1，variance的值接近1</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629153918937.png" alt="image-20200629153918937" style="zoom:50%;"></p>
<p>$(m_i)^2$为正则项regularization term，让结果不那么overfiting</p>
<p>在下图中，为一个高维的坐标（用一维进行了展示），曲线表示image是宝可梦的概率，可以在图中看到，对于一些很像宝可梦的image，$P(x)$值很高，但对于一些比较模棱两可的image，$P(x)$值就很低</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629154606838.png" alt="image-20200629154606838" style="zoom: 33%;"></p>
<p>那么我们到底要怎么来评估这个probability distribution呢？答案是高斯混合模型</p>
<h5 id="Gaussian-Mixture-Model"><a href="#Gaussian-Mixture-Model" class="headerlink" title="Gaussian Mixture Model"></a>Gaussian Mixture Model</h5><p>对于下图中的曲线图，蓝色表示多个高斯模型，黑色曲线表示这些模型通过一定的weight进行叠加之后的结果</p>
<p>现在有很多个gaussian model（1，2，3，4，5…），且都对应了自己的weight $P(m)$（蓝色方块），<span style="color: red">要先根据weight选对应的gaussian，再决定到底要从gaussian中sample哪些data</span> ;我们使用$P(m)$表示每个gaussian的weight，$P(x|m)$表示从对应的gaussian中选出data x的概率</p>
<p>x并不是代表着某个类别，而是用一个vector来进行表示，每个维度表示不同的特征，即 <u>Distributed representation is better than cluster</u></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629155005437.png" alt="image-20200629155005437" style="zoom: 67%;"></p>
<p>我们现在从正态分布中sample一个data z，z的每个dimension就表示某种attribute；</p>
<p>根据z我们就可以得出高斯分布mean和variance，即$\mu (z),\sigma(z)$，z有无穷多个可能，mean和variance也有无穷多个可能</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629160314318.png" alt="image-20200629160314318" style="zoom: 67%;"></p>
<p>在最下方的图中，我们可以认为最中间的圆点被sample到的几率最大，其他圆点被sample到的几率就相对较小；<span style="color: red">每个z被sample到之后，根据某个function，计算出对应的mean和variance，都对应着不同的gaussian model</span></p>
<p>这个function可以是一个neural network，input为z，output为$\mu(z),\sigma(z)$</p>
<p>由于现在是连续的z，$P(z)$的形式也发生了变化，</p>
<script type="math/tex; mode=display">
P(x)=\mathop{\int }_zP(z)P(x|z){\rm d}z</script><h5 id="Maximizing-Likelihood"><a href="#Maximizing-Likelihood" class="headerlink" title="Maximizing Likelihood"></a><strong>Maximizing Likelihood</strong></h5><p>现在我们需要找到z和$\mu(z),\sigma(z)$之间的关系，用network来进行计算的时候也需要一个评估手段，这里我们采用的是最大化$L=\sum_xlogP(x)$，即最大化我们已经看到的image x的likelihood；</p>
<p>那么我们现在就有一个NN的参数需要调整，来最大化likelihood L；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629162138964.png" alt="image-20200629162138964" style="zoom: 67%;"></p>
<p>现在我们还有另外一个distribution $q(z|x)$，和前一个是相反的，其中$z|x$表示给出x，再把x输入网络$NN’$，得到属于z的gaussian distribution，其mean和variance，即$\mu’(x),\sigma’(x)$</p>
<p>$P(x|z)$表示z决定了x的distribution，$q(z|x)$表示x决定了z的distribution</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629163947560.png" alt="image-20200629163947560" style="zoom:45%;"></p>
<p>其中$P(z,x)=P(x)P(z|x)$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629165559403.png" alt="image-20200629165559403" style="zoom:50%;"></p>
<p>我们本来需要调整的参数是$P(x|z)$，使得$P(x)$取得最大值，从而使likelihood取得最大值；但现在的lower bound为$L_b$，是$log(P(x))$的最小值，其中$P(z)$是已知的，不知道的参数是$P(x|z),q(z|x)$，两项都需要调整</p>
<p>Q：这里为什么突然多了一项需要调整的参数？</p>
<p>A：这里并不知道lower bound和likelihood之间的关系到底是怎样的，有可能升高了lower bound，但likelihood反而下降了；引入q就可以解决这个问题</p>
<p>根据原式子，即$P(x)=\mathop{\int }_zP(z)P(x|z){\rm d}z$，只和$P(x|z)$有关，与$q(z|x)$是无关的，即下图中<span style="color: purple">方框</span>部分是不变的，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629170614939.png" alt="image-20200629170614939" style="zoom: 67%;"></p>
<p><strong>如果我们现在固定$P(x|z)$，想通过$q(z|x)$来使$L_b$ 最大化，那么对应的KL divergence就会最小化，到一种很极端的情况，KL divergence会变为0；如果$L_b$继续上升，那么肯定会超出该区域，因此对应的$logP(x)$也会继续上升；KL divergence不断变小的过程，$q(z|x),P(z|x)$之间的差距也会不断缩小</strong></p>
<p>因此，这个过程不仅让likelihood变得越来越大，也会找到和$P(x|z)$接近的$q(z|x)$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629171631388.png" alt="image-20200629171631388" style="zoom:50%;"></p>
<p>对于$q(z|x)$，其中$z|x$表示给出x，再把x输入网络$NN’$，得到属于z的gaussian distribution，就可以知道z是从什么样的gaussian distribution得出来的</p>
<h5 id="Connection-with-Network"><a href="#Connection-with-Network" class="headerlink" title="Connection with Network"></a><strong>Connection with Network</strong></h5><p>现在我们的目标就是最小化$q(z|x),P(z)$之间的KL divergence，即使$q(z|x)$和normal distribution $P(z)$越接近越好，即minimize</p>
<script type="math/tex; mode=display">
\sum_{i=1}^3(exp(\sigma_i)-(1+\sigma_i)+(m_i)^2)</script><p>还需要maximizing</p>
<script type="math/tex; mode=display">
P(x)=\mathop{\int }_zP(z)P(x|z){\rm d}z</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629172034668.png" alt="image-20200629172034668" style="zoom:50%;"></p>
<p>对于input的x，我们先输入网络$NN’$，得到获取z的gaussian distribution；sample得到z之后，再输入网络$NN$，得到获取x的gaussian distribution，使得新分布的mean和x越接近越好</p>
<h5 id="KL散度"><a href="#KL散度" class="headerlink" title="KL散度"></a>KL散度</h5><blockquote>
<p>所谓KL散度，是指当某分布q(x)被用于近似p(x)时的信息损失。</p>
</blockquote>
<p>计算公式为</p>
<script type="math/tex; mode=display">
KL(p||q)=\sum p(x)log\frac{p(x)}{q(x)}</script><h5 id="Problems-of-VAE"><a href="#Problems-of-VAE" class="headerlink" title="Problems of VAE"></a>Problems of VAE</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629174600157.png" alt="image-20200629174600157" style="zoom:50%;"></p>
<h4 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h4><h5 id="The-evolution-of-generation"><a href="#The-evolution-of-generation" class="headerlink" title="The evolution of generation"></a>The evolution of generation</h5><p>v1：会generate一些奇怪的图片，然后会有一个第一代的discriminator，来辨别到底哪一个是real image；</p>
<p>V2： generater会根据上一次discriminater的结果进行调整，第二代生成的image就和真实的image更像了，再把image输入第二代discriminator，再进行比较；</p>
<p>…….</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629175524852.png" alt="image-20200629175524852" style="zoom:50%;"></p>
<h5 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h5><p>对于generator，可以看作是VAE里面的decoder，我们通常选择从某个distributionsample出来的vector，输入到第一代的generator，再生成相对应的image，有多少个vector，就生成多少个image；</p>
<p>把真实的image输入discriminator，与generator生成的image进行比较；并对这些data进行标注，real image标注为1，其余标注为0</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629201306815.png" alt="image-20200629201306815" style="zoom:50%;"></p>
<h5 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h5><p>首先随机sample一个vector，作为Generator的输入，generator会生成对应的image，再把这个image输入discriminator，输出是real image的概率，对于第一代（v1），generator还不成熟，因此概率只有0.87；generator接下来会调整自己的参数，使discriminator认为其生成的image是real image；</p>
<p>这generator+discriminator就相当于一个network，整体目标是使discriminator认为generator生成的image是real image，就可以根据这个指标来使用梯度下降算法，进行back propagation，来不断调整generator的参数；</p>
<p><span style="color: red">注：在训练过程中应该固定discriminator的参数，来调整generator的参数</span></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629201707553.png" alt="image-20200629201707553" style="zoom:50%;"></p>
<h5 id="Why-GAN-is-hard-to-train"><a href="#Why-GAN-is-hard-to-train" class="headerlink" title="Why GAN is hard to train?"></a>Why GAN is hard to train?</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629202903543.png" alt="image-20200629202903543" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>GAN</tag>
        <tag>Unsupervised Learning</tag>
        <tag>Autoencoder</tag>
        <tag>VAE</tag>
        <tag>PixelRNN</tag>
      </tags>
  </entry>
  <entry>
    <title>life_long_learning</title>
    <url>/2020/07/23/life-long-learning/</url>
    <content><![CDATA[<p>Life long learning又可以叫做终身学习；对于人类自身而言，我们每次都是用同一个脑来学习机器学习的每一堂课，每次的作业也都需要训练不同的神经网络，那么我们可不可以每次作业都使用同一个神经网络呢？</p>
<p>life long learning可简称为LLL，也可被称作Continuous Learning, Never Ending Learning, Incremental Learning；</p>
<p>我们可以让机器先学会task 1，再学第二个任务task 2，这时机器就学会了2项技能，……</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723145825710.png" alt="image-20200723145825710" style="zoom:67%;"></p>
<p>那么要让机器学会这么多的技能，到底要学会什么样的技能呢？</p>
<p>主要有以下三点：</p>
<ul>
<li>Knowledge retention，即怎么样让机器把学过的东西记下来，</li>
<li>knowledge transfer，</li>
<li>model expansion，</li>
</ul>
<h4 id="Knowledge-Retention"><a href="#Knowledge-Retention" class="headerlink" title="Knowledge Retention"></a>Knowledge Retention</h4><h5 id="Example-–-Image"><a href="#Example-–-Image" class="headerlink" title="Example – Image"></a>Example – Image</h5><p>现在有一个2层的network，每层都50个neural，我们希望这个network可以学习这两个task，task 1和2都是关于手写数字识别的，但task 1的数据加上了一些杂讯；</p>
<p>先用task 1的训练数据来对network进行训练，训练完成后，再分别对两个task的测试数据进行测试，task 1的测试数据得到了90%的准确率，而task 2的测试数据得到了96%的准确率；虽然network并没有看到过task 2的训练数据，但由于task 1和2是很接近的，因此学习到的东西可以transfer到task 2上，network在task 2的数据上也表现得很好；</p>
<p>如果现在把task 1训练好的参数作为task 2的network的初始值，再用task 2的训练数据继续训练下去，训练完成后，分别在task 1和2的测试集上进行测试；task 2的准确率上升到97%，但却忘记了task 1要怎么做，task 1的准确率下降到80%；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723150701827.png" alt="image-20200723150701827" style="zoom:67%;"></p>
<p>现在我们可以让task 1和2同时进行学习，即把这两者的训练资料都混合到一起，可以发现学习的效果还不错，在task 1上的准确率是89%，在task 2上的准确率是98%；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723152403209.png" alt="image-20200723152403209" style="zoom:67%;"></p>
<p>两个任务同时学习的效果还不错，但这两个任务分开学习，就会忘记之前学习过的内容。</p>
<h5 id="Example-–-Question-Answering"><a href="#Example-–-Question-Answering" class="headerlink" title="Example – Question Answering"></a>Example – Question Answering</h5><p>再举一个知识问答的例子；给出一个document，根据这个document来回答提出的问题；这里用到的数据集是bAbi，一共包括20个不同类型的问题；</p>
<p>现在就有两个选择，这20个task分开进行训练，也可以把这20个task同时训练</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723152851995.png" alt="image-20200723152851995" style="zoom:67%;"></p>
<p>如果我们选择分开训练，先训练task 1，在训练task 2，……；每个task训练完，我们都来观察题型5（who）正确率的变化，在题型1，2，3，4上正确率都很低，在题型5上正确率就突然到了100%，但做完题型6后，又不会做题型5了，忘记了之前学习的结果。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723153450380.png" alt="image-20200723153450380" style="zoom:67%;"></p>
<p>如果我们选择把所有题型的数据进行混合，只用一个network，来一起训练，可以发现结果正确率还不错。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723154043805.png" alt="image-20200723154043805" style="zoom:67%;"></p>
<h5 id="Multi-task-training"><a href="#Multi-task-training" class="headerlink" title="Multi-task training"></a>Multi-task training</h5><p>我们可以把所有任务的训练数据都进行混合，再输入数据network进行训练；这种训练方式就不会忘记之前学过的东西；</p>
<p>但这种学习方式会出现一个新问题，我们之前已经把999个task的数据进行混合，也把这个模型训练好了；但现在来了一个新的task 1000，如果还使用这种训练方式，我们就必须把前面999个task的训练资料一直保存，再将1000个task的数据重新混合，重新进行训练；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723154422819.png" alt="image-20200723154422819" style="zoom:67%;"></p>
<p>那么我们可不可以通过一种方式，使机器不要重新读取之前的任务数据，就可以保存机器过去学过的东西，这也就是life long learning要探讨的问题；</p>
<h5 id="Elastic-Weight-Consolidation-EWC"><a href="#Elastic-Weight-Consolidation-EWC" class="headerlink" title="Elastic Weight Consolidation (EWC)"></a>Elastic Weight Consolidation (EWC)</h5><p>我们可以通过一种经典的解法，EWC来解决这个问题；有一些weight对上一个task是非常重要的，如果这些weight发生了变化，机器就会忘记过去的任务要怎么做了；我们可以把这些重要的参数保护起来，只改变那些不重要的参数。</p>
<p>$\theta^b$表示前面任务学习完的模型参数，其中的每一个参数$\theta_i^b$都有一个guard $b_i$，来说明这个参数的重要性；</p>
<p>现在我们就有了一个新的loss function $L’(\theta)$，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723155444021.png" alt="image-20200723155444021" style="zoom:67%;"></p>
<p>如果$b_i=0$，那么参数$\theta_i$就可以不受任何的限制，可以任意调整$\theta_i$的值；</p>
<p>如果$b_i=\infty$，就表示$\theta_i$已经被强烈地保护起来了，不可以对$\theta_i$的值进行变化。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723160629095.png" alt="image-20200723160629095" style="zoom:67%;"></p>
<p>loss function中的$\sum b_i(\theta_i-\theta_i^b)^2$表示一种正则化，由于$b_i$的存在，我们希望$\theta_i,\theta_i^b$之间的距离有时候越近越好，有时候什么距离都可以。</p>
<p>我们再举一个具体的例子来叙述EWC。</p>
<p>下图表示有两个task，参数分别也只有两个$\theta_1,\theta_2$，颜色表示error surface的变化，颜色越深，表示loss越小；</p>
<p>假设task 1之前没有其他的训练任务，在训练task 1时，首先进行随机初始化，初始参数$\theta^0$在途中所示位置，沿着梯度下降的方向走，到了$\theta^b$；</p>
<p>现在进行task 2的训练，把$\theta^b$的值复制到task 2上，沿着梯度下降的方向走，走到loss最小的地方，到了图中$\theta^*$的位置；</p>
<p>这时我们会发现task 2的学习结果完全忘记了task 1，现在的$\theta^*$会得到很高的loss。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723161429363.png" alt="image-20200723161429363" style="zoom:67%;"></p>
<p>在使用EWC之后，我们会为每个参数设置一个$b_i$，不同文章有不同的做法；一个比较简单的做法是，算每个参数的二次微分，得到下图中右边的曲线图；</p>
<p>在task 1上，对于其中的一个参数$\theta_1$，其值对loss的结果影响不大，我们可以把$b_1$设置为一个很小的值；但$\theta_2$的变化对loss的影响就很大，就必须要给$\theta_2$一个很大的$b_2$，在下一个task时，就尽量不要对这个参数进行变化。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723163027322.png" alt="image-20200723163027322" style="zoom:67%;"></p>
<p>我们再来看task 2，现在参数移动的方向就发生了变化，为了保证不对$\theta_2$进行变化，task 2的参数更新方向只能是水平的，只改变$\theta_1$的值，不改变$\theta_2$的值。如果在这个方向进行变化，就算我们训练好了task 2，也不会对task 1的精确度造成太大影响。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723181638276.png" alt="image-20200723181638276" style="zoom:67%;"></p>
<p>这是原论文中的训练结果，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723181923345.png" alt="image-20200723181923345" style="zoom:67%;"></p>
<p>还有一些最新的研究，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723182233611.png" alt="image-20200723182233611" style="zoom:67%;"></p>
<h5 id="Generating-Data"><a href="#Generating-Data" class="headerlink" title="Generating Data"></a><strong>Generating Data</strong></h5><p>在进行multi-task learning时，既然memory是有限的，那么我们可不可以训练一个模型，来产生以前的训练资料呢？</p>
<p>实际上是可以的，首先需要训练一个generator，来生成task 1的训练数据，那么现在就可以丢掉task 1的数据；接下来就可以把task 1和2的训练数据进行混合，一起学习出新的模型</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723182325608.png" alt="image-20200723182325608" style="zoom:67%;"></p>
<p>但generator生成一些很高清的图像，是有很大难度的。</p>
<h5 id="Adding-New-Classes"><a href="#Adding-New-Classes" class="headerlink" title="Adding New Classes"></a>Adding New Classes</h5><p>对于之前我们叙述的任务，都是网络结构不需要调整的；如果现在有一个任务是进行10个类别的分类，另一个任务是进行20个类别的分类，就需要修改网络的结构，下图展示了一些修改网络结构的方法；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723183254274.png" alt="image-20200723183254274" style="zoom:67%;"></p>
<h4 id="Knowledge-Transfer"><a href="#Knowledge-Transfer" class="headerlink" title="Knowledge Transfer"></a>Knowledge Transfer</h4><p>我们希望机器不仅能记住knowledge， 还可以把不同任务之间的knowledge进行transfer；</p>
<p>如果对于每个task，我们都单独学习一个模型，就不会遗忘过去训练的模型了；但这种做法没有考虑不同模型之间的knowledge的转换，而且我们并不能把所有的模型都存储下来。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723183818913.png" alt="image-20200723183818913" style="zoom:67%;"></p>
<h5 id="Life-Long-v-s-Transfer"><a href="#Life-Long-v-s-Transfer" class="headerlink" title="Life-Long v.s. Transfer"></a>Life-Long v.s. Transfer</h5><p>transfer learning是先训练好一个模型task 1，在进行fine-tune得到另外一个模型task 2，并不会再重新回去学习task 1；</p>
<p>life long learning比transfer learning多考虑了一步，希望不忘记过去学到的东西。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723185524830.png" alt="image-20200723185524830" style="zoom:67%;"></p>
<h5 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a><strong>Evaluation</strong></h5><p>下面我们介绍如何来对life long learning进行评价，评价的方式有很多种，这里只说一个大概的方向。</p>
<p>一般都会画一个矩阵出来，纵轴表示在task上的performance，第一行表示学完task 1之后，在每个task上的performance；其中$R_{i,j}$表示在训练完task i之后，在task j上的performance；</p>
<p>如果$i&gt;j$，表示在训练任务i之前，已经学完了第j个任务，$R_{i,j}$则表示学任务i之后，之前的任务j到底忘记了多少；</p>
<p>如果$i&lt;j$，表示机器学习了第i个任务，但还没学到第j个任务，$R_{i,j}$则表示前i个任务能不能transfer到第j个任务上；</p>
<p>在这个矩阵的基础上，我们就可以定义各式各样的标准，比如我们可以算Accuracy，表示在第T个任务学习完后，在第1到T个任务上的performance的平均值，即把矩阵最后一行的performance都平均起来，</p>
<script type="math/tex; mode=display">
{\rm Accuracy}=\frac{1}{T}\sum_{i=1}^TR_{T,i}</script><p>如果想知道机器有多会做knowledge retention，我们就可以定义一个Backward Transfer， 先看机器在学完第一个task的performance $R_{1,1}$，再看一直学习到T个task之后，在task 1上的performance $R_{T,1}$，再计算这两者之间的差距；</p>
<script type="math/tex; mode=display">
{\rm Backward\ Transfer}=\frac{1}{T-1}\sum_{i=1}^{T-1}R_{T,i}-R_{i,i}</script><p>这个值一般是负值，在刚开始时准确率最高，学习到后面的task之后，就慢慢忘记了前面task 1，在task 1上的performance就变小了，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723190928029.png" alt="image-20200723190928029" style="zoom:67%;"></p>
<h5 id="Gradient-Episodic-Memory-GEM"><a href="#Gradient-Episodic-Memory-GEM" class="headerlink" title="Gradient Episodic Memory (GEM)"></a>Gradient Episodic Memory (GEM)</h5><p>对于现在在训练的task 2，如果我们修改其梯度的方向，就可以提高之前学习完的task 1的performance；</p>
<p>在下图中，现在要训练的模型是$\theta$，再计算出参数更新的方向g；同时，我们还需要一些前面task的训练资料，再根据这些资料计算出前面task的梯度反方向，也就是前面task参数更新的方向$g^1,g^2$，</p>
<p>如果$g^1,g^2$和g的内积分别都是正的（左），那么我们参数更新的方向就是现在的g；</p>
<p>但也有可能这个内积的负的（右），现在$g^1,g$之间的内积就是负的，如果继续往g的方向更新参数，很可能会对$g^1$这个方向的task造成伤害；我们可以把g的方向稍微变换一下，变成$g’$的方向，使$g’\cdot g^1\geq0,g’\cdot g^2\geq0$，这样进行参数的更新，就不会对之前的task 1和2造成伤害，很可能还会提高其performance；</p>
<p>但g的方向不能变化太大，不能伤害到现在的task。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723193437276.png" alt="image-20200723193437276" style="zoom:67%;"></p>
<p>下面是实验结果展示，GEM算法的正确率都还不错。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723200038272.png" alt="image-20200723200038272" style="zoom:67%;"></p>
<h4 id="Model-Expansion"><a href="#Model-Expansion" class="headerlink" title="Model Expansion"></a>Model Expansion</h4><p>network也可以学习自动扩张，比如扩充一些神经元，但这个扩张并不是毫无限制的，我们希望这个扩张是有效率的，模型扩张的速度最好不要和task加进来的速度成正比。</p>
<h5 id="Progressive-Neural-Networks"><a href="#Progressive-Neural-Networks" class="headerlink" title="Progressive Neural Networks"></a>Progressive Neural Networks</h5><p>首先需要训练task 1，再用新的model来训练task 2，由于要进行knowledge transfer，前一个task的hidden layer的输出，会作为task 2的后面几层的输入；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723200623400.png" alt="image-20200723200623400" style="zoom:67%;"></p>
<h5 id="Expert-Gate"><a href="#Expert-Gate" class="headerlink" title="Expert Gate"></a>Expert Gate</h5><p>每个task还是需要训练自己的模型，如果进来一个新的任务，会去和旧的任务进行比较，看哪一个旧任务和这个任务最接近；选择最接近的任务来作为新任务的初始化参数；</p>
<p>每次新加入一个任务，都会新加入一个模型，模型的数量和任务的数量是成正比的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723201318638.png" alt="image-20200723201318638" style="zoom:67%;"></p>
<h5 id="Net2Net"><a href="#Net2Net" class="headerlink" title="Net2Net"></a>Net2Net</h5><p>如果要把network进行扩张，就需要新加入一些neural，这些新加入的neural很可能会对之前的任务造成影响，很可能会忘记过去学习到的技能。那么怎么加入新的neural，而不忘记过去的技能呢？</p>
<p>如下图所示，要扩张的新神经元是$h[3]$，可以发现扩张之后的网络要做的事和扩张之前是一样的；</p>
<p>但这样做可能有一个问题，这两个网络是完全一样的，我们可以在扩张之后的network加一些noise，使这两个network有区别，但又不会影响原来network已经学习到的技能</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200723201801447.png" alt="image-20200723201801447" style="zoom:67%;"></p>
]]></content>
      <categories>
        <category>Knowledge Graph</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Life Long Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Meta Learning</title>
    <url>/2020/07/22/meta-learning/</url>
    <content><![CDATA[<blockquote>
<p>本文主要叙述了meta learning的核心思想，对比了machine learning和meta learning的三个步骤；接着介绍了meta learning一般会用到的数据集Omniglot；除了meta learning，本文还讲了transfer learning的其他两个技术：MAML和Reptile。</p>
</blockquote>
<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>Meta learning可以让机器学习如何去learn；机器现在学习了几个task，由于机器已经学会了如何去learn，根据过去的经验，机器在新任务上学习的效果会更好；</p>
<p>比如现在有task：speech recognition、image recognition，在将来有另外一个task 101: text classification，虽然文字辨识和前面的100个task都没什么关系，但机器学习到了一些learning skills，就可以让文字辨识做的更好，新的任务也会学习得更快。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200721142014553.png" alt="image-20200721142014553" style="zoom:67%;"></p>
<p><strong>life-long learning</strong>（终身学习）：只用一个模型来学习不同的task；</p>
<p>meta learning：每个task都有自己的模型，我们希望机器从以前的模型中来学习，使将来要训练的模型可以训练得更快更好。</p>
<p>在下图中，先回忆一下machine learning，我们希望学习出一个learning algorithm进行猫狗分类，算法的输入是training data，这个算法可以学习出一个function f；现在我们用一张图像输入这个函数f，就可以得出具体的类别；</p>
<p>在meta learning里，我们也可以把这个learning algorithm看成是一个function F，输入training data $D_{train}$，输出另外一个function $f^*$，这个函数就可以用来做猫狗辨识；即</p>
<script type="math/tex; mode=display">
f^*=F(D_{train})</script><p>meta learning的任务就是找到这样一个function F。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200721144054970.png" alt="image-20200721144054970" style="zoom:67%;"></p>
<p>再来总结下machine leaning和meta learning的区别。</p>
<ul>
<li><p>machine learning，是要机器有能力根据training data找出一个函数f；</p>
</li>
<li><p>meta learning则是要机器有能力去找一个函数F，这个F可以找出函数f，f可以用在machine learning里面，比如来进行图像辨识；模型的输出是另外一个function $f^*$，也就是一个neural network的参数；</p>
</li>
</ul>
<p>machine 和meta learning都是要找一个function，只是这个function的作用不同。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200721145318448.png" alt="image-20200721145318448" style="zoom:67%;"></p>
<h4 id="Three-Steps"><a href="#Three-Steps" class="headerlink" title="Three Steps"></a>Three Steps</h4><p>我们再来回忆下machine learning的三个步骤，</p>
<ul>
<li>Step 1: define a set of function；</li>
<li>Step 2: goodness of function，定义一个loss function；</li>
<li>Step 3: pick the best function，使用gradient descent算法，找出最好的那个function。</li>
</ul>
<p>meta learning也是三个步骤，把上面的function f换成learning algorithm F，</p>
<ul>
<li>Step 1: define a set of <u>Learning algorithm 𝐹</u>，因为我们并不知道哪一个learning algorithm最好；</li>
<li>Step 2: goodness of <u>Learning algorithm 𝐹</u>，定义一个loss function；</li>
<li>Step 3: pick the best <u>Learning algorithm 𝐹</u>，找出最好的那个algorithm。</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200721150620093.png" alt="image-20200721150620093" style="zoom:67%;"></p>
<h5 id="Step-1-define-a-set-of-learning-algorithm"><a href="#Step-1-define-a-set-of-learning-algorithm" class="headerlink" title="Step 1: define a set of learning algorithm"></a>Step 1: define a set of learning algorithm</h5><p>对于常规的一个learning algorithm，gradient descent的流程大概如下：首先要定义一个network structure，把这个网络的参数进行初始化，这里初始化为$\theta_0$；再根据training data计算gradient g，根据g来更新网络的参数；……；一直重复这个过程，直到模型收敛输出最终的参数$\hat\theta$。</p>
<p>在下图的方框中，红色的方块都是我们人为设计的，这些方块选择了不同的设计，也就得到了不同的learning algorithm。</p>
<p>那么如果用到了meta learning，我们就可以让机器来设计这些红色方块，就不用我们人为设计了；比如初始化参数这一项，不同的参数就对应不同的algorithm，现在我们就可以让机器来学习出要初始化的参数是什么，学习出来的参数就是机器认为最好的参数。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200721151145134.png" alt="image-20200721151145134" style="zoom:67%;"></p>
<h5 id="Step-2-goodness-of-a-function-𝐹"><a href="#Step-2-goodness-of-a-function-𝐹" class="headerlink" title="Step 2: goodness of a function 𝐹"></a>Step 2: goodness of a function 𝐹</h5><p>定义完algorithm之后，我们就要对这些algorithm进行评价，看哪一个algorithm最好；首先需要定义一个loss function。</p>
<p>现在我么有一个任务Task 1:猫狗分类。先准备一些训练资料，输入我们的learning algorithm F，使其输出对应的函数$f^1$；我们还需要用测试集来评估学习出来的参数的好坏，把测试集输入这个参数$f^1$，得到的结果我们用$l^1$来表示；</p>
<p>machine learning需要很大的一个testing set来评估好坏，但meta learning则需要准备task sets，才能衡量learning algorithm的好坏。</p>
<p>现在就有另外一个Task 2：苹果和橘子的分类，来作为测试的另外一个task。把现在的训练资料输入F，输出对应的参数$f^2$；把测试数据输入$f^2$这个网络，输出的结果为$l^2$;</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200721153030099.png" alt="image-20200721153030099" style="zoom:67%;"></p>
<p>现在有两个loss值 $l^1,l^2$，我们还可以训练其他的task，也可以得到类似的loss function；假设现在测试了N个task，把所有的loss都加起来，就可以对我们的learning algorithm F进行评价，$L(F)$的值越小，说明F越好，</p>
<script type="math/tex; mode=display">
L(F)=\sum_{n=1}^Nl^n</script><p>我们再来对machine和meta learning所需要的训练、测试资料进行对比；</p>
<p>对于machine learning，所需的资料是training data和testing data；</p>
<p>对于meta learning，所需的资料则是task的集合，包括training tasks和testing tasks；每一个task里面都有训练资料和测试资料。此外还有一个额外的要求，training和testing tasks需要是不太一样的，比如training task是猫狗分类、苹果橘子分类，testing task就得是其他的，比如汽车分类，不能和training task重复；</p>
<p>meta learning和machine learning一样，有时也需要验证，这时把training task分一部分出来当作validation tasks即可。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200721154729838.png" alt="image-20200721154729838" style="zoom:67%;"></p>
<p>在大多数情况下，meta learning和few-shot learning都会放到一起讨论。<strong>few-shot learning</strong>是指在我们做任务时资料很少，比如在做图像分类任务时，每个类别只有非常少量的图像。</p>
<p>在few-shot learning里，对于training tesks里面的其中一个task，我们把这个task的training set叫做<strong>support set</strong>，把testing set叫做<strong>query set</strong>；这些task的support set结合起来，就是整个meta learning的training set。</p>
<h5 id="Step-3：pick-the-best-learning-algorithm"><a href="#Step-3：pick-the-best-learning-algorithm" class="headerlink" title="Step 3：pick the best learning algorithm"></a>Step 3：pick the best learning algorithm</h5><p>找到loss function之后，我们就需要找一个最好的learning algorithm $F^<em>$，使对应的loss function取得最小值；即解一个最优化问题，找到 $F^</em>$，使$L(F^*)$取得最小值，</p>
<script type="math/tex; mode=display">
F^*=arg\mathop{\rm min}_FL(F)</script><p>训练完成后，就需要进行测试，测试也需要对应的support和query set。先把support set输入最好的那个learning algorithm $F^<em>$，输出一个函数$f^</em>$；把query set输入网络$f^*$，得到对应的loss l。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200721160441690.png" alt="image-20200721160441690" style="zoom:67%;"></p>
<h4 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h4><p>在做image classification时，有很多人都会用MNIST这个数据集；在meta learning里，通常会用Omniglot这个数据集；</p>
<p>Omniglot中有1623种不同的符号，每个符号有20种不同的写法；在下图的右上角，就表示有20个不同的人来画这同一个符号。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200721163050451.png" style="zoom:67%;"></p>
<p>在使用时，我们通常会设计成一个few-shot classification；也就是我们要先决定分类任务有多少个ways、shot，way表示类别，shot表示example的数量；那么<strong>N-ways K-shot classification</strong>，就表示在每个task里，有N个类别，有K个example。</p>
<p>在下图中，这个task有20个类别，每个类别只提供一个训练资料。support set有20个字符，每个字符都对应一个类别；我们希望机器只看了每个类别的一个example，就可以分辨这20个不同的类别；在这个task的测试阶段，输入query set中的图像，网络能输出对应的类别。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200721163430413.png" alt="image-20200721163430413" style="zoom:67%;"></p>
<p>那么我们要怎么确定training/testing task呢？</p>
<p>我们可以把Omniglot的数据集划分成training和testing characters；从N个training example中sample出N个类别的字符，再从每个类别中sample出K个example，这就可以当作是training/testing task。</p>
<h4 id="Techniques-Today"><a href="#Techniques-Today" class="headerlink" title="Techniques Today"></a>Techniques Today</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200721172350105.png" alt="image-20200721172350105" style="zoom:67%;"></p>
<h4 id="MAML"><a href="#MAML" class="headerlink" title="MAML"></a>MAML</h4><h5 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h5><p>MAML的核心思想是：学习一个初始化的规则，不像机器学习的任务是从一个distribution中sample出数据来进行初始化，而是机器自己学习出一个它认为最好的初始化参数。</p>
<p>首先需要定义一个loss function，来评价这个初始化参数$\phi$的好坏；</p>
<script type="math/tex; mode=display">
L(\phi)=\sum_{n=1}^Nl^n(\hat\theta^n)</script><p>其中$\phi$表示模型初始化的参数，不同的参数初始化，学习出来的模型也是不一样的，$\hat\theta^n$表示第n个task上学习出来的model，$l^n(\hat\theta^n)$表示使用$\hat\theta^n$这个model在第n个task的测试数据上的loss。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200721172317641.png" alt="image-20200721172317641" style="zoom:67%;"></p>
<p>我们可以使用gradient descent算法，找出对应的$\phi$，使得loss $L(\phi)$达到最小值；</p>
<script type="math/tex; mode=display">
\phi\leftarrow\phi-\eta\Delta_\phi L(\phi)</script><h5 id="Model-pre-training-vs-MAML"><a href="#Model-pre-training-vs-MAML" class="headerlink" title="Model pre-training vs MAML"></a>Model pre-training vs MAML</h5><p>在transfer learning中，如果一个task的数据很少，另一个task的数据很多，我们通常都会把model预训练到数据多的task上，然后在另一个task上用少量的资料fine-tuning，这就是<strong>model pre-training</strong>，其loss function为</p>
<script type="math/tex; mode=display">
L(\phi)=\sum_{n=1}^Nl^n(\phi)</script><p><span style="color: red"> Model pre-training的<strong>loss function</strong>和meta learning是不一样的</span>，meta learning是使用训练之后输出的模型$\hat\theta^n$来计算loss，但model pre-training使用最初的模型（初始化为$\phi$，其输出并不是另外一个模型的参数）来计算loss。</p>
<p>下面举一个更加具体的例子来说明这两者的区别。横轴表示model的变化，实际上是高维的，这里用一维表示；深绿色和浅绿色的曲线表示不同的task对应的loss和parameter之间的关系；</p>
<p><strong>MAML</strong></p>
<p><u>我们并不关心$\phi$在training tasks上的表现，只关心用$\phi$训练出来的结果$\hat\theta^n$，在task的测试数据上的表现。</u></p>
<p>在下图的<strong>MAML</strong>中，我们有一个初始的$\phi$，可能在task 1上表现不是很好，在task 2上表现很好（loss小）。但这是一个很好的初始值，如果沿着task 1的梯度方向移动，就找到了可以使task 1的loss最小化的参数$\hat\theta^1$；如果沿着task 2的梯度方向移动，也可以找到使task 2的loss最小化的参数$\hat\theta^2$。</p>
<p> <u>虽然这个初始值刚开始表现并不好（loss有点大），但模型使用这个初始值$\phi$<span style="color: red">进行训练之后</span>，在task 1和2上的表现都还不错，可以让这两个task都变得很强，我们就可以认为这是一个很好的初始值。</u></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200721211520093.png" alt="image-20200721211520093" style="zoom:67%;"></p>
<p><strong>Model Pre-training</strong></p>
<p>在下图的<strong>Model Pre-training</strong>中，mode pre-training的目的是找到一个初始值，这个初始值在两个任务上都要表现得很不错；但并不能保证这个初始值训练之后会得到一个更好的$\hat\theta^n$；</p>
<p>很可能这个$\phi$刚开始表现很好，但训练之后并不一定很好。对于下图中的初始值$\phi$，如果沿着task 1的梯度方向继续移动，会进入一个local minimum，这个loss并不小，比global minimum要高；</p>
<p>这个$\phi$对meta learning来说并不是一个好的初始值，但对model pre-training来说却是一个还不错的初始值，因为mode pre-training并没有把训练这件事考虑进去</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200721213808813.png" alt="image-20200721213808813" style="zoom:67%;"></p>
<p>下图为一个更加形象化的对比。MAML要找到一个初始值，这个初始值可以在训练之后得到很好的performance，看中的是“<strong>潜力</strong>”；model pre-training则是要找到一个初始值，这个初始值可以在训练时就取得很好的performance，在乎“<strong>现在表现如何</strong>”。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200721215540747.png" alt="image-20200721215540747" style="zoom:67%;"></p>
<h5 id="Practice"><a href="#Practice" class="headerlink" title="Practice"></a>Practice</h5><p>我们使用gradient descent来更新参数，现在这个learning algorithm只更新一次参数；即现在我们找到了初始值，这个初始值$\phi$是模型自己找出来的，经过一次gradient的更新之后，就得到了模型的训练结果$\hat\theta$;</p>
<p>$\hat\theta,\phi$之间的关系可以用一个式子更加直观地表示，</p>
<script type="math/tex; mode=display">
\hat\theta=\phi-\epsilon\Delta _\phi l(\phi)</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200721220257052.png" alt="image-20200721220257052" style="zoom:67%;"></p>
<p>Q：为什么模型只更新一次参数呢？一般的gradient descent算法都是更新成千上万次参数。</p>
<p>A：原因如下：</p>
<ul>
<li>meta learning训练的计算量一般都很大，每次更新参数很可能都需要1个小时，如果更新成千上万次，后果不堪设想；</li>
<li>MAML希望在训练之后，可以得到一个很好的结果，比如现在训练出了一个超级好的初始值，只需要更新一次就可以得到更好的结果，这在实际应用中是非常方便的；</li>
<li>虽然在训练时只能更新一次参数，但在测试时可以多更新几次，即在testing task上，我们可以多更新几次，来使模型达到更好的效果；</li>
<li>few-shot learning只有很少的数据，如果更新很多次参数，很可能产生overfitting，但只更新一次就不会出现这个问题。</li>
</ul>
<h5 id="Toy-Example"><a href="#Toy-Example" class="headerlink" title="Toy Example"></a>Toy Example</h5><p>现在有一个toy example，展示了MAML与transfer learning的关系。</p>
<p>对于训练和测试要用到的每个task，现在有一个x和y的sin函数，即$y=a\ sin(x+b)$；我们从这个函数中sample K个点出来，当作训练资料，再用这K个训练资料来估计原来的function，这个function要和原来拿来做sample的y要越接近越好；</p>
<p>sample不同的a、b，我们就可以生成不同的task。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200721230533643.png" alt="image-20200721230533643" style="zoom:67%;"></p>
<p>下图展示了在toy example上model pre-training和MAML的结果；</p>
<p>下图中<span style="color: orange">橙色的曲线</span>表示测试的task，从这个曲线中sample出10个点， 再用这些训练资料输入模型，来估计出一个function，这个function要和橙色曲线所对应的function越接近越好。</p>
<p>如果用<strong>model pre-training</strong>，即下图的右上方，会是图中水平的那个虚线；因为model pre-training的目标是找到一个初始化的参数，在所有的training task上表现都很好，这些task都是不同sin函数的集合，有的是波峰，有的是波谷，这些函数叠起来就是一条水平线；</p>
<p>这个水平线并不是一个很好的参数，如果使用这个<span style="color: green">水平线</span>当作初始值，再去做fine-tuning，不管是1个step、还是10个step，训练出来的参数都是一条水平线，只是把原来的水平线进行平移了而已，结果仍然很差；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200722091027774.png" alt="image-20200722091027774" style="zoom:67%;"></p>
<p>如果使用<strong>MAML</strong>，即上图的右下方，模型选择的初始化参数是<span style="color: green">绿色曲线</span>，在训练过程会更新一次参数，变成<span style="color: red">红色曲线</span>，可见红色曲线和目标函数已经有一些相似点了，波峰对应的都是波峰；在测试过程，我们可以多次更新参数，<span style="color: purple">紫色曲线</span>就是参数更新了10次的结果，和原来的橙色曲线已经非常接近了。</p>
<h5 id="Omniglot-amp-Mini-ImageNet"><a href="#Omniglot-amp-Mini-ImageNet" class="headerlink" title="Omniglot &amp; Mini-ImageNet"></a>Omniglot &amp; Mini-ImageNet</h5><p>下图展示了MAML在Omniglot &amp; Mini-ImageNet上的结果，可以看到都取得了很不错的效果。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200722093522638.png" alt="image-20200722093522638" style="zoom:67%;"></p>
<h5 id="Warning-of-Math"><a href="#Warning-of-Math" class="headerlink" title="Warning of Math"></a>Warning of Math</h5><p>现在我们就具体来对MAML求梯度的过程进行推导。</p>
<p>$\phi$可以看作是多个参数的集合，那么loss对$\phi$求梯度就可以看作，loss对其中的每个$\phi_i$求梯度，即$\frac{\partial l(\hat\theta)}{\partial\phi_i}$，那么$\Delta_\phi l(\hat\theta)$就可以看作一个vector；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200722101619668.png" alt="image-20200722101619668" style="zoom: 67%;"></p>
<p>我们先看其中的一个梯度$\frac{\partial l(\hat\theta)}{\partial\phi_i}$，其物理意义是：如果我们对$\phi_i$进行小小的变化，那么$l(\hat\theta)$会产生什么样的变化。$\phi_i$是一个初始参数，这个参数会影响最终训练出来的模型参数$\hat\theta$，也就影响了模型参数中的每个参数$\hat\theta_1,\hat\theta_2,…,\hat\theta_j,…$，这些参数也就会影响最后的loss；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200722101638527.png" alt="image-20200722101638527" style="zoom:67%;"></p>
<p>即初始参数$\phi$通过中间的每个参数$\hat\theta_j$影响了最后的loss，这里可以应用chain rule，即</p>
<script type="math/tex; mode=display">
\frac{\partial l(\hat\theta)}{\partial\phi_i}=\sum_j\frac{\partial l(\hat\theta)}{\partial\hat\theta_j}\frac{\partial \hat\theta_j}{\partial\phi_i}</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200722101608007.png" alt="image-20200722101608007" style="zoom:67%;"></p>
<p>meta learning的learning rate有两个，$\eta$是每一个参数在训练时的learning rate，$\epsilon$是训练时初始化的learning rate，这两个参数也是需要调整的，</p>
<script type="math/tex; mode=display">
\phi\leftarrow\phi-\eta\Delta_\phi L(\phi)\\
\hat\theta=\phi-\eta\Delta_\phi l(\phi)</script><p>loss function可以是cross entropy，也可以是regression，这和我们训练任务里面的测试资料（query set）有关；得到loss function后，我们就可以计算出$\frac{\partial l(\hat\theta)}{\partial\hat\theta_j}$；</p>
<p>现在的重点是对后面一项$\frac{\partial \hat\theta_j}{\partial\phi_i}$的计算，梯度更新的公式是$\hat\theta=\phi-\epsilon\Delta_\phi l(\phi)$，我们先只考虑其中的第j个维度，</p>
<script type="math/tex; mode=display">
\hat\theta_j=\phi_j-\epsilon\frac{\partial l(\phi)}{\partial\phi_j}</script><p>对于$\frac{\partial \hat\theta_j}{\partial\phi_i}$，如果$i\not=j$，那么</p>
<script type="math/tex; mode=display">
\frac{\partial \hat\theta_j}{\partial \phi_i}=\frac{\partial}{\partial \phi_i}\left(\phi_j-\epsilon\frac{\partial l(\phi)}{\partial\phi_j}\right)=-\epsilon\frac{\partial l(\phi)}{\partial\phi_j\partial\phi_j}</script><p>如果$i=j$，那么</p>
<script type="math/tex; mode=display">
\frac{\partial \hat\theta_j}{\partial \phi_i}=1-\epsilon\frac{\partial l(\phi)}{\partial\phi_j\partial\phi_j}</script><p>但要做这个二次微分$\frac{\partial l(\phi)}{\partial\phi_j\partial\phi_j}$是非常花时间的，在MAML的原始paper里面，作者提出的想法是不算这个二次微分，直接把这个二次微分看成0，那么我们就得到了，</p>
<script type="math/tex; mode=display">
{\rm if}\ i\not=j,\quad\frac{\partial \hat\theta_j}{\partial \phi_i}\approx0\\
{\rm if}\ i=j,\quad\frac{\partial \hat\theta_j}{\partial \phi_i}\approx1</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200722101854356.png" alt="image-20200722101854356" style="zoom:67%;"></p>
<p>那么现在我们就可以只考虑i和j相等的情况，把原来求梯度的式子可以进行化简，</p>
<script type="math/tex; mode=display">
\frac{\partial l(\hat\theta)}{\partial\phi_i}=\sum_j\frac{\partial l(\hat\theta)}{\partial\hat\theta_j}\frac{\partial \hat\theta_j}{\partial\phi_i}\approx\frac{\partial l(\hat\theta)}{\partial\hat\theta_i}</script><p>可进一步得出，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200722173809536.png" alt="image-20200722173809536" style="zoom:67%;"></p>
<p>再带入求$\Delta_\phi L(\phi)$，就不再是用$\phi$求偏微分，而是直接用$\hat\theta$来求，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200722174054716.png" alt="image-20200722174054716" style="zoom:67%;"></p>
<h5 id="Real-Implementation"><a href="#Real-Implementation" class="headerlink" title="Real Implementation"></a>Real Implementation</h5><p>下面我们将简要叙述MAML的具体实施过程。（$\phi_i$是一个初始参数，这个参数会影响最终训练出来的模型参数$\hat\theta$，也就影响了模型参数中的每个参数$\hat\theta_1,\hat\theta_2,…,\hat\theta_j,…$）</p>
<p>首先需要将初始参数$\phi$进行初始化，设其初始值为$\phi^0$；现在从training task中smaple一个<span style="color: green">task m</span>，MAML只更新一次参数，参数更新为$\hat\theta^m$；</p>
<p>为了更新参数$\phi^0$，我们还需要再更新一次$\hat\theta^m$的参数，$\phi^0$更新的方向和第二次参数更新方向是一致的，由于learning rate的差异，这两者的大小会不太一样，$\phi^0$就更新成了$\phi^1$；</p>
<p>现在从training task中sample出一个<span style="color: yellow">task n</span>，初始值是$\phi^1$，更新一次参数后就变成了$\hat\theta^n$；为了更新参数$\phi^1$，我们还需要再更新一次$\hat\theta^n$的参数，$\phi^1$更新的方向和第二次参数更新的方向是一致的，只是learning rate的大小不一样，$\phi^1$就更新成了$\phi^2$；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200722174809854.png" alt="image-20200722174809854" style="zoom:67%;"></p>
<p>如果我们使用model pre-training来更新初始参数，首先初始化参数$\phi^0$，从training task中sample出一个task m，再计算一个偏微分$\frac{\partial l(\hat\theta^m)}{\partial \phi_0}$，偏微分的方向如图中绿色箭头所示；那么$\phi^0$更新到$\phi^1$的方向和这个偏微分的方向是一致的；$\phi^1$到$\phi^2$也有类似的更新过程。</p>
<p><strong>总结</strong>：model pre-training初始值更新的方向，与$\phi$在每个task上算出来的gradient方向一致，即与$\frac{\partial l(\hat\theta^m)}{\partial \phi}$的方向是一致的；而MAMR则是要进行两次模型参数的更新，走两次gradient，初始值更新的方向和第二次gradient的方向是一致的。</p>
<h5 id="Application：translation"><a href="#Application：translation" class="headerlink" title="Application：translation"></a>Application：translation</h5><p>可以把MAML用到machine translation上面。</p>
<p>在下图中，数据集包括18个training task，2个validation tasks，分别表示18种、2种不同的语言翻译成英文；Romanian翻译成英文这个task，是在validation里面的；Finish表示不再training task里，也不在validation task里面，是一个测试任务。MetaNMT表示meta learning，MultiNMT表示Model pre-training；</p>
<p>可以发现meta learning的曲线始终在model pre-training上方，效果更好。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200722183337336.png" alt="image-20200722183337336" style="zoom:67%;"></p>
<h4 id="Reptile"><a href="#Reptile" class="headerlink" title="Reptile"></a><strong>Reptile</strong></h4><p>首先需要sample一个task m出来，再把参数初始化为$\phi^0$，使用$\phi^0$来训练这个模型，经过多次的参数更新后（不像MAML只更新一次），得到更新后的参数$\hat\theta^m$；再来观察$\phi^0,\hat\theta^m$之间的差距，从$\phi^0$到$\hat\theta^m$要走哪个方向，这个方向就是$\phi^0$到$\phi^1$要走的方向；</p>
<p>得到更新后的参数$\phi^1$后，就可以准备下一次参数的更新；首先sample一个task n出来，用初始化的参数$\phi^1$来训练这个模型，经过多次的参数更新，得到$\hat\theta^n$，$\phi^1$到$\phi^2$的方向也就是$\phi^1$到$\hat\theta^n$的方向</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200722213133176.png" alt="image-20200722213133176" style="zoom:67%;"></p>
<p>我们再来讨论一下reptile、MAML、model pre-training这三者的差别；</p>
<p>在下图中，有一个初始化的参数$\phi$，我们需要用这个参数来对模型进行训练，第一次参数更新的方向是$g_1$，第二次参数更新的方向是$g_2$；</p>
<p>如果是pre-training，参数$\phi$的更新方向是$g_1$的方向；如果是MAML，对应的方向则是$g_2$的方向；如果是Reptile，对应的方向则是$g_1+g_2$，是pre-training和MAML方向的和；</p>
<p>由于这里参数只进行了两次更新，如果更新更多次，Reptile的方向就不会是pre-training和MAML的和，也可以学习到更多pre-training和MAML学习不到的东西；</p>
<p>下图表示实验结果，准确率越高，模型效果越好；可以发现用pre-training的效果是最差的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200722215313291.png" alt="image-20200722215313291" style="zoom:67%;"></p>
<h4 id="More-…"><a href="#More-…" class="headerlink" title="More …"></a><strong>More …</strong></h4><p>前文讲的MAML和reptile都只针对初始化的参数，我们也可以用其他方法学习出网络的architecture和activation function，或者学习出如何调整learning rate的规则；要用network来生成network，显然是没办法进行微分的，这时就需要用到reinforce learning，来训练一个可以生成network的network。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200722220321573.png" alt="image-20200722220321573" style="zoom:67%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Meta Learning</tag>
        <tag>MAML</tag>
        <tag>Reptile</tag>
        <tag>Omniglot</tag>
      </tags>
  </entry>
  <entry>
    <title>Network Compression</title>
    <url>/2020/06/27/network-conpression/</url>
    <content><![CDATA[<h4 id="Why-？"><a href="#Why-？" class="headerlink" title="Why ？"></a>Why ？</h4><p>在未来我们可能需要model放到model device上面，但这些device上面的资源是有限的，包括存储控价有限和computing power有限</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627102237291.png" alt="image-20200627102237291" style="zoom:50%;"></p>
<h4 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627102418338.png" alt="image-20200627102418338" style="zoom:50%;"></p>
<h4 id="Network-Pruning"><a href="#Network-Pruning" class="headerlink" title="Network Pruning"></a>Network Pruning</h4><h5 id="Network-can-be-pruned"><a href="#Network-can-be-pruned" class="headerlink" title="Network can be pruned"></a>Network can be pruned</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627102602673.png" alt="image-20200627102602673" style="zoom:50%;"></p>
<h5 id="Network-Pruning-1"><a href="#Network-Pruning-1" class="headerlink" title="Network Pruning"></a>Network Pruning</h5><p>对于训练好的network，我们要判断其weight和neural的重要性：</p>
<ul>
<li>如果某个weight接近于0，那么我们可以认为这个neural是不那么重要的，是可以pruning的；如果是某个很正或很负的值，该weight就被认为对该network很重要；</li>
<li>如果某个neural在给定的dataset下的输出都是0，那么我们就可以认为该neural是不那么重要的</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627102810722.png" alt="image-20200627102810722" style="zoom:50%;"></p>
<p>在评估出weight和neural的重要性后，再进行排序，来移除一些不那么重要的weight和neural，这样network就会变得smaller，但network的精确度也会随之降低，因此还需要进行fine-tuning</p>
<p>最好是每次都进行小部分的remove，再进行fine-tuing，如果一次性remove很多，network的精确度也不会再恢复</p>
<h5 id="Why-Pruning"><a href="#Why-Pruning" class="headerlink" title="Why Pruning?"></a>Why Pruning?</h5><p>Q：为什么不直接train一个小的network呢？</p>
<p>A：小的network比较难train，<a href="https://www.youtube.com/watch?v=_VuWvQUMQVk" target="_blank" rel="noopener">大的network更容易optimize</a></p>
<p><strong>Lottery Ticket Hypothesis</strong></p>
<p>我们先对一个network进行初始化（<span style="color: red">红色的weight</span>），再得到训练好的network（<span style="color: purple">紫色的weight</span>），再进行pruned，得到一个pruned network</p>
<ul>
<li><p>如果我们使用pruned network的结构，再进行random init（<span style="color: green">绿色的weight</span>），会发现这个network不能train下去</p>
</li>
<li><p>如果我们使用pruned network的结构，再使用original random init（<span style="color: red">红色的weight</span>），会发现network可以得到很好的结果</p>
</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627104519698.png" alt="image-20200627104519698" style="zoom:50%;"></p>
<p>作者就说train这个network就像买大乐透一样，有的random可以tranin起来，有的不可以</p>
<p><strong>Rethinking the Value of Network Pruning</strong></p>
<p>Scratch-E/B表示使用real random initialization，并不是使用original random initialization，也可以得到比fine-tuing之后更好的结果</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627105605249.png" alt="image-20200627105605249" style="zoom:50%;"></p>
<h5 id="Pratical-Issue"><a href="#Pratical-Issue" class="headerlink" title="Pratical Issue"></a>Pratical Issue</h5><p>如果我们现在进行weight pruning，进行weight pruning之后的network会变得不规则，有些neural有2个weight，有些neural有4个weight，这样的network是不好implement出来的；</p>
<p>GPU对矩阵运算进行加速，但现在我们的weight是不规则的，并不能使用GPU加速；</p>
<p>实做的方法是将pruning的weight写成0，仍然在做矩阵运算，仍然可以使用GPU进行加速；但这样也会带来一个新的问题，我们并没有将这些weight给pruning掉，只是将它写成0了而已</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627110043391.png" alt="image-20200627110043391" style="zoom:50%;"></p>
<p>实际上做weight pruning是很麻烦的，通常我们都进行neuron pruning，可以更好地进行implement，也很容易进行speedup</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627111357372.png" alt="image-20200627111357372" style="zoom:50%;"></p>
<h4 id="Knowledge-Distillation"><a href="#Knowledge-Distillation" class="headerlink" title="Knowledge Distillation"></a>Knowledge Distillation</h4><h5 id="Student-and-Teacher"><a href="#Student-and-Teacher" class="headerlink" title="Student and Teacher"></a>Student and Teacher</h5><p>我们可以使用一个small network（student）来学习teacher net的输出分布（1:0.7…），并计算两者之间的cross-entropy，使其最小化，从而可以使两者的输出分布相近</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627111854934.png" alt="image-20200627111854934" style="zoom:50%;"></p>
<p>Q：那么我们为什么要让student跟着teacher去学习呢？</p>
<p>A：teacher提供了比label data更丰富的资料，比如teacher net不仅给出了输入图片和1很像的结果，还说明了1和7长得很像，1和9长得很像；因此，student跟着teacher net学习，是可以得到更多的information的</p>
<h5 id="Ensemble"><a href="#Ensemble" class="headerlink" title="Ensemble"></a>Ensemble</h5><p>在kaggle上打比赛，很多人的做法是将多个model进行ensemble，通常可以得到更好的精度。但在实际生活中，设备往往放不下这么多的model，这时我们就可以使用Knowledge Distillation的思想，使用student net来对teacher进行学习，在实际的应用中，我们只需要student net的model就好</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627112630920.png" alt="image-20200627112630920" style="zoom:50%;"></p>
<h5 id="Temperature"><a href="#Temperature" class="headerlink" title="Temperature"></a>Temperature</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627113311508.png" alt="image-20200627113311508" style="zoom:50%;"></p>
<h4 id="Parameter-Quantization"><a href="#Parameter-Quantization" class="headerlink" title="Parameter Quantization"></a>Parameter Quantization</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627114647806.png" alt="image-20200627114647806" style="zoom:50%;"></p>
<h4 id="Architecture-Design（most）"><a href="#Architecture-Design（most）" class="headerlink" title="Architecture Design（most）"></a>Architecture Design（most）</h4><h5 id="Low-rank-approximation"><a href="#Low-rank-approximation" class="headerlink" title="Low rank approximation"></a>Low rank approximation</h5><p>中间插入一个linear层，大小为K，那么也可以减少需要训练的参数</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627115631787.png" alt="image-20200627115631787" style="zoom:50%;"></p>
<h5 id="Review-Standard-CNN"><a href="#Review-Standard-CNN" class="headerlink" title="Review: Standard CNN"></a><strong>Review: Standard CNN</strong></h5><p>每个filter要处理所有的channel</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627120134937.png" alt="image-20200627120134937" style="zoom:50%;"></p>
<h5 id="Depthwise-Separable-Convolution"><a href="#Depthwise-Separable-Convolution" class="headerlink" title="Depthwise Separable Convolution"></a><strong>Depthwise Separable Convolution</strong></h5><p>每个filter只处理一个channel，不同channel之间不会相互影响</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627120350081.png" alt="image-20200627120350081" style="zoom:50%;"></p>
<p>和一般的convolution是一样的，有4个filter，就有4个不同的matrix</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627120519412.png" alt="image-20200627120519412" style="zoom:50%;"></p>
<p>第一步用到的参数量为$3\times3\times2=18$，第二步用到的参数量为$2\times4=8$，一共有26个参数</p>
<h5 id="Standard-CNN-vs-Depthwise-Separable-Convolution"><a href="#Standard-CNN-vs-Depthwise-Separable-Convolution" class="headerlink" title="Standard CNN vs Depthwise Separable Convolution"></a>Standard CNN vs <strong>Depthwise Separable Convolution</strong></h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627121053539.png" alt="image-20200627121053539" style="zoom:50%;"></p>
<p>对于普通的卷积，需要的参数量为$(k\times k\times I)\times O$；对于Depthwise Separable Convolution，需要的参数量为$k\times k\times I+I\times O$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627121419582.png" alt="image-20200627121419582" style="zoom:50%;"></p>
<h4 id="Dynamic-Computation"><a href="#Dynamic-Computation" class="headerlink" title="Dynamic Computation"></a>Dynamic Computation</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627122127490.png" alt="image-20200627122127490" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627122144833.png" alt="image-20200627122144833" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>network compression</tag>
      </tags>
  </entry>
  <entry>
    <title>Semi-supervised</title>
    <url>/2020/06/10/semi-supervised/</url>
    <content><![CDATA[<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610195909860.png" alt="image-20200610195909860" style="zoom:50%;"></p>
<p>对于猫狗分类问题，如果只有一部分data有label，还有其他很大一部分data是unlabeled，那么我们可以认为unlabeled data对我们网络的训练是无用的吗？</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610200040869.png" alt="image-20200610200040869" style="zoom:50%;"></p>
<p>Q：Why semi-supervised learning helps ?</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610200322008.png" alt="image-20200610200322008" style="zoom:50%;"></p>
<p>A：如图所示，图中灰色圆点表示unlabeled data，其他圆点表示labeled data。如果没有unlabeled data，此时可以用一条竖直的线将猫狗进行分类，boundary为竖直的那条线；但unlabeled data的分布也可以告诉我们一些信息，对我们的训练也是有帮助的，有了unlabeled data，此时的boundary为斜直线</p>
<h4 id="Semi-supervised-Learning-for-Generative-Model"><a href="#Semi-supervised-Learning-for-Generative-Model" class="headerlink" title="Semi-supervised Learning for Generative Model"></a>Semi-supervised Learning for Generative Model</h4><h5 id="Intuitive"><a href="#Intuitive" class="headerlink" title="Intuitive"></a>Intuitive</h5><p>不考虑unlabeled data，只有labeled data</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610201326618.png" alt="image-20200610201326618" style="zoom:50%;"></p>
<p>如果把unlabeled data也考虑进来，此时的boundary 也发生了变化</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610201826012.png" alt="image-20200610201826012" style="zoom:50%;"></p>
<h5 id="Formulation"><a href="#Formulation" class="headerlink" title="Formulation"></a>Formulation</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610204004390.png" alt="image-20200610204004390" style="zoom:50%;"></p>
<p>不同的maximum likelihood对比</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610204526467.png" alt="image-20200610204526467" style="zoom:50%;"></p>
<h4 id="Low-density-Separation-Assumption"><a href="#Low-density-Separation-Assumption" class="headerlink" title="Low-density Separation Assumption"></a>Low-density Separation Assumption</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610214224919.png" alt="image-20200610214224919" style="zoom:50%;"></p>
<h5 id="Self-training"><a href="#Self-training" class="headerlink" title="Self-training"></a>Self-training</h5><p>有labeled data和unlabeled data，重复以下过程：</p>
<ul>
<li>从labeled data中tarin了模型$f^*$；</li>
<li>将$f^*$应用到unlabeled data，得到带label的数据，称为Pseudo-label</li>
<li>从unlabeled data中移出这部分data，并加入labeled data；要移除哪部分data，要根据具体的限制条件而定</li>
<li>有了更多的label data，就可以继续训练我们的模型，返回第一步</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610205046125.png" alt="image-20200610205046125" style="zoom:50%;"></p>
<p>Q：这种训练方式对regression 有用吗 ？</p>
<p>W：不能，regression输出的是一个真实的值</p>
<p><strong>hard label vs soft label</strong></p>
<p>self-training用的是hard label；generative model用的是soft label</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610210239919.png" alt="image-20200610210239919" style="zoom:50%;"></p>
<h5 id="Entropy-based-Regularization"><a href="#Entropy-based-Regularization" class="headerlink" title="Entropy-based Regularization"></a>Entropy-based Regularization</h5><p>如果输出的每个类别的概率是相近的，那么这个模型就比较bad；输出的类别差距很大，比如某个类别的概率为1，其他都是0；我们可以用$E(y^u)$来衡量</p>
<script type="math/tex; mode=display">
E(y^u)=-\sum_{m=1}^5y_m^uln(y_m^u)</script><p>对于第一个和第二个distribution，那么$E(y^u)=0$；</p>
<p>对于第三个distribution，那么$E(y^u)=-ln(\frac{1}{5})=ln5$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610211113227.png" alt="image-20200610211113227" style="zoom:50%;"></p>
<p>那么我们现在就可以重新设计loss function，用cross entropy来估计$y^r,\hat y ^r$之间的差距，即$C(y^r,\hat y^r)$，使用labeled data，还加上了一个regularization term</p>
<script type="math/tex; mode=display">
L=\sum_{x^T}C(y^r,\hat y^r)+\lambda \sum_{x^u} E(y^u)</script><h5 id="Outlook-Semi-supervised-SVM"><a href="#Outlook-Semi-supervised-SVM" class="headerlink" title="Outlook: Semi-supervised SVM"></a>Outlook: Semi-supervised SVM</h5><p>对于unlabeled data，如果是SVM 二分类问题，可以把所有的unlabeled data都穷举为Class1或Class2，列举出所有可能的方案，再找出对应的boundary，计算loss，可以发现下图中黑色方框图具有最小的loss</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610213305033.png" alt="image-20200610213305033" style="zoom:50%;"></p>
<h4 id="Smoothness-Assumption"><a href="#Smoothness-Assumption" class="headerlink" title="Smoothness Assumption"></a>Smoothness Assumption</h4><h5 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610214254778.png" alt="image-20200610214254778" style="zoom:50%;"></p>
<p>假设：如果x是similar的，那么他们的y也是一样的</p>
<p>这样的假设是非常不精确的，下面我们做出一个更加精确的假设：</p>
<ul>
<li>x是分布不均匀的，有的地方很密集，有的地方很稀疏</li>
<li>$x^1,x^2$中间有个high density region，那么label $y^1,y^2$就很可能是一样的；但$x^2,x^3$中间没有high density region，其label相同的概率就非常小</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610214546977.png" alt="image-20200610214546977" style="zoom:50%;"></p>
<p>对于下图中的数字，2之间是有过渡形态的，所以这两个图片是similar的；而2与3之间没有过渡形态，因此是不similar的</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610215523269.png" alt="image-20200610215523269"></p>
<p>比较直观的做法是先进行cluster，再进行label</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610221020376.png" alt="image-20200610221020376" style="zoom:50%;"></p>
<h5 id="Graph-based-Approach"><a href="#Graph-based-Approach" class="headerlink" title="Graph-based Approach"></a>Graph-based Approach</h5><p>那么我们到底要怎么才能知道$x^1,x^2$到底在high density region是不是close呢 ？</p>
<p>我们可以把data point用图来表示，图的表示有时是比较nature，有时需要我们自己找出来point之间的联系</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610221342226.png" alt="image-20200610221342226" style="zoom:50%;"></p>
<p> <strong>Graph Construction</strong></p>
<p>首先定义不同point之间的相似度$s(x^i,x^j)$，可以通过以下两个算法来添加edge：</p>
<ul>
<li>KNN，对于图中红色的圆点，与其最相近的三个（k=3）neighbor相连接</li>
<li>e-Neighborhood，对于周围的neighbor，只有和他相似度大于1的才会被连接起来</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610221803745.png" alt="image-20200610221803745" style="zoom:50%;"></p>
<p>edge并不是只有相连和不相连两种选择而已，也可以给edge一些weight，让这个weight和这两个point之间的相似度成正比</p>
<p>labeled data会影响他的邻居，如果这个point是class1，那么他周围的某些point也可能是class1</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610224513039.png" alt="image-20200610224513039" style="zoom:50%;"></p>
<p><strong>Definition</strong></p>
<p>对于下图中的两幅图，如果从直观上看，我们可以认为左边的图更smooth</p>
<p>现在我们用数字来定量描述，S的定义如下</p>
<script type="math/tex; mode=display">
S=\frac{1}{2}\sum_{i,j}w_{i,j}(y^i-y^j)^2</script><p>根据公式我们可以算出左图的S=0.5，右图的S=3，值越小越smooth，越小越好</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610224714665.png" alt="image-20200610224714665" style="zoom:50%;"></p>
<p>对原来的S进行改造一下，$S=y^TLy$</p>
<p>其中$L=D-W$，W为权重矩阵，D表示将weight每行的和放到对角线的位置</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610225336692.png" alt="image-20200610225336692" style="zoom:50%;"></p>
<p>loss function其中一项就包括cross entropy计算的loss；smoothness的量S，前面再乘上一个可以调整的参数$\lambda$ ，$\lambda S$就表示一个regularization term</p>
<p>网络的整体目标是使loss function 取得最小值，即cross entropy项和smoothness都必须要达到最小值，和其他的网络一样，计算相应的gradient，做gradient descent即可</p>
<p>如果要计算smoothness不一定非要在output的地方，也可以是其他位置，比如hidden layer拿出来进行一些transform，或者直接拿hidden layer，都可以计算smoothness</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610225851786.png" alt="image-20200610225851786" style="zoom:50%;"></p>
<h4 id="Better-Representation"><a href="#Better-Representation" class="headerlink" title="Better Representation"></a>Better Representation</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610230634932.png" alt="image-20200610230634932" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>semi-supervised</tag>
        <tag>smoothness</tag>
      </tags>
  </entry>
  <entry>
    <title>Sequence to Sequence</title>
    <url>/2020/06/28/seq2seq/</url>
    <content><![CDATA[<h4 id="Generation"><a href="#Generation" class="headerlink" title="Generation"></a>Generation</h4><p>可以generation文字，也可以generation图片</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627143309760.png" alt="image-20200627143309760" style="zoom:50%;"></p>
<p>如果想生成一张图片，我们可以把图片的每个pixel看成一个character，从而组成一个sentence，比如下图中蓝色pixel就表示blue，红色的pixel表示red,……</p>
<p>接下来就可以用language model来生成图片，最初时间步的输入是特殊字符BOS</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627143546864.png" alt="image-20200627143546864" style="zoom:50%;"></p>
<p>对于一般的generation，如下图的右上部分所示，先根据蓝色pixel生成红色pixel，再根据红色pixel生成黄色pixel，再根据黄色pixel生成灰色pixel，….，并没有考虑pixel之间的位置关系</p>
<p>还有另外一个比较理想的生成图像方法，如果考虑了pixel之间的位置关系，如下图的右下部分所示，黑色pixel由附近的红色和灰色pixel生成</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627144748287.png" alt="image-20200627144748287" style="zoom:50%;"></p>
<p>在上图的左上角部分，该lstm块输入了三组参数，也输出了三组参数，把这些方块叠起来就可以达到中部位置图像的效果</p>
<p>对于一个$3\times 3$的画布，就是我们想要生成的图像的大小，一开始画布上还没有任何内容；现在在画布的左下角放入一个convolution的filter， 把其输出在放到3D的lstm的左下角（input），由于lstm也是有memory的，经过不断的process，可以到左上角，从而产生蓝色的pixel；再把蓝色的pixel放到画布的左下角</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627150215465.png" alt="image-20200627150215465" style="zoom:50%;"></p>
<p>把filter向右移，结合上次蓝色pixel的输出，根据上文的步骤，就可以生成红色的pixel，再把红色pixel放到画布对应的位置</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627150953989.png" alt="image-20200627150953989" style="zoom:50%;"></p>
<h4 id="Conditional-Generation"><a href="#Conditional-Generation" class="headerlink" title="Conditional Generation"></a>Conditional Generation</h4><p>必须是有条件的generation，要联系上下文</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627152018372.png" alt="image-20200627152018372" style="zoom:50%;"></p>
<p>如果要对一张图片进行解释，我们先使用一个CNN model将image转化为一个vector（红色方框），先生成了第一个单词”A”，在生成第二个单词时，还需要将整个image的vector作为输入，不然RNN可能会忘记image的一些信息</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627152512575.png" alt="image-20200627152512575" style="zoom:50%;"></p>
<p>RNN也可以用来做机器翻译，比如我们想要翻译“机器学习”，就先把这四个字分别输入绿色的RNN里面，最后一个时间节点的输出（红色方框）就包含了整个sentence的information，这个过程称之为<strong>Encoder</strong></p>
<p>把encoder的information再作为另外一个rnn的input，再进行output，这个过程称之为<strong>Decoder</strong></p>
<p>encoder和decoder是jointly train的，这两者的参数可以是一样的，也可以是不一样的</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627153002771.png" alt="image-20200627153002771" style="zoom:50%;"></p>
<p> encoder将之前所有说过的话都进行整合了，再作为decoder的input</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627154111622.png" alt="image-20200627154111622" style="zoom:50%;"></p>
<h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><h5 id="Dynamic-Conditional-Generation"><a href="#Dynamic-Conditional-Generation" class="headerlink" title="Dynamic Conditional Generation"></a>Dynamic Conditional Generation</h5><p>如果需要翻译的文字非常复杂，无法用一个vector来表示，就算可以表示也没办法表示全部的关键信息，这时候如果decoder每次input的都还是同一个vector，就不会得到很好的结果</p>
<p>因此我们现在先把“机器”作为decoder的第一个输入$c^1$，这样就可以得到更好的结果</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627154318421.png" alt="image-20200627154318421" style="zoom:50%;"></p>
<h5 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h5><p>绿色方块为RNN中hidden layer的output，分别是$h^1,h^2,h^3,h^4$；还有一个vector $z^0$，是可以通过network学出来的；把这两者输入一个match函数，可以得到$h^1,z^0$之间的match分数 $\alpha _0^1$</p>
<p>这个match函数可以自己设计，可以有参数，也可以没有参数</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627154905302.png" alt="image-20200627154905302" style="zoom:50%;"></p>
<p>得出的$\alpha _0^1,\alpha _0^2,\alpha _0^3,\alpha _0^4$再输入softmax函数，得到$\hat\alpha _0^1,\hat\alpha _0^2,\hat\alpha _0^3,\hat\alpha _0^4$，乘上$h^i$再加起来，就可以得到$c^0$，来作为decoder input，根据decoder，可得出第一个翻译的单词“machine”；$z^1$为decoder RNN其中hidden layer的输出，用$z^1$再来得出相应的decoder input</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627155414554.png" alt="image-20200627155414554" style="zoom:50%;"></p>
<p>再来计算$z^1,h^1$之间的match分数$\alpha_1^1$，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627155937349.png" alt="image-20200627155937349" style="zoom:50%;"></p>
<p>再继续算出$c^1$作为decoder的input，从而得出第二个单词“learning”</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627160046413.png" alt="image-20200627160046413" style="zoom:50%;"></p>
<p>一直重复这个过程，只到全部翻译完成</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627160411647.png" alt="image-20200627160411647" style="zoom:50%;"></p>
<h5 id="Speech-Recognition"><a href="#Speech-Recognition" class="headerlink" title="Speech Recognition"></a>Speech Recognition</h5><p>语音信号也可以看成一系列的vector sequence，第一个红色方框所在的帧match分数很高，就把对应的vector放到decoder，machine就会得出“h”</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627193908934.png" alt="image-20200627193908934" style="zoom:50%;"></p>
<h5 id="Image-Caption-Generation"><a href="#Image-Caption-Generation" class="headerlink" title="Image Caption Generation"></a>Image Caption Generation</h5><p>image可以分成多个region，每个region可以用一个vector来表示，计算这些vector和$z^0$之间match的分数，为0.7、0.1….，再进行weighted sum，得到红色方框的结果，再输入RNN，得到Word1，此时hidden layer的输出为$z^1$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627195250525.png" alt="image-20200627195250525" style="zoom:50%;"></p>
<p>再计算$z^1$和vector之间的match分数，把分数作为RNN的input，从而得出Word 2</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627195511422.png" alt="image-20200627195511422" style="zoom:50%;"></p>
<p>这里是一些具体的例子</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627195914955.png" alt="image-20200627195914955" style="zoom:50%;"></p>
<p>但也会产生一些不好的结果</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627200010931.png" alt="image-20200627200010931" style="zoom:50%;"></p>
<h5 id="Memory-Network"><a href="#Memory-Network" class="headerlink" title="Memory Network"></a>Memory Network</h5><p>memory network是有一篇文章，问machine一个问题，machine能够给出对应的答案</p>
<p>先将document分成多个vector，再给出这些vector与问题q之间match的分数$\alpha^1,..,\alpha^N$，与$x^i$分别相乘，得到extracted information，作为DNN的input，再进行不断的训练，从而得出对应的answer</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627200439344.png" alt="image-20200627200439344" style="zoom:50%;"></p>
<p>还有一个更加复杂的版本</p>
<p>match的部分和extracted的部分不一定是一样的，相当于输入了两组不同的参数，比如可以把$x^i$乘上一个matrix，就可以变成另外一组参数$h^i$，这个matrix是可以自动学出来的</p>
<p>这时计算extracted information的方式就发生了一些变化；sum之后的值不仅输入DNN，还会与问题q进行结合，再重新计算match分数，不断进行一个反复思考的过程</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627201014065.png" alt="image-20200627201014065" style="zoom:50%;"></p>
<p>q可以用来计算attention，计算出extracted information之后，与q加起来；上一次sum的结果会再继续参与计算attention，再extract，再进行sum，作为DNN的input，得到最后的答案</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627201933880.png" alt="image-20200627201933880" style="zoom:50%;"></p>
<p>可以把这个模型看作是两层layer的network，也有back propagation来更新network的参数</p>
<h5 id="Neural-Turing-Machine"><a href="#Neural-Turing-Machine" class="headerlink" title="Neural Turing Machine"></a>Neural Turing Machine</h5><p>Neural Turing Machine不仅可以读取memory的内容，也可以根据match分数来修改memory的内容；即你不仅可以通过memory读取information，也可以把information写到memory里面去</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627202652334.png" alt="image-20200627202652334" style="zoom:50%;"></p>
<p>对于初始值的memory $m_0^i$，计算出对应的match分数$\hat\alpha_0^i$，在进行weighted sum，得出$r^0$，来作为另外一个network f的input，这个network相当于是一个controller，f可以是DNN、SVM、GRU等；</p>
<p>$r^0,x^1$作为该network的输入，可以得出三个output，这些output可以控memory，比如新的attention是什么，新的memory里面的值如何进行修改</p>
<p>新的attention计算加入了$k^1$，即$\alpha_1^i=cos(m_0^i,k^1)$，再输入softmax，得到新的$\hat\alpha_1^i$的distribution</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627203211682.png" alt="image-20200627203211682" style="zoom:50%;"></p>
<p>$e^1$里面的值表示把原来memory里面的值清空，$a^1$表示把新的值写入memory，通过下面的公式再计算新的memory $m_1^i$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627204251989.png" alt="image-20200627204251989" style="zoom:50%;"></p>
<p>得到新的memory $m_1^i$之后，再计算出新的attention $\hat\alpha_1^i$，再进行weighted sum得到$r^1$，加上新的$x^2$作为network的input，得出新的output，再来对整个network进行操控</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627204747750.png" alt="image-20200627204747750" style="zoom:50%;"></p>
<h4 id="Tips-for-Generation"><a href="#Tips-for-Generation" class="headerlink" title="Tips for Generation"></a>Tips for Generation</h4><h5 id="Attention-1"><a href="#Attention-1" class="headerlink" title="Attention"></a>Attention</h5><p>$\alpha_t^i$下标表示time，上标表示component，该视频包括4个component，有4个时间节点；在第一个时间节点产生attention $\alpha_1^1,\alpha_1^2,\alpha_1^3,\alpha_1^4$，生成了第一个word $w_1$</p>
<p>attention也是可以调节的，有时会出现一些<strong>bad attention</strong>，在产生第二个word（$w_2$,<span style="color: red">women</span>）时，focus到第二个component，在产生第四个word $w_4$时，也focus到第二个component上，也是<span style="color: green">women</span>，多次attent在同一个frame上，就会产生一些很奇怪的结果</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627205536622.png" alt="image-20200627205536622" style="zoom:50%;"></p>
<p><strong>Good Attention</strong>：至少要包含input的每一个frame，每个frame都应该attent一下，每个frame进行attent的量也不能太多，这个量最好是同等级的</p>
<p>Q：那么如何保证这个标准呢？</p>
<p>A：有学者提出了一个regularization term，有一个新的可学习的参数$\tau$，$\sum_t\alpha_t^i$表示所有attention的sum，再计算$\sum_i(\tau-\sum_t\alpha_t^i)^2$的值，这个值越小越好；对于上文所说的bad attention的情况，这个值算出来是很大的，因此network就会再进行不断地学习，只到term的值不断减小</p>
<h5 id="Mismatch-between-Train-and-Test"><a href="#Mismatch-between-Train-and-Test" class="headerlink" title="Mismatch between Train and Test"></a>Mismatch between Train and Test</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627213016984.png" alt="image-20200627213016984" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627213436202.png" alt="image-20200627213436202" style="zoom:50%;"></p>
<p><strong>Exposure Bias</strong>：在training的时候，input为真正的答案；但在testing的时候，input为上一个时间节点的output</p>
<p>在下图中，由于RNN从来没有到右子树B训练过，但如果在testing时一开始就遇到了B，network就会出现错误</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627214936987.png" alt="image-20200627214936987" style="zoom:50%;"></p>
<h5 id="Modifying-Training-Process"><a href="#Modifying-Training-Process" class="headerlink" title="Modifying Training Process?"></a>Modifying Training Process?</h5><p>那么我们如何解决这种mismatch问题呢？可以尝试modify训练process</p>
<p>如果machine现在output B，即使是错误的output（和reference A不一样），我们也应该让这个错误的output作为下一次的input，那么training和testing就是match的</p>
<p>但在实际操作中，training是非常麻烦的；现在我们使用gradient descent来进行training，第一个gradient的方向告诉我们要把A的几率增大，output B，在第二个时间点，input为B，看到reference为B，把B的几率增大，就可以和reference相对应起来；</p>
<p>但实际上，第一个output为A的几率上升，那么output发生了变化，第二个时间点的input就发生了变化，是A；那么我们之前学习到的让B上升就没有意义了</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627215358053.png" alt="image-20200627215358053" style="zoom:50%;"></p>
<h5 id="Scheduled-Sampling"><a href="#Scheduled-Sampling" class="headerlink" title="Scheduled Sampling"></a><strong>Scheduled Sampling</strong></h5><p>由于使用Modifying Training Process很难train，现在我们就使用Scheduled Sampling</p>
<p>对于到底是model里的ouput，还是reference来作为input，我们可以给一个几率；铜板是正面，就使用model的output，如果是反面，就用reference</p>
<p>右上角的图，纵轴表示from reference的几率，一开始只看reference，reference的几率不断变小，model的几率不断增加</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627221402859.png" alt="image-20200627221402859" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627221933432.png" alt="image-20200627221933432" style="zoom:50%;"></p>
<h5 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627221956837.png" alt="image-20200627221956837" style="zoom:50%;"></p>
<p>同时走两条path</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627222144464.png" alt="image-20200627222144464" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627222358014.png" alt="image-20200627222358014" style="zoom:50%;"></p>
<h5 id="Better-idea？"><a href="#Better-idea？" class="headerlink" title="Better idea？"></a>Better idea？</h5><p>为什么不直接把distribution作为下一个input呢？即下图中的右图</p>
<p>但右边的结果会比较差，对于几率接近的输入，左图会sample一个几率最高的（高兴），但右图会保留这个distribution</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627222713634.png" alt="image-20200627222713634" style="zoom:50%;"></p>
<h5 id="Object-level-v-s-Component-level"><a href="#Object-level-v-s-Component-level" class="headerlink" title="Object level v.s. Component level"></a>Object level v.s. Component level</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627223140253.png" alt="image-20200627223140253" style="zoom:50%;"></p>
<p>应该是考虑整个sentence的准确率如何，而不应该只考虑单个的word</p>
<h5 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning?"></a>Reinforcement learning?</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627223424266.png" alt="image-20200627223424266" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627223531560.png" alt="image-20200627223531560" style="zoom:50%;"></p>
<h4 id="Pointer-Network"><a href="#Pointer-Network" class="headerlink" title="Pointer Network"></a>Pointer Network</h4><p>定义如下，找到其中的几个point，将剩下的point包围起来</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628095239086.png" alt="image-20200628095239086" style="zoom:50%;"></p>
<h5 id="Seq2seq"><a href="#Seq2seq" class="headerlink" title="Seq2seq"></a>Seq2seq</h5><p>input为4个point，但output只有三个point ($P_1,P_2,P_4$)，那这个问题还是sequence to sequence吗？</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628095644699.png" alt="image-20200628095644699" style="zoom:50%;"></p>
<p>由于output的个数是提前设定好的，并不能动态改变；<span style="color: red">在training的时候，output很只有50个point，但在testing的时候，output有100个point，由于之前只学习了50个point，因此现在并不能生成51-100之间的点</span>；因此这种做法是不可取的</p>
<p>这里我们对network进行了改进</p>
<p>对于输入的5个point（加入了END），先得到RNN中hidden layer的输出（$h^0,h^1,h^2,h^3,h^4,h^5$），计算attention weight（match score），再把这个distribution输入argmax函数，输出概率最大的point，即 1</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628100108506.png" alt="image-20200628100108506" style="zoom:50%;"></p>
<p>把这个output 1再输入另外一个RNN network，其hidden layer的输出为$z^1$，再根据$z^1,h^i$来计算相应的attention weight，输入argmax函数，得到output为 4</p>
<p>再把point 4作为另外一个RNN network的input，其hidden layer的输出为$z^2$，….</p>
<p>这个process会一直持续，知道END出现</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628101004928.png" alt="image-20200628101004928" style="zoom:50%;"></p>
<p><span style="color: red">对于改进后的network，如果现在的input是100个，那么其output就会有100个不同的选择，没有了之前的限制</span></p>
<h5 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h5><p>对于机器翻译，如果input包含一些地名、人名，并不需要进行翻译，这是我们就可以用pointer network，将这些名词直接copy</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628102849708.png" alt="image-20200628102849708" style="zoom:50%;"></p>
<p>对于chat-bot，也有一些名词并不需要学习，可以直接copy</p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Sequence to Sequence</tag>
      </tags>
  </entry>
  <entry>
    <title>Sequence Labeling Problem</title>
    <url>/2020/07/20/sequence-labeling/</url>
    <content><![CDATA[<p>本文主要介绍了几种传统的序列标注方法，包括HMM（隐马尔可夫模型），CRF（条件随机场），Structed perception/SVM。</p>
<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><h5 id="Sequence-Labeling"><a href="#Sequence-Labeling" class="headerlink" title="Sequence Labeling"></a>Sequence Labeling</h5><p>首先叙述sequence labeling的基本概念，machine的任务就是学习出一个函数f，可以实现sequence X到Y的转换。</p>
<p>现在我们假设X和Y的长度是一样的，输入$x_1,…,x_L$，输出为$y_1,..,y_L$，这个任务其使用RNN就可以完成，本文将叙述另外一种方法来进行sequence labeling。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200718154925173.png" alt="image-20200718154925173" style="zoom: 50%;"></p>
<h5 id="Example-Task"><a href="#Example-Task" class="headerlink" title="Example Task"></a>Example Task</h5><p>本文会使用POS tagging来当作讲解的例子，我们要对一个句子中的每个word进行标注，词性的类别有很多，有名词、动词、副词等</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200718162236242.png" alt="image-20200718162236242" style="zoom:50%;"></p>
<p>如果现在有一段话“John saw the saw.”，需要进行词性标注，那么“John”的词性就是是proper none，第一个“saw”是verb，第二个“saw”就是Noun；</p>
<p>pos tagging是sequence labeling中比较basic、也比较重要的一个task，是很多文字处理程序的基石，比如必须先进行pos tagging，才可以再进一步做文本分析。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200718162011348.png" alt="image-20200718162011348" style="zoom:67%;"></p>
<p>那么pos tagging任务是不是用一本词典就可以完成了呢？</p>
<p>答案是不能，pos tagging任务还需要考虑word在句子中的顺序，才可以得出正确的词性，比如“John saw the saw.”这句话中，第一个“saw”是verb，第二个“saw”就是Noun；因此我们只考查词典是不能完成任务的，还需要完全理解整个句子的语义才可以得出正确的结果。</p>
<h4 id="Hidden-Markov-Model-HMM"><a href="#Hidden-Markov-Model-HMM" class="headerlink" title="Hidden Markov Model (HMM)"></a>Hidden Markov Model (HMM)</h4><h5 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h5><p>如果使用HMM的思想来生成一个sequence，首先会生成一个POS sequence，再根据这个pos sequence来生成对应的词汇，从而生成最后需要的那个句子。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200718164901203.png" alt="image-20200718164901203" style="zoom:67%;"></p>
<p>Step1: 在pos sequence中的第一个词汇，有0.5的几率是det，0.4的几率是propnoun，0.1的几率是verb；现在我们就可以进行random的sample，假设sample到了propnoun；下一个词有0.8的几率是verb，假设就sample到了verb；下一个词很可能就结束了</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719100147183.png" alt="image-20200719100147183" style="zoom:67%;"></p>
<p>Step 2: 现在我们知道了pos sequence，就可以来进行填词，把每个词性对应的词汇找出来，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719102547829.png" alt="image-20200719102547829" style="zoom:67%;"></p>
<p>HMM就是在描述，使用这个pos sequence来生成这个句子的几率，怎么说出这句话来。我们用$P(x,y)$表示这个word sequence x和pos sequence y一起出现的几率，$P(y)$表示这个pos sequence出现的几率，$P(x|y)$表示在y这个条件下，生成这个word sequence的概率。</p>
<p>对于下图中的例子，$P(y)$表示PN放在句首的位置，PN后面接V的几率，V接D的几率，D接N的几率；$P(x|y)$就表示PN产生John的几率，V产生saw的几率，D产生the的几率，N产生saw的几率。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719102935252.png" alt="image-20200719102935252" style="zoom:67%;"></p>
<p>这里是HMM的进一步叙述，step 1和2分别计算transition 和emission probability</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719104130678.png" alt="image-20200719104130678" style="zoom:67%;"></p>
<h5 id="Estimating-the-probabilities"><a href="#Estimating-the-probabilities" class="headerlink" title="Estimating the probabilities"></a>Estimating the probabilities</h5><p>那么我们要怎么来计算这些概率呢，即PN后面接V，V产生saw的几率？</p>
<p>这些概率都是通过training data得到的。首先要收集一大堆的training data（sentence），这些sentence中的每个词我们都找专家标好了对应的词性，有了这些标记好的training data，我们就可以来计算这些概率了。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719104448417.png" alt="image-20200719104448417" style="zoom:67%;"></p>
<p>现在如果要计算这个tag是s，s的下一个tag是$s’$的概率，我们可以先计算出training data中s后面是s’的次数，再计算s出现的次数，即</p>
<script type="math/tex; mode=display">
P(y_{l+1}=s'|y_l=s)=\frac{count(s\rightarrow s')}{count(x)}</script><p>对于给出的tag是s，我们从s的词袋中找出对应的word是t的概率，我们可以计算出training data中tag s后面是word t的次数，再计算s出现的次数，即</p>
<script type="math/tex; mode=display">
P(x_{l}=t|y_l=s)=\frac{count(s\rightarrow t)}{count(x)}</script><p>$P(y_1|start)$表示$y_1$在句首出现的次数，$P(end|y_L)$表示$y_L$在句末出现的次数，我们就可以根据下图中的公式计算pos sequence y和word sequence x一起出现的几率$P(x,y)$，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719110735707.png" alt="image-20200719110735707" style="zoom:67%;"></p>
<h5 id="How-to-do-POS-Tagging"><a href="#How-to-do-POS-Tagging" class="headerlink" title="How to do POS Tagging?"></a>How to do POS Tagging?</h5><p>我们现在可以看到word sequence x，但并不知道pos sequence y，y是隐藏的，需要我们通过某种算法来求解。</p>
<p>我们要找到的y，其实就是使在给定word x的条件下，pos tag是y的概率最大的那个y，即</p>
<script type="math/tex; mode=display">
y=arg\mathop{\rm max}_{y\in Y}P(y|x)=arg\mathop{\rm max}_{y\in Y}\frac{P(x,y)}{P(x)}</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719112157732.png" alt="image-20200719112157732" style="zoom:67%;"></p>
<h5 id="Viterbi-Algorithm"><a href="#Viterbi-Algorithm" class="headerlink" title="Viterbi Algorithm"></a>Viterbi Algorithm</h5><p>由于x是给定的，我们可以只考虑分子部分，那么现在的问题就变成了</p>
<script type="math/tex; mode=display">
\tilde y=arg\mathop{\rm max}_{y\in Y}P(x,y)</script><p>那么现在我们可以对所有可能的y进行穷举，假设pos tag一共有$|S|$个，pos sequence y的长度是$L$，那么一共有$|S|^L$种可能，这个计算量是非常庞大的；</p>
<p>但现在有Viterbi Algorithm，我们可以对这个过程进行简化，可以把复杂度降低到$O(L|S|^2)$。这个算法可以看成是一个function，输入word x，输入可能性最大的那个tag y。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719113206100.png" alt="image-20200719113206100" style="zoom:67%;"></p>
<h5 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h5><p>HMM是一种structed learning的方法，需要回答三个问题，</p>
<p>Problem 1: 找到evaluation function，这里我们找的就是x和y的联合概率分布 $P(x,y)$ ；</p>
<p>Problem 2: 进行inference，找到对应的x，来使evaluation function $P(x,y)$的概率最大；</p>
<p>Problem 3: training，$P(y),P(x|y)$都可以从training data中的统计数据算出。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719114302286.png" alt="image-20200719114302286" style="zoom:67%;"></p>
<h5 id="Drawbacks"><a href="#Drawbacks" class="headerlink" title="Drawbacks"></a>Drawbacks</h5><p>对于HMM算法，要找出使得evaluation function值最大的$\tilde y$。现在假设$\hat y$是正确的tag sequence，我们就必须要要求$P(x,\hat y)&gt;P(x,y)$，才能保证$\hat y$是正确的sequence；但HMM并不能做到这件事，并不能保证错误的y带进去的概率$P(x,y)$，一定小于正确的$\hat y$带进去的概率。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719115754096.png" alt="image-20200719115754096" style="zoom:67%;"></p>
<p>现在举一个例子来说明这件事，有transition probability：P(V|N)=9/10，P(D|N)=1/10，分别表示tag N后面是V、N后面是D的概率；还有emission probability： P(a|V)=1/2，P(a|D)=1 ……，分别表示tag V对应的word是a、D对应的word是a的概率；</p>
<p>那么现在有一个需要解决的问题，现在我们知道在l这个时间点对应的word是a，$y_l$这时最有可能对应的tag是什么呢？</p>
<p>现在我们已经知道了$P(x_l|y_l),P(y_l|y_{l-1})$，把$y_l=V$带进去，我们可以得到tag N之后是V的概率为0.9，V对应的word是a的概率是0.5，$0.9\times 0.5=0.45$；如果我们把$y_l=D$带进去，概率就是$0.1\times 1=0.1$；明显是V对应的概率是最大的，因此我们可以认为这里的$y_l=V$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719120903970.png" alt="image-20200719120903970" style="zoom:80%;"></p>
<p>如果现在有这样的training data，N后面接V，有9个这样的数据，P后面接V，也有9个这样的数据，而N到D只有1个；那么N可以到V，也可以到D，到V的概率就是0.9，到D的概率是0.1；V只能退出a或者c，概率都是0.5；</p>
<p>按照HMM算法，计算概率的大小，$N\rightarrow V\rightarrow a$概率是0.45，而$N\rightarrow D\rightarrow a$概率是0.1，明显中间节点是V的概率更大；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719155658613.png" alt="image-20200719155658613" style="zoom:80%;"></p>
<p>但按照我们自己的分析，training data中明明就直接有一项$N\rightarrow D\rightarrow a$，但HMM却没有选择D，明显不符合常理。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719160753910.png" alt="image-20200719160753910" style="zoom:80%;"></p>
<p>这里我们对HMM进行总结：</p>
<ul>
<li><span style="color: red">缺点：如果有一个pair (x,y)从来没有在training data中出现过，hmm还是很可能给这个pair一个很高的几率；</span></li>
<li>优点：hmm在training data很少时比其他的方法performance要好。</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719161039381.png" alt="image-20200719161039381" style="zoom:67%;"></p>
<p>之所以HMM会出现这个问题，是因为hmm的transition和emission probability是<strong>分开计算</strong>的；我们可以用一个更复杂的模型，CRF来解决这个问题。</p>
<h4 id="Conditional-Random-Field-CRF"><a href="#Conditional-Random-Field-CRF" class="headerlink" title="Conditional Random Field (CRF)"></a>Conditional Random Field (CRF)</h4><p>我们可以把$P(x,y)$是正比于$exp(w\cdot \phi (x,y))$的，设一个比例系数R，可以看作是，</p>
<script type="math/tex; mode=display">
P(x,y)=\frac{exp(w\cdot \phi (x,y))}{R}</script><p>在CRF中，我们选择$P(y|x)$作为evaluation function</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719163106979.png" alt="image-20200719163106979" style="zoom:67%;"></p>
<h5 id="P-x-y-for-CRF"><a href="#P-x-y-for-CRF" class="headerlink" title="P(x,y) for CRF"></a>P(x,y) for CRF</h5><p>把hmm的evaluation function进行log，可得到多个式子相加的形式</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719165805703.png" alt="image-20200719165805703" style="zoom:67%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719170157895.png" alt="image-20200719170157895" style="zoom:67%;"></p>
<p>其中$N_{s,t}(x,y)$表示在$(x,y)$这个pair中，tag s和word t出现的次数。$(x,y)$表示专家标记之后的一个sentence。</p>
<p>下面讲一个具体的例子。</p>
<p>$logP(the|D)$出现了两次，即$N_{D,the}(x,y)=2$,</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719170732186.png" alt="image-20200719170732186" style="zoom:67%;"></p>
<p>$logP(x,y)$中的其他项也进行类似的处理,</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719171416399.png" alt="image-20200719171416399" style="zoom:67%;"></p>
<p>把这些展开的式子再代入原式子，就可以看作是两个vector进行dot product</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719171541932.png" alt="image-20200719171541932" style="zoom:67%;"></p>
<p>对于w这个矩阵，其中每个维度的元素和原来的hmm都有着某种联系。比如$w_{s,t}=logP(x_i=t|y_i=s)$，表示在tag为s的条件下，取word为t的概率大小；如果对这个式子再取exp，那么就可以得到$P(x_i=t|y_i=s)=e^{w_{s,t}}$；</p>
<p>但直接写$P(x,y)=exp(w\cdot \phi (x,y))$会有一些小问题，w矩阵里面的元素是可正可负的，如果是负的，exp之后就小于1，可以解释为概率值；如果是正的，exp之后就大于1，由于概率值都是小于1的，就不能解释为概率值；因此，这里$P(x,y),exp(w\cdot \phi (x,y))$是一个成正比的关系。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719172033172.png" alt="image-20200719172033172" style="zoom:67%;"></p>
<h5 id="Feature-Vector"><a href="#Feature-Vector" class="headerlink" title="Feature Vector"></a>Feature Vector</h5><p> $\phi (x,y)$就是feature vector，由两部分组成，</p>
<p><strong>Part 1</strong>: tag和word之间的关系，比如在(x,y)这个pair中，“the”被标记为D两次，因此value对应2；“dog”被标记为N一次，value的值就对应为1；没出现过的value就是0；</p>
<p>如果有$|S|$个可能的tags，$|L|$个可能的words，Part 1就有$|S|\times |L|$个dimensions。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719203144844.png" alt="image-20200719203144844" style="zoom:67%;"></p>
<p><strong>Part 2</strong>: tag之间的关系，比如在(x,y)这个pair中，tag D后面接N的次数为2，即$N_{D,N}(x,y)=2$，但tag D后面接D的次数是0，因此$N_{D,D}(x,y)=0$。</p>
<p>如果有$|S|$个可能的tags，$|L|$个可能的words，Part 2就有$|S|\times |S|+2|S|$个dimensions，S个tag和其他的S个tag都有关系，start和end和S个tag也有关系</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719203811164.png" alt="image-20200719203811164" style="zoom:67%;"></p>
<h5 id="Training-Criterion"><a href="#Training-Criterion" class="headerlink" title="Training Criterion"></a>Training Criterion</h5><p>$P(y|x)$表示在已知的word x上，生成pos sequence y的概率。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719205358911.png" alt="image-20200719205358911" style="zoom:67%;"></p>
<h5 id="Gradient-Ascent"><a href="#Gradient-Ascent" class="headerlink" title="Gradient Ascent"></a>Gradient Ascent</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719221245972.png" style="zoom:67%;"></p>
<h5 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h5><p>首先求对应的梯度</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719221342908.png" style="zoom:67%;"></p>
<p>在求出来的梯度中，$N_{s,t}(x^n,\hat y^n)$表示word t和tag s在$(x^n,\hat y^n)$中出现的次数；后面一项表示对于任意的y‘，都需要计算给出的word $x^n$的tag是y’的概率 $P(y’|x^n)$，还需要计算word t和tag s在$(x^n,y’)$中出现的次数 $N_{s,t}(x^n,y’)$ ；</p>
<p>我们再来叙述下梯度中两个式子所表示的物理含义，</p>
<ul>
<li>第一项，如果s，t在$(x^n,\hat y^n)$出现的次数越多，就增加w；</li>
<li><p>第二项，如果s，t在$(x^n,y’)$出现的次数越多，并不是在正确答案中出现的次数多，那么就减小w。</p>
</li>
<li><p>如果s，t在正确答案中出现次数多，就减小w，任意一个y’出现的次数也多的话，就增加w，这两项可以实现一个<strong>tradeoff</strong>。</p>
</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719221517202.png" alt="image-20200719221517202" style="zoom:67%;"></p>
<p>但这里要计算所有可能的y‘，硬算的话计算量会很大，可以用Viterbi algorithm算法来减小计算量。</p>
<p>下图为vector的表示，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719222851553.png" alt="image-20200719222851553" style="zoom:67%;"></p>
<h5 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h5><p>求出w后，就可以开始进行inference，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719223144083.png" alt="image-20200719223144083" style="zoom:67%;"></p>
<h5 id="CRF-v-s-HMM"><a href="#CRF-v-s-HMM" class="headerlink" title="CRF v.s. HMM"></a>CRF v.s. HMM</h5><p>CRF不仅增加$P(x,\hat y)$，还会减小$P(x,y’)$，但HMM并没有减小这个操作；</p>
<p>我们想要得到的结果是$P(x,\hat y)&gt;P(x,y)$，HMM并不能实现，而CRF却能够实现；这是由于CRF会在学习过程不断调整参数，比如把原来的$P(a|V)=1/2$调整为0.1，使出现正确答案的概率最大。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719223315109.png" alt="image-20200719223315109" style="zoom:67%;"></p>
<h5 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719224753879.png" style="zoom:67%;"></p>
<h4 id="Structured-Perceptron-SVM"><a href="#Structured-Perceptron-SVM" class="headerlink" title="Structured Perceptron/SVM"></a>Structured Perceptron/SVM</h4><h5 id="Structured-Perceptron"><a href="#Structured-Perceptron" class="headerlink" title="Structured Perceptron"></a>Structured Perceptron</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719225001941.png" alt="image-20200719225001941" style="zoom:67%;"></p>
<h5 id="Structured-Perceptron-v-s-CRF"><a href="#Structured-Perceptron-v-s-CRF" class="headerlink" title="Structured Perceptron v.s. CRF"></a>Structured Perceptron v.s. CRF</h5><p>Structured Perceptron只减掉某一个y（几率最大的那个），CRF则是减去所有的y‘</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719225047285.png" alt="image-20200719225047285" style="zoom:67%;"></p>
<h5 id="Structured-SVM"><a href="#Structured-SVM" class="headerlink" title="Structured SVM"></a>Structured SVM</h5><p>还额外考虑error</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719225440619.png" alt="image-20200719225440619" style="zoom:67%;"></p>
<h5 id="Error-Function"><a href="#Error-Function" class="headerlink" title="Error Function"></a>Error Function</h5><p>error function表示$\hat y^n$和y之间的差距，problem 2.1除了考虑dot product的值最大，还需要考虑error function的值，也需要越大越好</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719225619209.png" alt="image-20200719225619209" style="zoom:67%;"></p>
<p>上图中还有一个计算error function的example，在$\hat y,y$之间有三个tag不一样，sequence的长度为10，因此error function的值为3/10</p>
<h4 id="Concluding-Remarks"><a href="#Concluding-Remarks" class="headerlink" title="Concluding Remarks"></a><strong>Concluding Remarks</strong></h4><p>这里是几种算法performance的比较，最好的是structed svm</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719230003644.png" alt="image-20200719230003644" style="zoom:67%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200719225927570.png" alt="image-20200719225927570" style="zoom:67%;"></p>
<h5 id="How-about-RNN？"><a href="#How-about-RNN？" class="headerlink" title="How about RNN？"></a>How about RNN？</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720092212672.png" alt="image-20200720092212672" style="zoom:67%;"></p>
<h5 id="Integrated-together"><a href="#Integrated-together" class="headerlink" title="Integrated together"></a>Integrated together</h5><p>我们可以把RNN、LSTM和HMM、crf的思想结合起来，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720092346614.png" style="zoom:67%;"></p>
<p> 比如语音识别领域，我们可以用RNN的结果来表示$P(x_l|y_l)$ </p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720092639541.png" alt="image-20200720092639541" style="zoom:67%;"></p>
<p>在semantic tagging，把input vector输入这个双向的RNN，得到新的feature，再抽取出新的$(x,y)$，</p>
<p> <img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720103225145.png" alt="image-20200720103225145" style="zoom:67%;"></p>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p>[1] 视频地址：<a href="https://www.bilibili.com/video/BV1zJ411575b" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1zJ411575b</a></p>
<p>[2] 课程地址，<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html" target="_blank" rel="noopener">http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html</a></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Sequence Labeling</tag>
        <tag>HMM</tag>
        <tag>CRF</tag>
      </tags>
  </entry>
  <entry>
    <title>T-distributed Stochastic Neighbor Embedding (t-SNE)</title>
    <url>/2020/06/28/t-SNE/</url>
    <content><![CDATA[<blockquote>
<p>本文主要叙述了t-SNE，即T-distributed Stochastic Neighbor Embedding ；先介绍了LLE的主要思想，再总结了它的缺点，从而引出t-SNE；</p>
</blockquote>
<h4 id="Manifold-Learning"><a href="#Manifold-Learning" class="headerlink" title="Manifold Learning"></a>Manifold Learning</h4><p>在高维空间里，距离该点很远的点很可能与这个点也是有关联的，因此我们可以把3-D的空间进行降维，那么我们就可以更方便地进行clustering或unsupervised learning 任务</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628202133893.png" alt="image-20200628202133893" style="zoom:50%;"></p>
<h4 id="Locally-Linear-Embedding-LLE"><a href="#Locally-Linear-Embedding-LLE" class="headerlink" title="Locally Linear Embedding (LLE)"></a>Locally Linear Embedding (LLE)</h4><p>用$w_{ij}$表示$x_i,x_j$之间的联系，先找到使得$\sum_i ||x^i-\sum_jw_{ij}x^j||_2$最小化的$w_{ij}$，再根据这个$w_{ij}$来找到降维的结果$z_i,z_j$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628202738023.png" alt="image-20200628202738023" style="zoom:50%;"></p>
<p>如果并不知道之前的$x_i,x_j$，那么就可以用LLE这种方法，也可以得出$z_i,z_j$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628203501209.png" alt="image-20200628203501209" style="zoom:50%;"></p>
<h4 id="Laplacian-Eigenmaps"><a href="#Laplacian-Eigenmaps" class="headerlink" title="Laplacian Eigenmaps"></a>Laplacian Eigenmaps</h4><p>Review: 在之前的semi-supervised learning中，如果$x^1,x^2$在一个high density region内是相近的，那么我们就可以认为$\hat y^1,\hat y^2$也有类似的表现</p>
<p>如果$y^i,y^j$是connected的，那么其$w_{ij}$就是对应的相似度；如果没有connect，其$w_{ij}$就是0</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628204015777.png" alt="image-20200628204015777" style="zoom:50%;"></p>
<p>我们也可以得出类似smoothness的式子，计算$z^i,z^j$之间的smoothness</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628205136836.png" alt="image-20200628205136836" style="zoom:50%;"></p>
<p>那么我们现在的目标就是找到$z^i,z^j$，来使S达到最小值，还需要有一些额外的constrains</p>
<p>现在我们对z加入一些constrains，如果降维后z的维数是M，那么我们就不希望取出来的这些点还生活在比M还低维的空间里面；我们现在希望把塞进高维空间的低维空间展开，我们就不希望展开之后的点在一个更低维的空间里面</p>
<h4 id="T-distributed-Stochastic-Neighbor-Embedding-t-SNE"><a href="#T-distributed-Stochastic-Neighbor-Embedding-t-SNE" class="headerlink" title="T-distributed Stochastic Neighbor Embedding (t-SNE)"></a>T-distributed Stochastic Neighbor Embedding (t-SNE)</h4><p>对于之前的LLE方法，类似的data之间是很close的，<u>但不同类别之间的data却没有分开，是叠成一团的</u></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628210651621.png" alt="image-20200628210651621" style="zoom:50%;"></p>
<p><strong>为了找到对应的$z^i,z^j$</strong>，先计算$x^i,x^j$之间的相似度$S(x^i,x^j)$，再得出一个分布 $P(x^j|x^i)$；还要计算$z^i,z^j$之间的相似度$S’(z^i,z^j)$，得出分布$Q(z^j|z^i)$；</p>
<p>这两个分布应该越接近越好，使用L来表示</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628211215304.png" alt="image-20200628211215304" style="zoom:50%;"></p>
<p>可以使用gradient descent，有了L函数，再分别对$z^i,z^j$求偏微分即可</p>
<p>但t-SNE要对所有的point之间都求similarity，因此计算量比较大，在数据量很大的情况下电脑的计算速度会非常慢</p>
<p>因此，对于很高的dimensions，通常先做降维（PCA），比如可以降维到50维，再使用t-SNE降到2维</p>
<p><span style="color: red">通常我们使用t-SNE来对高维的数据进行可视化</span></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628211326492.png" alt="image-20200628211326492" style="zoom:50%;"></p>
<p>在上图中，红色曲线表示SNE，蓝色曲线表示t-SNE，纵轴表示distribution；如果我们想要维持相同的distribution，即在同一个水平线上，就达到了如图所示的效果；相同的几率，t-SNE的$||z^i-z^j||_2$之间的距离越大</p>
<p>如果本来就离得很近，那么经过t-SNE之间的距离还是很小；如果本来就离得很远，那么从原来的distribution拉到t-SNE之后，距离会更远；</p>
<p>到实际的例子中，<span style="color: red">如果本来是同一个类别的data，由于这些data之间的距离很近，不会收到t-SNE很大的影响；但如果是属于不同类别的data，距离是比较远的，t-SNE会放大这种距离</span></p>
<p>对于下图中的MNIST，先使用PCA进行降维，再进行可视化，就可以得到下图中的good visualization</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628213646582.png" alt="image-20200628213646582" style="zoom:50%;"></p>
<p>下图中有一个更加直观的例子，使用t-SNE算法，运用gradient descent的思想，不同类别的data之间的距离会越来越大</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628214657615.png" alt="image-20200628214657615" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628214721028.png" alt="image-20200628214721028" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Unsupervised Learning</tag>
        <tag>Dimension Reduction</tag>
        <tag>LLE</tag>
        <tag>t-SNE</tag>
      </tags>
  </entry>
  <entry>
    <title>Transfer Learning</title>
    <url>/2020/07/20/transfer-learning/</url>
    <content><![CDATA[<blockquote>
<p>本文主要针对source和target data的不同情况，介绍了几种主要的transfer learning的方式，包括Fine-tuning、Multitask learning、domain-adversarial training、zero-shot learning。</p>
</blockquote>
<p>如果现在没有和我们任务直接相关的数据，比如我们要做的任务是猫狗分类，很可能手头只有elephant和tiger的数据集，是不同标签的；也有可能是动漫的cat/dog数据，和真实数据集的domain不一样。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720142155947.png" alt="image-20200720142155947" style="zoom:67%;"></p>
<p>那么这些不相关的数据，会对我们的任务是有帮助的吗？</p>
<h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><p>现在有一些data是和我们的任务有关的，是target data，有一些data是和我们的任务无关的，是source data；target data和source data可能是有label的，也可能是没有label的，我们也将分为这四种情况来进行讨论。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720180422641.png" alt="image-20200720180422641" style="zoom:67%;"></p>
<p>如果target data和source data都是有label的，那么我们就可以进行model fine-tuning。</p>
<h4 id="Model-Fine-tuning"><a href="#Model-Fine-tuning" class="headerlink" title="Model Fine-tuning"></a>Model Fine-tuning</h4><p>现在我们只有很小一部分target data $(x^t,y^t)$，但却有很多的source data $(x^s,y^s)$；如果target data只有少量的几个example，那么我们就可以称作是<strong>one-shot learning</strong>。</p>
<p>我们可以针对这样的task举一个例子，speaker adaption，任务是辨识某一个人的声音，但只有这个人的少量audio data，但我们有很多其他人的audio data；</p>
<p>为了解决这个task，我们可以先使用source data来训练一个model，然后再用target data来进行fine-tuning；即把source data训练的model当成一个初始值，再用我们source data来进行训练任务。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720143147319.png" alt="image-20200720143147319" style="zoom:67%;"></p>
<p>但对这个模型进行训练的时候我们必须非常小心，由于target data的数量太少了，必须要注意overfitting问题。</p>
<p>我们在训练过程中加入一些小技巧，conservative training、layer transfer。</p>
<h5 id="Conservative-Training"><a href="#Conservative-Training" class="headerlink" title="Conservative Training"></a>Conservative Training</h5><p>现在有大量的source data，即很多其他人的audio data，用这些data来训练一个model；把这个训练好的model来作为另外一个model的初始值，再用target data来进行训练；</p>
<p>为了防止overfitting，我们需要加上一些constrain。在看到同一个data的时候，我们希望这两个model的output越接近越好；或者这两个model的参数之间的差距也要越小越好，可以计算这两者之间的L2-norm。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720143243580.png" alt="image-20200720143243580" style="zoom:67%;"></p>
<h5 id="Layer-Transfer"><a href="#Layer-Transfer" class="headerlink" title="Layer Transfer"></a>Layer Transfer</h5><p>首先用source data训练好一个model，把这个model中的其中几个layer的参数拿出来，到新的model里面去；</p>
<p>再用target data来训练没有复制过去的layer（rest layer），target data只需要训练非常少的参数，因此就可以避免overfitting；</p>
<p>如果target data足够多，也可以直接直接对整个network进行fine-tuning。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720153717937.png" alt="image-20200720153717937" style="zoom:67%;"></p>
<p>对于不同的task，需要copy的layer也是不同的。比如Image相关的任务，通常都是copy前几层的参数，因为前几层都是在做一些比较基础的工作，类似于有没有直线，有没有简单的几何图形。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720154835368.png" alt="image-20200720154835368" style="zoom:67%;"></p>
<p>下面是Image使用Layer transfer进行的实验，source data包括500个类别，target data包括也包括500个其他的类别，数据来自imagenet；下图中横轴表示copy的层数，纵轴表示copy这几个layer，再用target data来进行训练所能达到的准确度accuracy；</p>
<p>红色的曲线（only train the rest layer）：首先用source data训练好model，copy其中的某几个layer之后，只用target data训练剩下没copy的layer；随着copy的层数不断增多，可以从图中看到训练的准确度是在不断下降的；只有copy前面几个layer，performance稍微有些进步；</p>
<p>橙色的曲线（fine-tune the whole network）：现在我们不只是用target data来训练剩下的layer，而是对整个network进行fine-tuning，可以发现随着copy的layer数量越多，模型准确度越高；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720155321103.png" alt="image-20200720155321103" style="zoom:67%;"></p>
<p>下图中红色的曲线和上图中红色曲线相对应，只训练剩下的layer，纵轴表示source和target data之间的相关程度。从图中我们可以看出，如果source和target data差别很大，在做transfer learning的时候performance会下降很多；但如果只copy 前几个layer，performance就掉得不会太多。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720161214747.png" alt="image-20200720161214747" style="zoom:67%;"></p>
<h4 id="Multitask-Learning"><a href="#Multitask-Learning" class="headerlink" title="Multitask Learning"></a>Multitask Learning</h4><h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>在进行fine-tunign时，我们更多关心的是target对应的准确率，并不太关心source的准确率；而multitask learning则是两者的准确率都考虑进来了。</p>
<p>有两个不同的task A，B，这两个task所使用的input feature都是一样的，前几层的网络结构也是一样的；但在之后的某个层就分叉了，Task A会用到部分output，Task B会用到另外一部分的output；</p>
<p>这样做的其中一个好处就是，我们会用task A和B的data一起来对前几个layer进行训练，这几个layer很可能有更好的performance；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720162244439.png" alt="image-20200720162244439" style="zoom:67%;"></p>
<p>即使这两个task的前几个input层的layer都不一样，只有中间某些layer是可以共用的，我们也可以使用transfer learning的思想。</p>
<h5 id="Multilingual-Speech-Recognition"><a href="#Multilingual-Speech-Recognition" class="headerlink" title="Multilingual Speech Recognition"></a>Multilingual Speech Recognition</h5><p>Multitask Learning有一个很成功的应用，即Multilingual Speech Recognition（多语音辨识）；</p>
<p>现在我们有一堆多种语言的语音数据，我们要对这些audio进行语音辨识。我们可以训练一个model，来同时对这五种语言进行辨识，这个model的前面几个layer都是共用参数的，只有后面几个layer的参数不一样。因为都是人类说的语言，所以前几层可以共用参数。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720163332515.png" alt="image-20200720163332515" style="zoom:67%;"></p>
<h4 id="Domain-adversarial-training"><a href="#Domain-adversarial-training" class="headerlink" title="Domain-adversarial training"></a>Domain-adversarial training</h4><p>如果现在target data是unlabeled，而source data是labeled，我们可以通过Domain-adversarial training来进行transfer learning。</p>
<p>在下图中，source data是带label的，是来自MNIST的数据，$(x^s,y^s)$是training data；target data是没有label的，来自MNIST-M，$(x^t)$是testing data；这两者是mismatch的；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720180600188.png" alt="image-20200720180600188" style="zoom:67%;"></p>
<p>那么我们怎么让在MNIST上训练出来的model，应用到MNIST-M（背景五颜六色）上呢？</p>
<p>如果直接把MNIST-M中的某个图像x输入训练好的network，然后输出对应的classification，效果会非常不好；我们可以把这整个network的前几层看作是在extract feature，后面几层看作是在进行classification；</p>
<p>在下图中，我们把extract出来的feature做一个可视化的展示，如果把MNIST的数据输入进去，即图中蓝色的点，会发现有10个cluster，classifier可以根据这些feature进行分类；但如果把MNIST-M输入network，会变成图中红色的点，是没有规则的，classifier并不能根据这些feature进行分类。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720181241730.png" alt="image-20200720181241730" style="zoom:67%;"></p>
<p>针对这个问题，我们可以对网络结构进行改进；</p>
<p>我们可以通过network中的feature extractor，即Domain-adversarial training，把domain的这些特性给消掉。不同的domain不应该像上图那样分成两群，而应该被混在一起；</p>
<p>为了训练这个feature extractor，我们可以把提取出来的feature输入一个classifier，提取出来的feature要想办法骗过classifier；如果把extractor看作是generator，那么classifier就可以看作是discriminator，和起来就可以看作是GAN；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720182043251.png" alt="image-20200720182043251" style="zoom:67%;"></p>
<p>但骗过这个classifier太简单了，feature只要全都是0，就可以骗过这个classifier；</p>
<p>因此这个feature不仅要能骗过classifier，还需要保留原来digit的特性；如果输入label predictor，也需要做的好，要能输出对应的label（0～9）；</p>
<p>这个feature extractor需要满足两个条件，Maximize <strong>label</strong> classification accuracy + minimize <strong>domain</strong> classification accuracy，这也就是Domain-adversarial training中adversarial的由来。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720190801381.png" alt="image-20200720190801381" style="zoom:67%;"></p>
<p>虽然Domain-adversarial training原理很简单，但训练起来还是很麻烦的</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720191620557.png" alt="image-20200720191620557" style="zoom:67%;"></p>
<p>这里就是论文中的实验结果，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720191716307.png" alt="image-20200720191716307" style="zoom:67%;"></p>
<h4 id="Zero-shot-learning"><a href="#Zero-shot-learning" class="headerlink" title="Zero-shot learning"></a>Zero-shot learning</h4><p>在Zero-shot learning里面，training和testing data的task是不一样的；比如source data是要进来猫狗分类的数据，但testing data却只有草泥马，这是在training data中没有的类别</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720192119360.png" alt="image-20200720192119360" style="zoom:67%;"></p>
<p>在语音辨识领域，如果我们并不知道所有可能的source data，我们如何来进行辨识呢？</p>
<p>我们对这个问题进行了转化，不是要辨识一段语音属于哪一个word，而是要辨识语音属于哪一个“音标”，即把辨识的单位变成“音标”。还需要建立表示“音标”和word之间转换关系的表格，当我们辨识出“音标”之后，查表就可以得到对应的word。就算有一段语音没有在training data中出现过，我们也可以先辨识出其对应的“音标”，再得到具体的word。</p>
<p>现在回到图像领域。</p>
<p>如果是在图像领域，我们可以把每个class用attribute来表示，如下图的<span style="color: red"> Database</span>所示，比如dog有毛，是四条腿，有尾巴的，……。这个attribute的数量要足够多，每个class都有其特别的attribute组合，如果出现两个类别的class有相同的attribute组合，这个方法就失效了。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720193359055.png" alt="image-20200720193359055" style="zoom:67%;"></p>
<p>那么现在的任务就变成了，辨识每张image里面的attribute，比如看到dog的图像，就要说这是有毛、有四条腿、有尾巴的动物；</p>
<p>在testing的时候，即使出现一个从来没看过的动物，network只需要辨识这个动物的attribute就可以了，再去查看属性表，看哪一个class和这些attribute最接近。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720194107595.png" alt="image-20200720194107595" style="zoom:67%;"></p>
<p>这个attribute的数量可能会非常多，这时我们就需要做attribute embedding（降维成一个vector，表示成space上的一个点）；首先我们需要把图像的attribute先投影到embedding space上，把表格里面的属性也投影到这个space，分别写作$f(x^i),g(y^i)$，f和g可以看作是一个network；</p>
<p>在training的时候，我们希望$f(x^i),g(y^i)$越接近越好；在testing的时候，即使我们从来没见过这个草泥马，我们也可以把这个草泥马的attribute投影到embedding space上，看哪个class的attribute和这个点最接近</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720194343007.png" alt="image-20200720194343007" style="zoom:67%;"></p>
<p>如果没有这个database（class和attribute的关系），我们可以借助word vector来完成这个任务。即用word vector来表示这个动物，这个vector的某个dimension就表示动物的某个attribute，再把attribute也用对应的word vector表示，再来做embedding</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720195143255.png" alt="image-20200720195143255" style="zoom:67%;"></p>
<p>$x^1,y^1$投影到embedding space之后，他们之间的距离要越接近越好，对应着一个最小化问题；但这样做会造成一个问题，只要把所有的word都投影到一个点上，距离就是0；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720200937712.png" alt="image-20200720200937712" style="zoom: 67%;"></p>
<p>因此要对loss function进行一些变化，我们要使同一个pair之间的距离越小越好，还要使不同pair之间的距离越大越好，即</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720201144961.png" alt="image-20200720201144961" style="zoom:67%;"></p>
<p>如果后面一项小于0，这个loss function的值就是0，把这一项进行化简，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720201233705.png" alt="image-20200720201233705" style="zoom:67%;"></p>
<p>还有一个<strong>更简单</strong>的方法来做zero-shot learning，即<strong>Convex Combination of Semantic Embedding</strong>；<span style="color: red">只需要off-the-shelf NN for ImageNet和一个word vector就可以完成；</span></p>
<ul>
<li>首先把图像输入neural network，但NN可能没办法分辨出到底是哪一个class，lion和tiger的几率都是0.5；</li>
<li>再找出Lion和tiger的word vector，把这两者的vector用1:1的比例进行混合，即0.5V(tiger)+0.5V(lion)，得到混合之后的vector；</li>
<li>观察哪个vector和这个混合之后的vector最为接近，这里是V(liger)。</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720201614381.png" alt="image-20200720201614381" style="zoom:67%;"></p>
<p>这里是这个方法的实验结果，ConSE即是Convex Combination of Semantic Embedding，第一只海狮取得了不错的效果，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720202952205.png" alt="image-20200720202952205" style="zoom:50%;"></p>
<p>还有关于翻译的例子，训练数据只有English到Japanese、Korean等，但没有Japanese到Korean的数据，通过transfer learning的思想，也可以对这两种语言进行翻译。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720203150995.png" alt="image-20200720203150995" style="zoom:67%;"></p>
<h4 id="Self-taught-learning"><a href="#Self-taught-learning" class="headerlink" title="Self-taught learning"></a>Self-taught learning</h4><p>刚才讲了很多source data都有label的情况，现在我们来叙述source data是unlabeled，target data是label的情况，即self-taught learning。</p>
<p>现在我们有足够多的source data，即使是unlabeled，可以学习一个feature extractor，再去target data上抽取feature。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200720203754918.png" alt="image-20200720203754918" style="zoom:67%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Transfer Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>多处理机</title>
    <url>/2020/03/11/%E5%A4%9A%E5%A4%84%E7%90%86%E6%9C%BA/</url>
    <content><![CDATA[<h5 id="计算机分类"><a href="#计算机分类" class="headerlink" title="计算机分类"></a>计算机分类</h5><ul>
<li>按照<u>Flynn分类</u>法，可将计算机分为<u>SISD、SIMD、MISD、MIMD</u>四类。</li>
<li><u>冯氏分类</u>法，<u>字串位串、字串位并、字并位串、字并位并</u>。</li>
<li><u>Handler分类法</u>。</li>
</ul>
<h5 id="存储器系统结构"><a href="#存储器系统结构" class="headerlink" title="存储器系统结构"></a>存储器系统结构</h5><ul>
<li>集中式存储器：各处理器可共享一个集中式的物理存储器；</li>
<li>分布式存储器：存储器分布到各个处理器中。<ul>
<li>如果大多数的访存都是针对本地节点进行的，就可以<u>降低</u>对存储器和互联网络的<u>带宽要求</u>；</li>
<li>对本地存储器的<u>访问延迟小</u>。</li>
<li>缺点：处理器之间的<u>通信</u>较为<u>复杂</u>。</li>
</ul>
</li>
</ul>
<p>存储器在物理上分布到各节点中，但<u>逻辑地址空间</u>的组织方式以及处理器之间<u>通信方式</u>不同：</p>
<ul>
<li>把在物理上分离的存储器作为一个<u>统一的共享逻辑空间</u>进行编址，不同处理器上的同一个物理地址指向同一个存储单元。<u>分布式共享存储器系统（DSM）</u>。采用<u>共享存储器</u>通信机制。</li>
<li>把每个节点中的存储器编址为一个<u>独立的地址空间</u>，不同节点的地址空间之间是相互独立的，不同处理器上的同一个物理地址指向不同的存储单元。<u>机群</u>。采用<u>消息传递</u>机制。</li>
</ul>
<h5 id="通信机制"><a href="#通信机制" class="headerlink" title="通信机制"></a>通信机制</h5><p><strong>共享存储器</strong></p>
<p>处理器之间的通信是通过load和store指令对相同存储器地址进行读/写来实现的。</p>
<ul>
<li>与常用的对称式多处理机的通信机制兼容；</li>
<li>处理机之间的通信方式比较复杂时，编程相对容易；</li>
<li>在开发程序时，可以将重点放到对性能影响较大的数据访问上；</li>
<li>通信数据量小时，通信开销小，贷款利用好；</li>
<li>可以通过Cathe缓冲技术来减少远程通信的频度。</li>
</ul>
<p><strong>消息传递</strong></p>
<p>处理器之间是通过显示地发送消息来进行通信的，把消息看成是一个远程进程调用（RPC）。</p>
<ul>
<li>硬件更简单；</li>
<li>通信是显示的，可以更容易搞清楚何时发生通信以及通信开销是多少；</li>
<li>显示通信可以使编程者重点注意并行计算的主要通信开销，使之有可能开发出结构更好、性能更高的并行程序；</li>
<li>可以减少不当同步带来错误的可能性。</li>
</ul>
<h5 id="并行处理面临的挑战"><a href="#并行处理面临的挑战" class="headerlink" title="并行处理面临的挑战"></a>并行处理面临的挑战</h5><ul>
<li>程序中的并行性有限；</li>
<li>相对较大通信开销。</li>
<li>计算：Amdahl</li>
</ul>
<script type="math/tex; mode=display">
加速比=\frac{1}{\frac{可加速部分比例}{部件加速比}+(1-可加速部分比例)}</script><script type="math/tex; mode=display">
S=\frac{1}{\frac{Fe}{Se}+(1-Fe)}</script><h5 id="并行计算机系统结构（MIMD）"><a href="#并行计算机系统结构（MIMD）" class="headerlink" title="并行计算机系统结构（MIMD）"></a>并行计算机系统结构（MIMD）</h5><ul>
<li>PVP</li>
<li><strong>SMP</strong>，对称式共享存储器多处理机</li>
<li>DSM</li>
<li><strong>MPP</strong>，大规模并行处理机</li>
<li><strong>机群</strong></li>
</ul>
<h5 id="对称式共享存储器多处理机（SMP）"><a href="#对称式共享存储器多处理机（SMP）" class="headerlink" title="对称式共享存储器多处理机（SMP）"></a>对称式共享存储器多处理机（SMP）</h5><p>也称为集中式共享多处理机，一般由十几个处理器构成，各处理器共享一个集中式的物理存储器，这个主存相对于各处理器的关系是对称的。</p>
<ul>
<li>商用处理器</li>
<li>共享存储器</li>
<li>集中共享</li>
</ul>
<h5 id="大规模并行处理机（MPP）"><a href="#大规模并行处理机（MPP）" class="headerlink" title="大规模并行处理机（MPP）"></a>大规模并行处理机（MPP）</h5><p>按照当前的标准，具有几百台~几千台处理机的任何机器都是大规模并行处理系统。</p>
<ul>
<li>处理节点使用商用微处理器，每个节点可以有多个微处理器；</li>
<li>具有较好的可扩展性；</li>
<li>采用分布式非共享的存储器，各节点有自己的地址空间；</li>
<li>采用消息传递的通信机制。</li>
</ul>
<h5 id="机群"><a href="#机群" class="headerlink" title="机群"></a>机群</h5><p>是一种价格低廉、易于构建、扩放性极强的并行计算机系统。它由多台同构或者异构的独立计算机通过高性能网络和局域网连在一起，协同完成特定的并行计算任务。从用户的角度看，它就是一个单一的、集中的计算资源。</p>
<ul>
<li>商用处理器</li>
<li>分布非共享式</li>
<li>消息传递</li>
</ul>
]]></content>
      <categories>
        <category>计算机体系结构</category>
      </categories>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>机群</tag>
        <tag>MPP</tag>
        <tag>SMP</tag>
        <tag>消息传递</tag>
        <tag>共享存储器</tag>
      </tags>
  </entry>
  <entry>
    <title>指令级并行</title>
    <url>/2020/03/11/%E6%8C%87%E4%BB%A4%E7%BA%A7%E5%B9%B6%E8%A1%8C/</url>
    <content><![CDATA[<h5 id="指令级并行"><a href="#指令级并行" class="headerlink" title="指令级并行"></a>指令级并行</h5><p>指令之间存在的并行性，利用它，计算机可以并行执行两条或两条以上的指令。开发ILP的途径有两种：</p>
<ul>
<li>资源重复，重复设置多个处理部件；</li>
<li>采用流水线技术，使指令重叠并行执行。</li>
</ul>
<p>指令的<strong>静态调度</strong>：依靠编译器进行静态调度，以减少相关和冲突。它不是在程序执行的过程中，而是在编译期间进行调度和优化。</p>
<p>指令的<strong>动态调度</strong>：能够在保持程序数据流和异常行为的情况下，通过硬件对指令执行顺序进行重新安排，以提高流水线的利用率且减少停顿现象。是由硬件在程序执行时进行优化的。</p>
<p>基本程序块：一串连续的代码除了入口和出口以外，没有其他的分支指令和转入点。</p>
<p>不精确异常：当执行指令i导致发生异常时，处理机的现场与严格按照程序顺序执行时指令i的现场不同。</p>
<p>精确异常：发生异常时，处理机的现场与严格按照程序顺序执行指令i的现场相同。</p>
<h5 id="相关与冲突"><a href="#相关与冲突" class="headerlink" title="相关与冲突"></a>相关与冲突</h5><ul>
<li>数据相关</li>
<li>名相关，名指指令所访问的寄存器或存储器单元的名称。<ul>
<li>反相关</li>
<li>输出相关</li>
</ul>
</li>
<li>控制相关</li>
</ul>
<p>流水线冲突 指由于相关的存在，使得指令流中的下一条指令不能在指定的时钟周期执行。<u>结构冲突</u>是硬件资源冲突引起的，<u>数据冲突</u>是数据相关和名相关引起的，<u>控制冲突</u>是控制相关引起的。</p>
<p>为了保证程序执行的正确性，必须保证的最关键的两个特性是：</p>
<ul>
<li><u>数据流</u></li>
<li>保持<u>异常行为</u>：无论怎么改变指令的执行顺序，都不能改变程序中异常的发生情况。</li>
</ul>
<h4 id="指令的动态调度"><a href="#指令的动态调度" class="headerlink" title="指令的动态调度"></a>指令的动态调度</h4><p>能够在保持程序数据流和异常行为的情况下，通过硬件对指令执行顺序进行重新安排，以提高流水线的利用率且减少停顿现象。优点：</p>
<ul>
<li>能够处理一些<u>编译时情况不明的相关</u>（比如涉及存储器访问的相关），并简化了编译器；</li>
<li>能够使本来是面向某一流水线优化编译的代码在<u>其他动态调度的流水线上</u>也能执行。</li>
</ul>
<h5 id="记分牌"><a href="#记分牌" class="headerlink" title="记分牌"></a>记分牌</h5><p>把流水线中的译码段ID分解成了两个段：流出和读操作数。记录的信息由三部分组成：</p>
<ul>
<li>指令状态表</li>
<li>功能部件状态表</li>
<li>结果寄存器状态表</li>
</ul>
<h5 id="Tomasulo"><a href="#Tomasulo" class="headerlink" title="Tomasulo"></a>Tomasulo</h5><p>核心思想：（1）记录和检测指令相关，操作数一旦就绪就立即执行，把发生RAW冲突的可能性较小到最低；（2）通过寄存器换名来消除WAR冲突和WAW冲突。寄存器换名是通过保留站来实现，它保存正在流出和等待流出指令所需要的操作数。</p>
<p>基本思想：只要操作数有效，就取其放到保留站中，避免指令流出时才到寄存器取数据，这就使得即将执行的指令从相应的保留站中读取数据，而不用到寄存器中。指令的执行结果也是直接放到等待数据的其它保留站中去。一条指令流出时，存放操作数的寄存器被换名为该寄存器对应的保留站的名称。</p>
<h4 id="动态分支预测"><a href="#动态分支预测" class="headerlink" title="动态分支预测"></a>动态分支预测</h4><p>用硬件动态地进行分支处理的方法。在程序运行的过程中，根据分支指令过去的表现预测将来的行为。如果分支行为发生了变化，预测结果也跟着变化。</p>
<h5 id="分支历史表（BHT）"><a href="#分支历史表（BHT）" class="headerlink" title="分支历史表（BHT）"></a>分支历史表（BHT）</h5><p>用来记录相关指令最近一次或几次的执行情况是成功还是失败，并根据此进行预测。</p>
<h5 id="分支目标缓冲器（BTB）"><a href="#分支目标缓冲器（BTB）" class="headerlink" title="分支目标缓冲器（BTB）"></a>分支目标缓冲器（BTB）</h5><p>是一种动态分支预测技术。将（1）<u>执行过的成功分支指令的目标地址</u>（2）<u>预测的分支目标地址</u> 记录在一张硬件表格中。在每次取指令时，用该指令的地址与BTB表中所有项目的第一个字段进行比较，以便尽早知道指令是否分支成功，尽早知道目标地址，达到减少分支开销的目的。</p>
<p>BHT方法是在<u>译码ID</u>段，就能获得分支目标地址、顺序下一条指令以及预测的结果。BTB方法在<u>取址IF</u>段就能知道这些信息。</p>
<h5 id="基于硬件的前瞻执行"><a href="#基于硬件的前瞻执行" class="headerlink" title="基于硬件的前瞻执行"></a>基于硬件的前瞻执行</h5><p>是解决控制相关的方法，它对分支指令的结果进行猜测，然后按照这个猜测继续取址、流出和执行后续的指令。但指令执行的结果不写回寄存器或存储器，而是放到一个称为ROB（再定序列缓冲器）中。等到相应的指令得到“确认”（即确实是应该执行）后，才将结果写入寄存器或存储器中。</p>
<p>把三种思想结合在了一起：</p>
<ul>
<li><u>动态分支预测</u>。用来选择后续执行的指令。</li>
<li>在控制相关的结果尚未出来之前，<u>前瞻</u>地执行后续指令。</li>
<li>用<u>动态调度</u>对基本块的各种组合进行<u>跨基本块的组合</u>。</li>
</ul>
<h4 id="超标量流水线"><a href="#超标量流水线" class="headerlink" title="超标量流水线"></a>超标量流水线</h4><p>是一种多指令流出技术。它在每个时钟周期流出的指令条数不固定，依代码的具体情况而定，但有个上限。</p>
<h4 id="超长指令字"><a href="#超长指令字" class="headerlink" title="超长指令字"></a>超长指令字</h4><p>是一种多指令流出技术。在每个时钟周期内流出的指令数都是固定的，这些指令构成一条长指令或者一个指令包，在这个指令包中，指令之间的并行性是通过指令显式地表现出来的。这种处理机的指令调度由编译器静态完成。</p>
]]></content>
      <categories>
        <category>计算机体系结构</category>
      </categories>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>指令级并行</tag>
        <tag>指令相关</tag>
        <tag>指令冲突</tag>
        <tag>动态调度</tag>
        <tag>记分牌</tag>
        <tag>Tomasulo</tag>
        <tag>BHT</tag>
        <tag>BTB</tag>
        <tag>ROB</tag>
        <tag>前瞻执行</tag>
        <tag>超标量</tag>
        <tag>VLIW</tag>
        <tag>超长指令字</tag>
      </tags>
  </entry>
  <entry>
    <title>Gradient Descent</title>
    <url>/2020/06/04/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</url>
    <content><![CDATA[<h4 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h4><p>梯度下降的目标是使损失函数L最小化，$\theta^* = arg\ min\ L(\theta)$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603152209444.png" alt="image-20200603152209444" style="zoom:50%; "></p>
<h4 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h4><p>从$\theta_0$开始，先计算出$\theta_0$的梯度，其中红色箭头表示梯度的方向，蓝色箭头表示移动的方向。<br><u>梯度的方向是函数值在这个点增长最快的方向，想要使损失函数L的值达到最小值，就必须要往相反的方向运动。</u></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603152117920.png" alt="image-20200603152117920" style="zoom: 50%;"></p>
<p>学习率会出现以下四种不同的情况：</p>
<ul>
<li>学习率太小，即图中蓝色的线，每次跨越的步长很小很小，梯度每次变化的值也小，模型要达到local minima，就必须需要更多的训练时间；</li>
<li>学习率太大，即图中绿色的线，每次跨越的步长会很大，很可能形成在山谷之间震荡的现象；</li>
<li>学习率特别大，即图中黄色的线，就很可能会直接跳出local minima，loss会越来越大；</li>
<li>学习率刚好合适，即图中红色的线，每次跨越的步长非常合适，达到local minima的时间也不需要特别多。</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603161210664.png" alt="image-20200603161210664" style="zoom:50%;"></p>
<p>由于手动设置learning rate会导致很多问题，就出现了一些自适应的梯度调整方法。</p>
<ul>
<li>刚开始训练时，我们离local minimum的距离还很远，因此可以使用稍大的learning rate；</li>
<li>在经过多次的训练后，离local minimum的距离已经很近了，所以这时可以使用小的learning rate；</li>
<li>在经过t次的训练后，learning rate可以衰减为$\eta^t=\frac{\eta}{\sqrt{t+1}}$</li>
</ul>
<h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><blockquote>
<p>Divide the learning rate of each parameter by the <strong>root mean square of its previous derivatives</strong></p>
</blockquote>
<h5 id="理论推导"><a href="#理论推导" class="headerlink" title="理论推导"></a>理论推导</h5><p>使用这个公式来更新参数w，$w^{t+1} \leftarrow  w^t - \frac{\eta^t}{\sigma^t}g^t$<br>其中，t表示第t次的update，$g^t = \frac{\partial L(\theta ^t)}{\partial w}$，是损失函数L对参数w的导数，$\sigma^t$表示其先前导数的均方根（root mean square）</p>
<p>计算w的具体例子如下所示，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603165047318.png" alt="image-20200603165047318" style="zoom: 50%; "></p>
<p>得出了$\sigma^t$和$\eta^t$的表达式后，再带入原式，消除分母上的$\sqrt{t+1}$即可得出下面的公式，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603165417281.png" alt="image-20200603165417281" style="zoom:50%;"></p>
<h5 id="Contradiction"><a href="#Contradiction" class="headerlink" title="Contradiction"></a>Contradiction</h5><p>如上图所示，对于一般的梯度下降算法（vanilla gradient descent），当梯度g越大时，步长就越大；对于Adagrad，   $g^t$在分子上，梯度越大步长也越大，$\sum_{i=0}^t(g^i)^2$在分母上，数值越大步长也就越小，看似出现了一个矛盾。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603172539601.png" alt="image-20200603172539601" style="zoom:50%;"></p>
<p>有学者对此也做出了解释，认为Adagrad可以解释$g^t$和$\sum_{i=0}^t(g^i)^2$之间的反差，造成了反差的效果。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603173305074.png" alt="image-20200603173305074" style="zoom:50%;"></p>
<p><strong>gradient越大，函数值离minima的距离就越远</strong>这个说法不一定在所有情况下都是成立的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603174942400.png" alt="image-20200603174942400" style="zoom:50%;"></p>
<p>对于左图中的两个参数w1和w2，画两条直线，保持其中一个变量不变，得出另一个变量的变化曲线，分别对应右图中的曲线。在右图中，对于w1中的a点和w2中的c点，c点距离minimum的距离最近，但梯度却更大。因此在分析梯度和步长时，我们不能只考虑一阶导数的大小，还必须要要考虑二阶导数的大小，即$y^{‘’}=2a$。</p>
<p>右图中的w1曲线，曲率半径比w2的曲线更大，一阶导数变化得更平缓，因此二阶导数的变化就比w2大</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603175920345.png" alt="image-20200603175920345" style="zoom:50%;"></p>
<p>再来回顾下Adagrad中每次更新w的表达式，$w^{t+1}\leftarrow w^t - \frac{\eta}{\sqrt{\sum_{t=0}^t(g^i)^2}}g^t$</p>
<p>一阶导数用$g^t$表示，二阶导数的值则用分母中的$\sum_{i=0}^t(g^i)^2$来进行评估，即使用一阶导数的值来表示二阶导数的值。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603180425226.png" alt="image-20200603180425226" style="zoom:50%;"></p>
<h4 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h4><p>对于传统的梯度下降算法，损失函数L的计算包含了所有的样本;<br>随机梯度下降算法，损失函数$L^n$则只使用其中一个样本，计算效率可以提高很多</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603215238564.png" alt="image-20200603215238564" style="zoom:50%;"></p>
<p>对比示意图如下，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603215557501.png" alt="image-20200603215557501" style="zoom:50%;"></p>
<h4 id="Feature-scaling"><a href="#Feature-scaling" class="headerlink" title="Feature scaling"></a>Feature scaling</h4><h5 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h5><blockquote>
<p>Make different features have the same scaling</p>
</blockquote>
<p>使不同量级的数据集都具有相同的规模，比如x2的都是大于100的值，经过feature scaling，就可以使x2的数值范围和x1相接近。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603220106332.png" alt="image-20200603220106332" style="zoom:50%;"></p>
<p>x2对应w2，x1对应w1，对于同一个w1、w2，但x2的数值不同</p>
<p>在左图中，由于x1的数值相对于x2来说都很小，x1的变化对于y来说影响很小，w1对y的影响也很小，对loss的影响也小，因此梯度$\frac{\partial L}{\partial w_1}$在w1方向的变换也比较平缓；x2的数值较大，对loss的影响也大，因此梯度$\frac{\partial L}{\partial w_2}$在w2方向的变换就比较sharp</p>
<p>在右图中，x1和x2的规模（scale）是接近的，对y的影响不相上下，对loss的影响也差不多</p>
<h5 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603225755433.png" alt="image-20200603225755433" style="zoom:50%;"></p>
<p>计算方式：用$m_i$表示当前样本的平均值，$\sigma_i$为当前样本的标准差，i表示维度，$x_i^r$表示第r个example，使用公式$x_i^r \leftarrow \frac{x_i^r-m_i}{\sigma_i}$来进行归一化计算</p>
<p>feature scaling其实就是将每一个example都进行归一化，使之服从标准正态分布$f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$</p>
<script type="math/tex; mode=display">
\frac{X-\bar {X}}{\sqrt{D(x)}} \sim N(0,1)</script><h4 id="Gradient-Descent-Theory"><a href="#Gradient-Descent-Theory" class="headerlink" title="Gradient Descent Theory"></a>Gradient Descent Theory</h4><h5 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h5><p>在求解最小化问题时，$\theta^* = arg\ min\ L(\theta)$，每次更新$\theta$的值，并不一定能使$𝐿(\theta^0) &gt;𝐿(\theta^1)&gt;𝐿(\theta^2) &gt;⋯$成立</p>
<p>对于给出的$\theta^1,\theta^2$，我们要如何根据这些值来找出最小的loss？这也是我们接下来会研究的问题</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200604110917247.png" alt="image-20200604110917247" style="zoom:50%;"></p>
<h5 id="Taylor-Series"><a href="#Taylor-Series" class="headerlink" title="Taylor Series"></a>Taylor Series</h5><p>泰勒公式定义如下，将函数h(x)在x=x0处展开</p>
<script type="math/tex; mode=display">
h(x) = \sum_{k=0}^{\infty}\frac{h^{(k)}(x_0)}{k!}(x-x_0)^k</script><p>当$x\rightarrow x_0$时，表达式可写为$h(x)\approx h(x_0)+h’(x_0)(x-x_0)$</p>
<p>对于二元函数，当$x\rightarrow x_0,y\rightarrow y_0$时，相应的表达式可以简化为</p>
<script type="math/tex; mode=display">
h(x,y)\approx h(x_0,y_0)+\frac{\partial h(x_0,y_0)}{\partial x}(x-x_0) + \frac{\partial h(x_0,y_0)}{\partial y}(y-y_0)</script><h5 id="Back-to-Formal-Derivation"><a href="#Back-to-Formal-Derivation" class="headerlink" title="Back to Formal Derivation"></a>Back to Formal Derivation</h5><p>损失函数loss可以用以下公式表示，</p>
<script type="math/tex; mode=display">
L(\theta)=L(a,b)+\frac{\partial L(a,b)}{\partial \theta_1}(\theta_1-a) +\frac{\partial L(a,b)}{\partial \theta_2}(\theta_2-b)</script><p>简化表达形式，令$s=L(a,b),\ u=\frac{\partial L(a,b)}{\partial \theta_1},\ v=\frac{\partial L(a,b)}{\partial \theta_2}$</p>
<p>则$L(\theta)\approx s+u(\theta_1-a)+v(\theta_2-b)$</p>
<p>如下图所示，在图中red circle的范围内，找到$\theta_1,\theta_2$，使得loss最小化，设red circle的半径为d，圆心坐标为(a,b)，就新增了一个限制条件$(\theta_1-a)^2+(\theta_2-b)^2\leq d^2$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200604114112061.png" alt="image-20200604114112061" style="zoom:50%;"></p>
<p>由于s与$\theta_1,\theta_2$不相关，这里把s项去掉，$L(\theta)$可以转为内积</p>
<script type="math/tex; mode=display">
\begin{aligned}

L(\theta)_{min} & \approx   u(\theta_1-a)+v(\theta_2-b) \\
 & =  (u,v)\cdot(\theta_1-a,\theta_2-b) \\
 & =  (u,v)\cdot(\Delta \theta_1,\Delta\theta_2)
\end{aligned}</script><p>当$(\Delta \theta_1,\Delta\theta_2)$与$(u,v)$方向相反时，两者的内积为最小值，由于两者的模长不同，用参数$\eta$来表示两者之间的关系。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200604114656448.png" alt="image-20200604114656448" style="zoom:50%;"></p>
<p>由于$\Delta\theta_1=\theta_1-a,\Delta\theta_2=\theta_2-b$，则$\theta_1=a+\Delta\theta_1,\theta_2=b+\Delta\theta_2$，可得出</p>
<script type="math/tex; mode=display">
\begin{bmatrix} \theta_1 \\ \theta_2  \end{bmatrix}
=
\begin{bmatrix} a \\ b  \end{bmatrix}
- \eta \begin{bmatrix} u \\ v  \end{bmatrix}

=
\begin{bmatrix} a \\ b  \end{bmatrix}
- \eta \begin{bmatrix} \frac{\partial L(a,b)}{\partial \theta_1} \\ \frac{\partial L(a,b)}{\partial \theta_2}  \end{bmatrix}</script><h5 id="Limitation"><a href="#Limitation" class="headerlink" title="Limitation"></a>Limitation</h5><p>在真实的实验环境中，我们往往会设置一个临界值（比如$10^{-4}$），当该点的梯度小于该值（即$\approx 0$）时，就停止训练。</p>
<p>因此，gradient descent的限制是，gradient为0的点并不一定是local minimum，还有可能是saddle point，也有可能是接近于0的点</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200604123420086.png" alt="image-20200604123420086" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>gradient descent</tag>
        <tag>adagrad</tag>
        <tag>SGD</tag>
      </tags>
  </entry>
  <entry>
    <title>流水线</title>
    <url>/2020/03/25/%E6%B5%81%E6%B0%B4%E7%BA%BF/</url>
    <content><![CDATA[<h4 id="流水线的特点"><a href="#流水线的特点" class="headerlink" title="流水线的特点"></a>流水线的特点</h4><ul>
<li>把一个处理过程<u>分解</u>为若干个子过程，每个过程由一个专门的功能部件来实现</li>
<li>各<u>功能段的时间</u>应尽可能相等，否则将引起流水线堵塞和断流</li>
<li>每一个段的后面都要有一个<u>缓冲寄存器</u>，称为流水寄存器</li>
<li>适合于大量<u>重复</u>的时序过程，只有在源源不断地输入任务，才能充分发挥流水线的效率</li>
<li>需要有<u>通过时间与排空时间</u></li>
</ul>
<p>在指令流水线中，解决控制相关的方法主要有：<u>冻结或排空流水线</u>、<u>预测分支失败</u>、<u>预测分支成功</u>、<u>延迟分支</u>。</p>
<h4 id="减少分支延迟"><a href="#减少分支延迟" class="headerlink" title="减少分支延迟"></a>减少分支延迟</h4><p>三种通过软件（编译器）来处理的方法（静态的方法）：预测分支失败/成功，延迟分支。</p>
<p><strong>3种方法的共同特点</strong>是：它们对分支的处理方法在<u>程序执行过程中</u>是<u>始终不变</u>的，它们要么总是预测分支失败，要么总是预测分支成功。</p>
<h5 id="预测分支失败"><a href="#预测分支失败" class="headerlink" title="预测分支失败"></a>预测分支失败</h5><p>沿着失败的指令继续处理指令，就好像什么都没发生过似的。当确定分支指令是失败时，说明预测正确，流水线正常流动；当确定分支指令是成功时，流水线就把分支指令之后取出的指令转化为空操作，并按分支目标地址重新取指令执行。</p>
<h5 id="预测分支成功"><a href="#预测分支成功" class="headerlink" title="预测分支成功"></a>预测分支成功</h5><p>当流水线ID段检测到分支指令后，一旦算出了分支目标地址，就开始从该目标地址取指令执行。</p>
<h5 id="延迟分支"><a href="#延迟分支" class="headerlink" title="延迟分支"></a>延迟分支</h5><p>主要思想是从逻辑上“延长”分支指令的执行时间。把延迟分支看成是由分支指令和若干个延迟槽构成，不管分支是否成功，都要执行延迟槽内的指令。</p>
<ul>
<li><p>从前调度</p>
</li>
<li><p>从目标处调度</p>
</li>
<li><p>从失败出调度</p>
</li>
</ul>
<p>三种调度方法的优缺点分别是：<img src="/2020/03/25/流水线/image-20200314165639374.png" alt="三种调度方法的优缺点"></p>
<h4 id="流水线的额外开销"><a href="#流水线的额外开销" class="headerlink" title="流水线的额外开销"></a>流水线的额外开销</h4><ul>
<li>流水寄存器延迟，流水线段与段之间都要设置流水寄存器；</li>
<li>时钟偏移开销，流水线中的时钟到达各流水寄存器的最大差值时间，时钟到达各流水寄存器的时间不是完全相同的。</li>
</ul>
]]></content>
      <categories>
        <category>软件工程</category>
      </categories>
      <tags>
        <tag>流水线</tag>
        <tag>吞吐率</tag>
        <tag>加速比</tag>
        <tag>效率</tag>
      </tags>
  </entry>
  <entry>
    <title>测试</title>
    <url>/2020/03/12/%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<h5 id="测试步骤"><a href="#测试步骤" class="headerlink" title="测试步骤"></a>测试步骤</h5><ul>
<li>模块（单元）测试</li>
<li>子系统测试</li>
<li>系统（集成）测试</li>
<li>验收（确认）测试</li>
<li>平行运行</li>
</ul>
<h5 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h5><p>主要使用白盒测试技术。集中检测软件设计的最小单元-模块，对重要的执行通路进行测试，可以发现模块内部的错误。可以用代码审查和计算机测试，来完成单元测试工作。测试重点为：</p>
<ul>
<li>模块接口</li>
<li>局部数据结构</li>
<li>重要的执行通路</li>
<li>出错处理通路</li>
<li>边界条件</li>
</ul>
<h5 id="集成测试"><a href="#集成测试" class="headerlink" title="集成测试"></a>集成测试</h5><p>是测试和组装软件的系统化技术，把模块组装成程序，在组装的过程进行测试。</p>
<p>非渐增式：先分别测试所有模块，再把这些模块组合成要求的程序；</p>
<p>渐增式：把下一个将要测试的模块同已经测试好的那些模块组合到一起进行测试；</p>
<ul>
<li>自底向上集成</li>
<li>自顶向下集成</li>
</ul>
<p>模块测试的特点是主要应用白盒测试，对多个模块的测试可以并发进行；集成测试的特点是可以在测试过程发现接口问题。</p>
<p>回归测试：重新执行已经做过测试的某个测试的子集。</p>
<p><img src="/2020/03/12/测试/test.png" alt></p>
]]></content>
      <categories>
        <category>软件工程</category>
      </categories>
      <tags>
        <tag>软件工程</tag>
        <tag>单元测试</tag>
        <tag>集成测试</tag>
        <tag>白盒测试</tag>
        <tag>黑盒测试</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机体系结构的基础知识</title>
    <url>/2020/03/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<h4 id="计算机系统结构、组成和实现"><a href="#计算机系统结构、组成和实现" class="headerlink" title="计算机系统结构、组成和实现"></a>计算机系统结构、组成和实现</h4><p>计算机系统结构是指传统机器程序员所看到的计算机属性，即概念结构与功能特性。</p>
<p>在设计主存系统的时候，确定主存容量、寻址方式等属于计算机体系结构；确定时钟周期、逻辑设计等属于计算机组成；而主存存储芯片的类型、线路设计、微组装技术属于计算机实现。</p>
<p>计算机组成是计算机体系结构的逻辑实现，计算机实现是计算机体系结构的物理实现。一种体系结构可以有多种组成，一种组成可以有多种实现。</p>
<h4 id="计算机系统结构的分类"><a href="#计算机系统结构的分类" class="headerlink" title="计算机系统结构的分类"></a>计算机系统结构的分类</h4><h5 id="Flynn分类法"><a href="#Flynn分类法" class="headerlink" title="Flynn分类法"></a>Flynn分类法</h5><p>按照指令流和数据流的多倍性进行分类的。</p>
<ul>
<li>SISD，标量流水线</li>
<li>SIMD，向量流水线</li>
<li>MISD，只是一种人为的划分，目前并没有实际的机器</li>
<li>MIMD，多处理机，<ul>
<li>机群，</li>
<li>PVP（并行向量处理机），</li>
<li>SMP（对称式共享存储器多处理机），</li>
<li>MPP（大规模并行处理机），</li>
<li>DSM（分布式共享存储器多处理机）。</li>
</ul>
</li>
</ul>
<h5 id="冯氏分类法"><a href="#冯氏分类法" class="headerlink" title="冯氏分类法"></a>冯氏分类法</h5><p>用系统的最大并行度对计算机进行分类。最大并行度$P_m$为：单位时间内能够处理的最大二进制位数。</p>
<ul>
<li>字串位串</li>
<li>字串位并</li>
<li>字并位串</li>
<li>字并位并</li>
</ul>
<h5 id="Handler分类法"><a href="#Handler分类法" class="headerlink" title="Handler分类法"></a>Handler分类法</h5><p>根据并行度和流水线来分类。</p>
<h4 id="计算机体系结构设计的主要方法"><a href="#计算机体系结构设计的主要方法" class="headerlink" title="计算机体系结构设计的主要方法"></a>计算机体系结构设计的主要方法</h4><h5 id="由上往下"><a href="#由上往下" class="headerlink" title="由上往下"></a>由上往下</h5><p>从层次结构中的最上面一级开始，逐层往下设计各层机器。先设计软件，再设计硬件。</p>
<h5 id="由下往上"><a href="#由下往上" class="headerlink" title="由下往上"></a>由下往上</h5><p>从层次结构的最下面一级开始，逐层往上设计各层的机器。先设计硬件，再设计软件。</p>
<h5 id="从中间开始"><a href="#从中间开始" class="headerlink" title="从中间开始"></a>从中间开始</h5><p>这里的“中间”是指层次结构中软硬件的交界面，一般指的是传统机器语言级和操作系统级之间。先进行软硬件的分工，确定好这个界面，从这个界面开始，软件设计者开始往上设计操作系统、编译系统等，硬件设计者开始往下设计传统机器级、微程序机器级等。优点：</p>
<ul>
<li>软件和硬件并行设计可以缩短设计周期，设计过程中可以交流协调，是一种交互式的、很好的设计方案。</li>
</ul>
<h4 id="实现软件的可移植性"><a href="#实现软件的可移植性" class="headerlink" title="实现软件的可移植性"></a>实现软件的可移植性</h4><h5 id="统一高级语言"><a href="#统一高级语言" class="headerlink" title="统一高级语言"></a>统一高级语言</h5><p>可以解决所有计算机之间的软件移植，比如Java语言。</p>
<h5 id="采用系列机"><a href="#采用系列机" class="headerlink" title="采用系列机"></a>采用系列机</h5><p>系列机是指同一厂家生产的具有相同的系统结构，但具有不同组成和实现的同一系列不同型号的计算机。</p>
<p>采用系列机可以解决<u>同一系列的计算机</u>之间的软件移植。</p>
<ul>
<li>向上兼容，程序不修改就可以运行于比它<u>高档</u>的计算机</li>
<li>向后兼容，程序不修改就可以运行于在它<u>之后投入市场</u>的计算机</li>
</ul>
<h5 id="模拟和仿真"><a href="#模拟和仿真" class="headerlink" title="模拟和仿真"></a>模拟和仿真</h5><p>可以在不同体系结构的计算机之间相互移植，在一种计算机上实现另一种计算机的指令系统。</p>
<p><strong>模拟</strong>是用<u>软件</u>的方法在一台计算机（宿主机）上实现另一台计算机（虚拟机）的指令系统。运行速度慢，性能较差。</p>
<p><strong>仿真</strong>是用现有计算机（宿主机）的<u>微程序</u>去解释实现另一台计算机（目标机）的指令系统。</p>
<p>主要区别在于解释执行所用的语言。仿真是用微程序去解释，其解释程序放在控制存储器中；模拟是用机器语言去解释，其解释程序放在主存中。仿真的运行速度更快，但仿真只能在系统结构差异不大的计算机之间使用。</p>
<h4 id="摩尔定律"><a href="#摩尔定律" class="headerlink" title="摩尔定律"></a>摩尔定律</h4><p>集成电路芯片上所集成的晶体管数目每隔18个月就翻一番。</p>
<h4 id="并行性"><a href="#并行性" class="headerlink" title="并行性"></a>并行性</h4><p>计算机系统的并行性具有不同的等级。</p>
<p>从处理数据的角度来看：</p>
<ul>
<li>字串位串，每次只对一个字的一位进行处理</li>
<li>字串位并</li>
<li>字并位串</li>
<li>字并位并</li>
</ul>
<p>从执行程序的角度来看：</p>
<ul>
<li>指令内部并行，单指令之间各种微操作并行</li>
<li>指令级并行，并行或并发执行两条或两条以上指令</li>
<li>线程级并行<ul>
<li>线程是一个相对独立、可独立调度和指派的执行单元，它比进程要轻巧。</li>
</ul>
</li>
<li>任务级或过程级并行</li>
<li>作业或程序级并行</li>
</ul>
<h4 id="指令集结构"><a href="#指令集结构" class="headerlink" title="指令集结构"></a>指令集结构</h4><p>根据CPU内部状态，可以将指令系统的机构分为：</p>
<ul>
<li><u>堆栈</u>型结构</li>
<li><u>累加器</u>型结构</li>
<li><u>通用寄存器</u>结构</li>
</ul>
<p>将通用存储器型结构进一步细分为：<u>RR型、RM型、MM型</u>。</p>
<h4 id="向量的处理方式"><a href="#向量的处理方式" class="headerlink" title="向量的处理方式"></a>向量的处理方式</h4><ul>
<li>横向处理方式，按行的方式从左到右进行计算的，相当于进行了N次循环；</li>
<li>纵向处理方式，按列的方式从上到下进行计算的，将整个向量按相同的运算处理完之后，再去进行别的运算；</li>
<li>纵横处理方式，又称为分组处理方式，把向量分成若干组，组内按照纵向方式处理，一次处理各组。</li>
</ul>
<p>后面两种方式都适用于向量处理机。</p>
<h4 id="提高向量处理机性能的常用技术"><a href="#提高向量处理机性能的常用技术" class="headerlink" title="提高向量处理机性能的常用技术"></a>提高向量处理机性能的常用技术</h4><ul>
<li>设置多个处理部件，使它们并行工作；</li>
<li>采用链接技术，加快一串向量指令的执行；</li>
<li>采用循环开采技术，加快循环的处理；</li>
<li>采用多处理机系统，进一步提高系统性能。</li>
</ul>
<p><strong>链接</strong>：具有<u>先写后读相关（RAW）</u>的两条指令，在不出现<u>功能部件</u>冲突和其他<u>$V_i$（寄存器）</u>冲突的情况下，可以把功能部件链接起来进行流水处理。</p>
<p><strong>分段开采技术</strong>：当向量的长度大于向量处理器的长度时，必须把长向量分为长度固定的段，然后循环分段处理，每一次循环只处理一个向量段。</p>
]]></content>
      <categories>
        <category>计算机体系结构</category>
      </categories>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>Flynn分类法</tag>
        <tag>冯氏分类法</tag>
        <tag>Handler分类法</tag>
        <tag>模拟</tag>
        <tag>仿真</tag>
        <tag>系列机</tag>
        <tag>分段开采</tag>
        <tag>链接</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机体系结构重要考点</title>
    <url>/2020/03/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E9%87%8D%E8%A6%81%E8%80%83%E7%82%B9/</url>
    <content><![CDATA[<h4 id="同时多线程"><a href="#同时多线程" class="headerlink" title="同时多线程"></a>同时多线程</h4><p>线程：进程内的一个相对独立、可独立调度和指派的执行单元，它比进程要“轻巧”得多。线程中包含有调度所需的信息，但它自己基本上不拥有系统资源，只拥有在运行过程中必不可少的一点资源。线程切换时，只需保存和设置少量寄存器的内容，开销很小。</p>
<p>线程级并行：指并行执行两个或两个以上的线程。</p>
<p>同时多线程（SMT）：是一种在多流出、动态调度的处理器上结合了线程级并行和指令级并行的技术，它是多线程技术的一种改进。通过寄存器命名和动态调度机制，来自各个独立线程的多条指令可以同时流出，而不用考虑它们之间的相互依赖关系。</p>
<h5 id="实现多线程的主要方法"><a href="#实现多线程的主要方法" class="headerlink" title="实现多线程的主要方法"></a>实现多线程的主要方法</h5><p><u>细粒度多线程</u>：在每条指令之间都可以进行线程的切换，从而使得多个线程可以交替执行。</p>
<ul>
<li>优点：能够隐藏由长时间停顿带来的吞吐率的损失，也能够隐藏由段时间停顿带来的损失。</li>
<li>缺点：减慢了单个线程的执行。即使没有任何停顿的线程也不能连续执行，而且会因为其他线程的指令的插入执行而被延迟。</li>
</ul>
<p><u>粗粒度多线程</u>：线程之间的切换只发生在时间较长的停顿出现的时候。</p>
<ul>
<li>优点：减少了切换次数，很大程度上也不会降低单个线程的执行速度。</li>
<li>缺点：减少吞吐率损失的能力有限。由于实现粗粒度多线程的CPU只能执行单个线程的指令，不能交叉执行多个线程，因此当发生停顿时，流水线必须排空或暂停。</li>
</ul>
<h4 id="存储系统"><a href="#存储系统" class="headerlink" title="存储系统"></a>存储系统</h4><h5 id="写策略"><a href="#写策略" class="headerlink" title="写策略"></a>写策略</h5><p>写直达法：在执行写操作时，不仅把数据写入Cathe中相应的块，而且也写入下一级存储器。减少CPU写停顿的常用方法是，采用写缓冲器。</p>
<p>写回法：只把数据写入Cathe中相应的块，不写入下一级存储器。这些最新数据只有在相应的块被替换出时，才被写回下一级存储器。速度快，要设置“修改位”。</p>
<p>写不命中时，</p>
<p>按写分配法：写不命中时，把所写单元的块从主存中调入Cathe，然后再进行写入。与<u>写回法</u>配合。</p>
<p>不按写分配法：写不命中时，直接写入下一级存储器而不将相应的块调入Cathe。</p>
<h5 id="三种类型的Cathe不命中"><a href="#三种类型的Cathe不命中" class="headerlink" title="三种类型的Cathe不命中"></a>三种类型的Cathe不命中</h5><p><u>强制性不命中</u>：当第一次访问一个快时，该块不在Cathe中，需从下一级存储器中调入Cathe。（冷启动访问不命中，首次访问不命中）</p>
<p><u>容量不命中</u>：如果<u>程序执行时所需的块</u>不能全部调入Cathe中，当某些块被调出后，若又重新被访问，就会发生不命中。</p>
<p><u>冲突不命中</u>：在组相联或者直接映像Cathe中，若太多的块映像到同一组（块）中，则会出现该组中某个块被别的块替换掉，然后又重新被访问的情况。（碰撞不命中，冲突不命中）</p>
<p>关系：</p>
<ul>
<li><strong>相联度</strong>越高，冲突不命中就越少；</li>
<li>强制性不命中和容量不命中不受相联度的影响；</li>
<li>强制性不命中不受<strong>Cathe容量</strong>的影响，但容量不命中却随着容量的增加而减少。</li>
</ul>
<h4 id="IO系统"><a href="#IO系统" class="headerlink" title="IO系统"></a>IO系统</h4><h5 id="I-O系统的可靠性、可用性和可信性"><a href="#I-O系统的可靠性、可用性和可信性" class="headerlink" title="I/O系统的可靠性、可用性和可信性"></a>I/O系统的可靠性、可用性和可信性</h5><p>可靠性：系统从某个初始参考点开始<u>一直连续提供服务的能力</u>，用平均无故障时间（<strong>MTTF</strong>）来衡量。MTTF的导数是失效率。系统总体的失效率是各部件的失效率之和。系统<u>中断服务的时间</u>用平均修复时间（<strong>MTTR</strong>）来衡量。</p>
<p>可用性：系统正常工作的时间在连续两次服务间隔时间中所占的比例。</p>
<script type="math/tex; mode=display">
可用性=\frac{MTTF}{MTTF+MTTR}</script><p>   平均失效间隔时间，MTBF</p>
<script type="math/tex; mode=display">
MTBF=MTTF+MTTR</script><p>可信性：是服务的质量，是<u>不可度量</u>的。</p>
<h5 id="廉价磁盘冗余阵列-RAID"><a href="#廉价磁盘冗余阵列-RAID" class="headerlink" title="廉价磁盘冗余阵列 (RAID)"></a>廉价磁盘冗余阵列 (RAID)</h5><p>RAID0：<u>无冗余和无校验</u>的磁盘阵列，成本最低。</p>
<p>RAID1：<u>镜像</u>磁盘阵列。</p>
<p>RAID2：采用纠错的<u>海明码</u>的磁盘阵列，冗余盘数量为$log_2m$，m为数据盘的个数。</p>
<p>RAID3：<u>位</u>交叉<u>奇偶校验</u>，只需要一个校验盘。</p>
<p>RAID4：<u>块</u>交叉<u>奇偶校验</u>，只需要一个校验盘。</p>
<p>RAID5：<u>无独立校验</u>的<u>奇偶校验</u>磁盘阵列，将校验信息分布到磁盘阵列中的各个磁盘。</p>
<p>以上奇偶校验磁盘阵列只是在<u>一个盘</u>出现故障的情况下，仍能继续工作和恢复数据。</p>
<p>RAID6：<u>P+Q双校验</u>磁盘阵列，可以容忍<u>两个磁盘</u>出错。</p>
<p>RAID10：RAID1+0，先进行镜像（RAID1），再进行条带存放（RAID0）。<br>RAID01：RAID0+1，先进行条带存放（RAID0），再进行镜像（RAID1）。</p>
]]></content>
      <categories>
        <category>计算机体系结构</category>
      </categories>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>机群</tag>
        <tag>MPP</tag>
        <tag>SMP</tag>
        <tag>消息传递</tag>
        <tag>共享存储器</tag>
        <tag>同时多线程</tag>
        <tag>RAID</tag>
      </tags>
  </entry>
  <entry>
    <title>软件工程学概述</title>
    <url>/2020/03/08/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E5%AD%A6%E6%A6%82%E8%BF%B0/</url>
    <content><![CDATA[<h4 id="软件危机"><a href="#软件危机" class="headerlink" title="软件危机"></a>软件危机</h4><ol>
<li><p>定义<br>在计算机软件的开发和维护过程会出现的一系列严重问题。包含两方面的问题：如何开发软件，以满足对软件日益增长的需求；如何维护数量不断膨胀的已有软件。</p>
</li>
<li><p>典型表现：</p>
<ul>
<li>软件成本日益增长；</li>
<li>软件开发效率低；</li>
<li>软件质量差；</li>
<li>软件维护困难；</li>
<li>软件开发速度跟不上计算机发展速度；</li>
<li>软件通常没有合适的文档资料。</li>
</ul>
</li>
<li><p>产生原因：</p>
<ul>
<li>技术原因<ul>
<li>软件<strong>规模</strong>越来越大；</li>
<li>软件的<strong>复杂度</strong>越来越高；</li>
</ul>
</li>
<li>管理原因<ul>
<li>软件开发缺乏正确的理论指导，过分依赖个人技巧和创造性；</li>
<li>对用户需求没有完整正确的认识，就匆忙着手忙于编写程序。</li>
</ul>
</li>
</ul>
</li>
<li><p>消除软件危机的途径</p>
<ul>
<li>对计算机软件有一个正确的认识；</li>
<li>还必须充分认识到软件开发是一种组织良好、管理严密、各类人员协同配合、共同完成的工程项目；</li>
<li>推广使用在时间中总结出来的开发软件的成功的技术和方法，并且探索更好更加有效的技术和方法；</li>
<li>开发和使用更好的软件工具。</li>
</ul>
</li>
</ol>
<h4 id="软件工程"><a href="#软件工程" class="headerlink" title="软件工程"></a>软件工程</h4><ol>
<li>定义：<ul>
<li>把系统的、规范的、可度量的途经应用于软件开发、运行和维护过程，也就是把工程应用于软件；</li>
<li>研究1提到的途径。</li>
</ul>
</li>
<li>基本原理：<ul>
<li>用分阶段的生命周期计划严格管理</li>
<li>坚持进行阶段评审</li>
<li>实行严格的产品控制</li>
<li>结果应能清楚地审查</li>
<li>采用现代化程序设计技术</li>
<li>开发小组人员应少而精</li>
<li>必须承认不断改进软件工程的必要性</li>
</ul>
</li>
<li>软件工程方法学的3个要素：<u>方法、工具和过程</u>。</li>
</ol>
<h4 id="软件生命周期"><a href="#软件生命周期" class="headerlink" title="软件生命周期"></a>软件生命周期</h4><p>由软件定义、软件开发和运行维护3个时期组成。</p>
<ol>
<li>软件定义<ul>
<li><u>问题定义</u></li>
<li><u>可行性分析</u></li>
<li><u>需求分析</u>，确定目标系统具有哪些功能，并用正式文档记录对目标系统的需求（规格说明书）；</li>
</ul>
</li>
<li>软件开发<ul>
<li><u>总体设计</u></li>
<li><u>详细设计</u></li>
<li><u>单元和编码测试</u></li>
<li><u>综合测试</u>，最基本的是集成测试和验收测试</li>
</ul>
</li>
<li><u>软件维护</u>，包括四类维护性活动：<u>改正性维护，完善性维护，适应性维护，预防性维护</u>。</li>
</ol>
<h4 id="软件过程"><a href="#软件过程" class="headerlink" title="软件过程"></a>软件过程</h4><p>为了获得高质量软件所需要完成的一系列任务的框架，它规定了完成各项任务的工作步骤。</p>
<ol>
<li><p>瀑布模型</p>
<ul>
<li>具有顺序性和依赖性</li>
<li>推迟实现</li>
<li>质量保证<br>是带<u>反馈环</u>的，每个阶段都有必须完成的文档，在每个阶段结束前都要对文档进行评审，是<u>文档驱动</u>的。</li>
</ul>
</li>
<li><p>快速原型模型<br>快速建立起来的可以在计算机上运行的程序，它所能完成的功能往往是最终产品能完成功能的一个子集。不带反馈环。</p>
</li>
<li><p>增量模型<br>把软件产品作为一系列的增量构件来设计、编码、集成和测试。每个构件由多个相互作用的模块构成，并且能够完成特定的功能。<br>优点：</p>
<ul>
<li>能在较短时间内向用户提交可完成部分工作的产品；</li>
<li>使用户有比较充裕的时间学习和适应新产品，减少一个全新软件可能给客户组织带来的冲击。</li>
</ul>
</li>
<li><p>螺旋模型<br>使用原型以及其他方法来尽量降低风险，看作是在每个阶段之前都增加了风险分析过程的快速原型模型。<br>优点：</p>
<ul>
<li>有利于已有软件的重用</li>
<li>有助于把软件质量作为软件开发的一个重要目标</li>
<li>减少了过多测试所带来的风险</li>
<li>维护只是模型的另一个周期，在维护和开发之前并没有本质区别</li>
<li><u>风险驱动</u>，可以及时中断项目。<br>风险驱动也是它的一个弱点，需要专业的风险评估人员。</li>
</ul>
</li>
<li><p>喷泉模型<br>是典型的面向对象的软件过程模型之一，体现了面向对象软件开发过程<u><strong>迭代</strong></u>和<u><strong>无缝</strong></u>的特性。</p>
</li>
</ol>
<h4 id="可行性分析"><a href="#可行性分析" class="headerlink" title="可行性分析"></a>可行性分析</h4><p>包括：</p>
<ul>
<li>技术可行性</li>
<li>经济可行性</li>
<li>操作可行性</li>
<li>法律可行性</li>
</ul>
<h5 id="基本过程"><a href="#基本过程" class="headerlink" title="基本过程"></a>基本过程</h5><ul>
<li>复查系统规模和目标；</li>
<li>研究目前正在使用的系统；</li>
<li>导出新系统的高层逻辑模型；</li>
<li>进一步定义问题；</li>
<li>导出和评价供选择的解法；</li>
<li>推荐行动方针；</li>
<li>草拟开发计划；</li>
<li>书写文档提交审查。</li>
</ul>
<h5 id="数据流图"><a href="#数据流图" class="headerlink" title="数据流图"></a>数据流图</h5><p>基本要素：</p>
<ul>
<li>数据的源点/终点</li>
<li>变换数据的处理</li>
<li>数据存储</li>
<li>数据流</li>
</ul>
<h4 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h4><h5 id="需求分析的任务"><a href="#需求分析的任务" class="headerlink" title="需求分析的任务"></a>需求分析的任务</h5><ul>
<li>确定对系统的综合要求，功能要求，性能要求，接口需求，可靠性和可用性需求；</li>
<li>分析系统的数据要求，建立数据模型（E-R图）；</li>
<li>导出系统的逻辑模型，用数据流图、E-R图、状态转换图、数据字典和主要的处理算法来描述这个逻辑模型；</li>
<li>修正系统开发计划。</li>
</ul>
<h5 id="获取需求的方法"><a href="#获取需求的方法" class="headerlink" title="获取需求的方法"></a>获取需求的方法</h5><ul>
<li>访谈；</li>
<li>面向数据流自顶向下求精，<u>结构化分析方法</u>就是面向<u>数据流</u>自顶向下逐步求精进行需求分析的方法；</li>
<li>简易的应用规格说明技术，提倡用户与开发者密切合作；</li>
<li>快速建立软件模型。</li>
</ul>
<h5 id="需求分析的过程"><a href="#需求分析的过程" class="headerlink" title="需求分析的过程"></a>需求分析的过程</h5><ul>
<li>问题识别</li>
<li>分析与综合</li>
<li>编写需求文档分析</li>
<li>评审</li>
</ul>
<h5 id="需求分析规格说明书"><a href="#需求分析规格说明书" class="headerlink" title="需求分析规格说明书"></a>需求分析规格说明书</h5><p>主要内容为：描述系统的数据要求、功能要求、性能要求、可靠性和可用性要求、出错处理需求、接口需求、约束、逆向需求以及将来可能提出的要求。</p>
<h5 id="主要建模工具"><a href="#主要建模工具" class="headerlink" title="主要建模工具"></a>主要建模工具</h5><ul>
<li>建立功能模型的<u>数据流图</u></li>
<li>建立数据模型的<u>E-R图</u></li>
<li>建立行为模型的<u>状态图</u></li>
<li>层次方框图</li>
<li>Warnier图</li>
<li>IPO图   </li>
</ul>
]]></content>
      <categories>
        <category>软件工程</category>
      </categories>
      <tags>
        <tag>软件工程</tag>
        <tag>软件过程</tag>
        <tag>软件危机</tag>
        <tag>软件生命周期</tag>
        <tag>软件工程方法学</tag>
      </tags>
  </entry>
</search>
