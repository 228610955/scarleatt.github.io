<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Anomaly Detection</title>
    <url>/2020/07/03/Anomaly-detection/</url>
    <content><![CDATA[<h4 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h4><p>对于给定的训练数据集，做异常检测的目的是找到一个函数，这个函数可以检测输入的x是不是和训练数据集相似的。异常检测知识检测出和训练数据不一样的输入，检测出来的数据并不一定都是不好的，有可能是好的，也有可能是不好的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703142752745.png" alt="image-20200703142752745" style="zoom:60%;"></p>
<p>那么到底怎么来定义这个similarity呢？这也就是异常检测需要探讨的问题，不同的方法就有不同的定义方式。</p>
<h4 id="What-is-Anomaly"><a href="#What-is-Anomaly" class="headerlink" title="What is Anomaly?"></a>What is Anomaly?</h4><p>Q：那么我们到底如何来定义anomaly呢？如何确定我们检测出来的输入是anomaly的呢？</p>
<p>A：取决于具体的训练数据集。在下图中，如果训练集中有很多雷丘，那么皮卡丘就是异常；但如果有很多只皮卡丘，那么雷丘就是异常,….</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703143729820.png" alt="image-20200703143729820" style="zoom:60%;"></p>
<h4 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h4><p>异常检测有很多应用，以下简要叙述三个：</p>
<ol>
<li><p>Fraud Detection，training data是正常刷卡行为， 那么现有一笔新的交易记录x，我们就可以进行异常检测，来判断x是不是盗刷行为；比如日常都是小额消费，但突然多了一些连续的高额消费，就可以认为这是盗刷行为；</p>
</li>
<li><p>Network Intrusion Detection，training data是正常连接行为，现在有一个新的连接x进来，那么我们就可以让机器自己进行异常检测，来判断新连接是不是异常的；</p>
</li>
<li><p>Cancer Detection，training data是正常细胞的资料，比如细胞核的大小、分裂的频率等，如果来一个新的细胞x，机器可以自己决定到底是正常细胞还是癌细胞。</p>
</li>
</ol>
<h4 id="Binary-Classification"><a href="#Binary-Classification" class="headerlink" title="Binary Classification?"></a>Binary Classification?</h4><p>Q：给定正常的数据（Class 1）为$\{x^1,x^2,..,x^N\}$，异常的数据（Class 2）为$\{\tilde x^1,\tilde x^2,..,\tilde x^N\}$；那么异常检测可以认为是一个二分类问题吗？</p>
<p>A：如果正常数据集是“宝可梦”，那么异常数据就“不是宝可梦”，但“不是宝可梦”包含很多种类别，比如茶壶、树等；而且异常数据并不像正常数据那么容易收集；因此并不能简单地进行一个二分类问题。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703145452812.png" alt="image-20200703145452812" style="zoom:60%;"></p>
<h4 id="Categories"><a href="#Categories" class="headerlink" title="Categories"></a>Categories</h4><p>异常检测可以分为以下几个类别：</p>
<ol>
<li>training data带label，每个训练数据$x^i$都有对应的标签$\hat y ^i$，一共有N个类别，那么我们就可以训练一个classifier，来进行分类，如果机器能辨别出这个input是1-N个类别，那么就认为是正常数据；如果机器不能辨别，即输出类别为“unknown”，我们就认为这是异常数据；</li>
<li>如果training data是unlabel，这时又分为两种情况：（1）training data是clean的，所有的数据都是正常的；（2）training data中有一小部分数据是异常的，比如银行要对用户数据进行异常检测，那么training data就不可避免地包括一些异常数据。</li>
</ol>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703150215978.png" alt="image-20200703150215978" style="zoom:60%;"></p>
<h4 id="Case-1-With-Classifier"><a href="#Case-1-With-Classifier" class="headerlink" title="Case 1: With Classifier"></a>Case 1: With Classifier</h4><h5 id="Example-Application"><a href="#Example-Application" class="headerlink" title="Example Application"></a>Example Application</h5><p>现在我们判断一个人物是不是来自辛普森的家庭。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703151112051.png" alt="image-20200703151112051" style="zoom:60%;"></p>
<p>在下图中对于输入的正常数据$\{x^1,x^2,..,x^N\}$，都有其对应的类别标签$\{\hat y^1,\hat y^2,..,\hat y^N\}$，那么我们就可以训练一个classifier，来预测其输出的类别。一位很喜爱辛普森的学者进行了实验，得出了很不错的分类结果。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703151635273.png" alt="image-20200703151635273" style="zoom:60%;"></p>
<h5 id="How-to-use-the-Classifier"><a href="#How-to-use-the-Classifier" class="headerlink" title="How to use the Classifier"></a>How to use the Classifier</h5><p>那么根据训练出来的classifier来做异常检测，来判断一个人物到底是不是来自辛普森家庭。</p>
<p>我们可以让classifier再输出一个confidence score，同时我们还需要定义一个界限threshold $\lambda$，如果信心分数$c(x)&gt;\lambda$，就认为是正常数据；否则就认为是异常数据。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703152031850.png" alt="image-20200703152031850" style="zoom:60%;"></p>
<p>那么我们如何得到这个confidence score呢？</p>
<p>在下图的例子中，如果是一个辛普森家庭的人物输入classifier，这个分类器能够很自信地得出分类结果是“霸子”（0.97）；但如果是一个非辛普森的人物输入，classifier的输出分类结果就比较均匀，4个分类就过都是0.20+。那么我们就可以认为classifier对第一个input是很自信的，是normal；第二个input则不自信，被认为是anomaly data。</p>
<p>那么我们就可以把这个confidence score认为是分类结果的distribution中的最大值，也可以做cross-entropy。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703153414866.png" alt="image-20200703153414866" style="zoom:60%;"></p>
<p>那么现在我们把辛普森家庭和其他动漫图片的所有数据都进行confidence score计算。在下图中，我们可以发现属于辛普森的confidence score distribution，几乎全都是1；但对于其他动漫的人物，只有10%的score为1，其他大多数的图片得到的分数都是比较低的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703154546578.png" alt="image-20200703154546578" style="zoom:60%;"></p>
<h5 id="Example-Framework"><a href="#Example-Framework" class="headerlink" title="Example Framework"></a><strong>Example Framework</strong></h5><p>训练数据集是带label的，全都是属于辛普森家庭，那么我们可以训练出一个classifier，根据这个分类器，我们可以计算出每张图片的confidence score，根据threshold来判断这张图片是不是anomaly。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703155315020.png" alt="image-20200703155315020" style="zoom:60%;"></p>
<p><strong>development set</strong>中需要包括带label（是否属于辛普森）的图片，<u>但这个图片不仅包括辛普森家庭的人物，还包括不属于辛普森家庭的人物</u>。将数据输入训练好的classifier，根据不同的$\lambda$，得到系统的performance，那么我们就可以通过development set来调整$\lambda$的值，选择使系统得到最好performance的$\lambda$。</p>
<h5 id="Evalution"><a href="#Evalution" class="headerlink" title="Evalution"></a>Evalution</h5><p>现在有100张辛普森人物的图片，5张非辛普森。在图中红色部分则表示非辛普森的score，一共有5个方块，得到的分数都很低。</p>
<p>正确率并不能来准确衡量一个系统的好坏，很可能一个系统有一个很高的正确率，但其实并不好。在下图中，我们可以计算出这个系统所计算出的accuracy为95.2%，但这并不是因为我们的异常检测系统很好，而是因为数据集的分布差异，异常数据所占的比例非常小。这并不是我们想要的异常检测系统，这个系统几乎会把所有的输入都判断成是异常的，但由于异常数据占比很小，最后的正确率还是会很高。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703160756452.png" alt="image-20200703160756452" style="zoom:60%;"></p>
<p>因此我们不能用正确率accuracy来判断系统的performance。</p>
<p>在下图中，我们把threshold设置为大于0.5的值，这时只有1个异常值被检测出来，还有剩下4个异常值没有被检测出来，这4个被认为是<u>missing</u>了；而在所有的100个辛普森人物里面，只有1个被检测为异常值，被认为是<u>False alarm</u>，剩下的99个都认为是正常的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703162036050.png" alt="image-20200703162036050" style="zoom:60%;"></p>
<p>如果我们现在把threshold设置为大于0.8的值，这时只有2个异常值被检测出来，还有剩下3个异常值没有被检测出来（missing）；而在所有的100个辛普森人物里面，只有6个被检测为异常值（false alarm），剩下的94个都认为是正常的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703162819041.png" alt="image-20200703162819041" style="zoom:60%;"></p>
<p>那么这两个threshold分别为0.5和0.8的异常检测系统，到底哪一个更好呢？这取决于你认为missing更严重，还是false alarm更严重。</p>
<p>在下图中的<span style="color: red">Cost Table A</span>，如果是missing，就扣一分；如果是false alarm，就扣100分；如果用这种方式来衡量系统的好坏，那么左边的被扣了104分，右边的被扣了603分。</p>
<p><span style="color: blue">Cost Table B</span>，如果是missing，就扣100分；如果是false alarm，就扣1分；如果用这种方式来衡量系统的好坏，那么左边的被扣了401分，右边的被扣了306分。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703162727493.png" alt="image-20200703162727493" style="zoom:60%;"></p>
<p>在不同的场景下，就会有不同的cost table。比如癌症检测，missing比false alarm造成的影响大得多，如果这个人患了癌症却没被检测出来，后果会非常严重，因此这种情况我们table B比较好。</p>
<h5 id="Possible-Issues"><a href="#Possible-Issues" class="headerlink" title="Possible Issues"></a>Possible Issues</h5><p>下图中展示了猫狗分类的例子，黑色线条表示decision boundary，靠近boundary得到的confidence score不高，如果在boundary上，就说明分类器不能确定图片的类别是猫还是狗。对于一些特别像猫的动物，比如老虎，也能够得到很好的分数，因为老虎有着和猫很像的特征，而这个特征是是分类器进行分类的关键，就可以迷惑分类器，得出错误的分类结果。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703165414744.png" alt="image-20200703165414744" style="zoom:60%;"></p>
<p>如果这个分类器判断是不是辛普森家庭人物的关键是，人物的脸和皮肤是不是黄色。那么我们可以把其他动漫人物的脸涂成黄色，就可以发现分类器判断为辛普森的概率提高了很多。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703165502001.png" alt="image-20200703165502001" style="zoom:60%;"></p>
<h5 id="To-Learn-More-……"><a href="#To-Learn-More-……" class="headerlink" title="To Learn More ……"></a>To Learn More ……</h5><p>也有一些文献提出了解决方法，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703170742579.png" alt="image-20200703170742579" style="zoom:60%;"></p>
<h4 id="Case-2-Without-Labels"><a href="#Case-2-Without-Labels" class="headerlink" title="Case 2: Without Labels"></a>Case 2: Without Labels</h4><h5 id="Twitch-Plays-Pokemon"><a href="#Twitch-Plays-Pokemon" class="headerlink" title="Twitch Plays Pokémon"></a>Twitch Plays Pokémon</h5><p>先介绍一个游戏，Twitch Plays Pokémon，每个用户都可以在switch上输入自己的指令（up、left、…），可以同时有8万个用户，在这些用户中有些是异常用户，会输入无关指令干扰游戏的正常执行。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703171432329.png" alt="image-20200703171432329" style="zoom:60%;"></p>
<h5 id="Problem-Formulation-1"><a href="#Problem-Formulation-1" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h5><p>那么我们如何移除这些游戏干扰者呢？</p>
<p>我们首先对每一个玩家都用x表示，即表示成一个向量；对于其中一个维度$x_2$，如果这个人经常发表无政府言论，我们就可以认为这个人是来游戏中捣乱的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703171722577.png" alt="image-20200703171722577" style="zoom:60%;"></p>
<p>但现在数据集是没有label的，没有classifier了，那么我们可以建立一个模型$P(x)$，这个模型可以告诉我们发生某种行为的概率有多大。如果$P(x)\geq\lambda$，那么我们就可以认为x是正常的，反之则认为是anomaly。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703172257910.png" alt="image-20200703172257910" style="zoom:60%;"></p>
<p>如果现在对于一个玩家用二维向量来表示，一个维度表示“说垃圾话”的几率，另一个表示“无政府状态发言”的几率。对N个玩家的数据进行可视化，可以得到下图中的散点图。如果我们对其中“说垃圾话”维度的数据进行可视化，可以发现并不是所有的玩家都不说垃圾话，大部分都会说一点点；对于第二个维度，到底有多少人的发言是在无政府状态下进行的，可以发现多数人都是在无政府状态下发言的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703172908070.png" alt="image-20200703172908070" style="zoom:60%;"></p>
<p>对于$P(x)$比较大的玩家，我们就可以认为是normal的；对于$P(x)$小的玩家，我们就认为其是anomaly的。但这并不具体，我们应该用数值化的方法来计算每一个玩家的score。</p>
<h5 id="Maximum-Likelihood"><a href="#Maximum-Likelihood" class="headerlink" title="Maximum Likelihood"></a><strong>Maximum Likelihood</strong></h5><p>首先我们假设这些数据点都是从概率密度函数$f_{\theta} (x)$中取出来的，$\theta$决定了概率密度函数的形状，需要从训练数据中学习出来。现在我们已经有了数据点，但不知道其概率分布$f_{\theta} (x)$的形式。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703193052117.png" alt="image-20200703193052117" style="zoom:60%;"></p>
<p>这里引入了likelihood的概念，表示根据我现在有的概率密度函数$f_{\theta} (x^1),f_{\theta} (x^2),..,f_{\theta} (x^N)$，产生左图数据分布的概率有多大。</p>
<p>Q：那么我们怎么算图中的数据被产生出来的几率大小呢？</p>
<p>A：对于图中的所有数据点$\{x^1,x^2,..,x^N\}$，产生数据点$x^1$的概率是$f_{\theta} (x^1)$，产生数据点$x^2$的概率是$f_{\theta} (x^2)$，…，产生数据点$x^N$的概率是$f_{\theta} (x^N)$；那么对于产生所有数据点的概率，就可以用以下的式子表示，</p>
<script type="math/tex; mode=display">
L(\theta)=f_{\theta} (x^1)f_{\theta} (x^2)\cdot\cdot\cdot f_{\theta} (x^N)</script><p>也就是likelihood，和$\theta$是有关的，选择不同的$\theta$，就会有不同的概率密度函数，也就会有不同的likelihood，那么现在我们的任务就是找到$\theta^*$，使得likelihood可以取得最大值，即</p>
<script type="math/tex; mode=display">
\theta ^*=arg \ \mathop{\rm max}_\theta L(\theta)</script><h5 id="Gaussian-Distribution"><a href="#Gaussian-Distribution" class="headerlink" title="Gaussian Distribution"></a><strong>Gaussian Distribution</strong></h5><p>下图中的$f_{\mu,\sum}(x)$为高斯分布，输入为x，输出为x被sample的概率，是一个数值，$\mu$表示均值，$\sum$表示协方差矩阵。 </p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703195322546.png" alt="image-20200703195322546" style="zoom:60%;"></p>
<p>由于此时的概率密度函数发生了变化，我们现在将likelihood的形式进行变化，即</p>
<script type="math/tex; mode=display">
L(\mu,\sum)=f_{\mu,\sum} (x^1)f_{\mu,\sum} (x^2)\cdot\cdot\cdot f_{\mu,\sum} (x^N)</script><p>那么此时我们就需要找到$\mu^<em>,\sum^</em>$，来使likelihood最大化，</p>
<script type="math/tex; mode=display">
\mu^*,\sum=arg \ \mathop{\rm max}_{\mu,\sum} L(\mu,\sum)</script><p>在上图中，由于高斯分布的特性，数据点在$\mu$附近的地方很容易被sample到，越远被sample到的几率就越低；如果$\mu$在数据点很密集的地方，我们就可以认为likelihood的值很大；如果$\mu$在偏离高密度的地方，likelihood会小很多。</p>
<p>$\mu^*$就是输入数据点求均值，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703201627458.png" alt="image-20200703201627458" style="zoom:60%;"></p>
<p>找出了对应的$\mu^<em>,\sum^</em>$之后，我们就可以代入对应的概率密度函数，进行异常检测。如果$f_{\mu^<em>,\sum^</em>}(x)&gt;\lambda$，就认为是正常数据；如果$f_{\mu^<em>,\sum^</em>}(x)\leq\lambda$，则认为是异常数据（anomaly）。</p>
<p>如果把这个二维平面上所有的数据都输入这个概率分布，我们就可以得出下图；颜色越深，表示这越是一个一般的玩家，颜色越浅，表示异常行为越显著。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703201402084.png" alt="image-20200703201402084" style="zoom:60%;"></p>
<p>如果现在有三个玩家$x=(x_1,x_2,x_3,x_4,x_5)^T$，表现出了不同的行为，根据模型算出来不同的结果。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703202451744.png" alt="image-20200703202451744" style="zoom:60%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Anomaly Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Attack and Defense</title>
    <url>/2020/06/26/Attack/</url>
    <content><![CDATA[<h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626095005396.png" alt="image-20200626095005396" style="zoom:50%;"></p>
<h4 id="What-do-we-want-to-do"><a href="#What-do-we-want-to-do" class="headerlink" title="What do we want to do?"></a>What do we want to do?</h4><p>attack要做的事就是把找到原图片$x^0$对应的$x’$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626095352498.png" alt="image-20200626095352498" style="zoom:50%;"></p>
<h4 id="Loss-Function-for-Attack"><a href="#Loss-Function-for-Attack" class="headerlink" title="Loss Function for Attack"></a>Loss Function for Attack</h4><p><strong>Training</strong>：network训练得出的结果$y^0$必须和$y^{true}$越接近越好，此时的loss function为$L_{train}(\theta)=C(y^0,y^{true})$，<u>输入的$x$是固定的</u>，需要不断调整$\theta$的值，使得$L_{train}(\theta)$取得最小值；</p>
<p><strong>Non-targeted Attack</strong>：如果我们需要attack一个network，此时network的输出$y’$和$y^{true}$应该越大越好，此时的loss function为$L(x’)=-C(y’,y^{true})$，前面多了一个负号。此时的<u>network中的参数$\theta$是固定的</u>，需要不断调整输入$x’$，使network的输出$y’$和$y^{true}$的差距尽量远；</p>
<p><strong>Targeted Attack</strong>：如果我们不仅想要$y’,y^{true}$之间的距离尽量远，还想使$y’,y^{false}$之间的距离尽量近，就需要使用targeted attack；此时的loss function为$L(x’)=-C(y’,y^{true})+C(y’,y^{false})$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626095914128.png" alt="image-20200626095914128" style="zoom:50%;"></p>
<p>不仅需要限制输出之间的差异，还需要限制输入$x^0,x’$之间的差异，只有这输入之间的差异d小于$\epsilon$，我们才可以认为$x’$是与$x^0$相似的，才达到了attack 一个network的目的，即使输入尽可能具有迷惑性，从而使网络输出错误的结果</p>
<h4 id="Constraint"><a href="#Constraint" class="headerlink" title="Constraint"></a>Constraint</h4><p>那么我们怎么计算d呢？</p>
<p>有以下两种主要的方法：</p>
<ul>
<li>L2-norm，为$x^0,x’$之间每个像素差异的平方和；</li>
<li>L-infinity，为$x^0,x’$之间每个像素差异的最大值</li>
</ul>
<p>如果我们改变图中的每个pixel，另外一幅图只改变其中一个pixel，使得这两者之间的L2-norm是一样的，但第二种方式得出的L-infinity更大</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626102113579.png" alt="image-20200626102113579" style="zoom:50%;"></p>
<h4 id="How-to-Attack"><a href="#How-to-Attack" class="headerlink" title="How to Attack"></a>How to Attack</h4><p>就像我们训练一个neural network一样，但需要训练的参数是$x’$，此时就需要找到一个参数$x^*$，来最小化$L(x’)$，限制条件是$d(x^0,x’)\leq\epsilon$</p>
<script type="math/tex; mode=display">
x^*=arg\mathop{min}_\limits {d(x^0,x')\leq\epsilon}L(x')</script><p>这里我们也使用了gradient descent算法，只是此时需要调整的参数变成了$x^t$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626103758330.png" alt="image-20200626103758330" style="zoom:50%;"></p>
<p>当$d(x^0,x^t)&gt;\epsilon$时，就需要更新这个参数了，使用$fix(x^t)$来更新</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626123032000.png" alt="image-20200626123032000" style="zoom:50%;"></p>
<p>更新的参数需要满足一定的条件，</p>
<ul>
<li>如果使用L2-norm，必须选择在差值在半径以内的参数，超过了这个半径，就设为$\epsilon $；</li>
<li>如果使用L-infinity，现在超过了这个方形的区域，就必须想办法把它拉回来，在y轴方向超过了$\epsilon$，就把值设为$\epsilon$；在x轴方向超过了$\epsilon$，就把值设为$\epsilon$</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626123713569.png" alt="image-20200626123713569" style="zoom:50%;"></p>
<h4 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h4><p>如果我们现在要attack一个network，其真实类别为Tiger cat，但attack之后的network认为这是star fish</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626161331898.png" alt="image-20200626161331898" style="zoom:50%;"></p>
<p>由于这两者之间的差异很小，很难识别，这里我们将差值x50来进行展示</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626161942541.png" alt="image-20200626161942541" style="zoom:50%;"></p>
<p>可能猫和猫之间是比较类似的，这里我们将attack的target设置为keyboard，该network认为这是keyboard的概率为0.98</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626162037439.png" alt="image-20200626162037439" style="zoom:50%;"></p>
<p>如果我们再对图片加入噪声，network认为这是Persian cat，再继续加入噪声，我们都快分辨不出这张图是一只猫了，network就认为这是fire screen</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626162150972.png" alt="image-20200626162150972" style="zoom:50%;"></p>
<p>我们假设$x_0$是在高维平面上的一个点，沿着任意方向随机移动，我们可以看到在接近$x^0$ 的时候，是 tiger cat（正确分类）的可能性是很高的，但如果再移动多一点，是Persian cat和Egyptian cat的可能性是很高的 </p>
<p>上述说的是随机方向进行移动，如果图片是225*225pixel的，那么就是5万多个高维的方向，现在我们选取其中几个特定的方向。在这几个特定的方向中，$x^0$可变化的范围就变得非常狭窄，$x^0$稍微变化一下，network输出为另一个类别（key board）的可能性就很高</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626163736373.png" alt="image-20200626163736373" style="zoom:50%;"></p>
<h4 id="Attack-Approaches"><a href="#Attack-Approaches" class="headerlink" title="Attack Approaches"></a>Attack Approaches</h4><ul>
<li><p>FGSM (<a href="https://arxiv.org/abs/1412.6572" target="_blank" rel="noopener">https://arxiv.org/abs/1412.6572</a>)</p>
</li>
<li><p>Basic iterative method (<a href="https://arxiv.org/abs/1607.02533" target="_blank" rel="noopener">https://arxiv.org/abs/1607.02533</a>) </p>
</li>
<li><p>L-BFGS (<a href="https://arxiv.org/abs/1312.6199" target="_blank" rel="noopener">https://arxiv.org/abs/1312.6199</a>)</p>
</li>
<li>Deepfool (<a href="https://arxiv.org/abs/1511.04599" target="_blank" rel="noopener">https://arxiv.org/abs/1511.04599</a>)</li>
<li>JSMA (<a href="https://arxiv.org/abs/1511.07528" target="_blank" rel="noopener">https://arxiv.org/abs/1511.07528</a>)</li>
<li>C&amp;W (<a href="https://arxiv.org/abs/1608.04644" target="_blank" rel="noopener">https://arxiv.org/abs/1608.04644</a>)</li>
<li>Elastic net attack (<a href="https://arxiv.org/abs/1709.04114" target="_blank" rel="noopener">https://arxiv.org/abs/1709.04114</a>)</li>
<li>Spatially Transformed (<a href="https://arxiv.org/abs/1801.02612" target="_blank" rel="noopener">https://arxiv.org/abs/1801.02612</a>) </li>
<li>One Pixel Attack (<a href="https://arxiv.org/abs/1710.08864" target="_blank" rel="noopener">https://arxiv.org/abs/1710.08864</a>)</li>
<li>…… only list a few</li>
</ul>
<p>虽然有很多方法都可以用来attack network，但这些方法的主要区别在于使用了不同的constrains，或者使用了不同的optimization methods</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626170309446.png" alt="image-20200626170309446" style="zoom:50%;"></p>
<p>我们先介绍一下<strong>FGSM</strong>，对于第一维，如果$\frac{\partial L}{\partial x_1}$是大于0的值，那么$\Delta x_1=sign(\frac{\partial L}{\partial x_1})=+1$；如果对于第二维，如果$\frac{\partial L}{\partial x_2}$是小于0的值，不管值多大，都得出$\Delta x_2=sign(\frac{\partial L}{\partial x_2})=-1$；即对于$x^0$的所有维，要么$+\epsilon$，要么$-\epsilon$，即可得到最好的结果$x^*$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626171523647.png" alt="image-20200626171523647" style="zoom:50%;"></p>
<p>FGSM使用L-infinity作为distance constrain，如果gradient指向左下角，那么$x^<em>$就在方框的右上角；如果gradient指向左上角，那么$x^</em>$就在方框的右下角；因此，在FGSM里面，我们只在意gradient的方向，不在意其具体的大小 </p>
<p>那么FGSM到底是怎么运作的呢？</p>
<p>我们可以看作FGSM是使用了非常大的一个learning rate，使x飞出了方形区域，由于L-infinity的限制，输出会被限制到方形区域内部，即$x^*$在方形区域的右上角</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626174020656.png" alt="image-20200626174020656" style="zoom:60%;"></p>
<h4 id="White-Box-v-s-Black-Box"><a href="#White-Box-v-s-Black-Box" class="headerlink" title="White Box v.s. Black Box"></a>White Box v.s. Black Box</h4><p>在之前的attack中，我们假设已经知道了network的参数$\theta$，目标是找到最优化的$x’$，这种attack就称为<strong>White Box</strong></p>
<p>但在大多数的情况中，我们都不知道network的参数，但也需要去attack这个network，这就是<strong>Black Box</strong></p>
<h4 id="Black-Box-Attack"><a href="#Black-Box-Attack" class="headerlink" title="Black Box Attack"></a>Black Box Attack</h4><p>如果我们现在已经知道了black network的training data，那么我们就可以用同样的training data来训练一个proxy network，再生成attacked object，如果能过成功attack新的proxy network，那么我们就可以把这个object也作为black network的输入，也可以attack成功</p>
<p>如果不能得到相应的training data，network如果是一个在线版本，我们可以输入大量的图片，得出相对应的分类结果，从而可以组合成相对应的训练资料，来得出proxy network</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626174930630.png" alt="image-20200626174930630" style="zoom:50%;"></p>
<p>也有一些实验数据证明黑箱攻击是有可能成功的，现在假设Black Box有五种，现在我们来训练proxy network，我们用ResNet-152生成的图片，如果black box的network也是ResNet-152，那么attack成功的几率就会非常高，表格中的4%表示系统辨识的准确率</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626175459885.png" alt="image-20200626175459885" style="zoom:50%;"></p>
<h4 id="Attack-in-the-Real-World"><a href="#Attack-in-the-Real-World" class="headerlink" title="Attack in the Real World"></a>Attack in the Real World</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626183115909.png" alt="image-20200626183115909" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626183139180.png" alt="image-20200626183139180" style="zoom:50%;"></p>
<h4 id="Defense"><a href="#Defense" class="headerlink" title="Defense"></a>Defense</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626183341651.png" alt="image-20200626183341651" style="zoom:50%;"></p>
<h5 id="Passive-defense"><a href="#Passive-defense" class="headerlink" title="Passive defense"></a>Passive defense</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626183453090.png" alt="image-20200626183453090" style="zoom:50%;"></p>
<p>这个filter可以是smoothing，对于原来输入的attack图片network认为其是keyboard，经过smoothing之后，network就认为是tiger cat了，是正确的分类结果</p>
<p>Q：那么为什么smoothing可以达到这种效果呢？</p>
<p>A：只有某几种方向上的的信号可以使attack成功。如果使用了smoothing这种filter，就把这几种信号改变了，那么attack就失效了 ；加上smoothing并不会伤害原来的图片，所以network仍然可以得出正确的结果</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626183523115.png" alt="image-20200626183523115" style="zoom:50%;"></p>
<p>根据这种思想，有学者就提出了feature squeeze</p>
<p>对于同一个input，我们先得出model的输出结果$Prediction_0$，再根据$Squeeze_1，Squeeze_2$得出结果$Prediction_1,Prediction_2$，如果$Prediction_0$和$Prediction_1,Prediction_2$之间的差值d很大，那么我们就可以认为input是来attack的</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626184313560.png" alt="image-20200626184313560" style="zoom:50%;"></p>
<p>还有另外一种方法</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626184847976.png" alt="image-20200626184847976" style="zoom:50%;"></p>
<h5 id="Proactive-defense"><a href="#Proactive-defense" class="headerlink" title="Proactive defense"></a>Proactive defense</h5><p>首先我们通过某种算法找出漏洞，找到相应的adversarial input，再把这些input和之前的training data一起作为新的input data，输入network，相当于进行了data augmentation，这个过程进行T次，每次的input data都是不一样的</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200626185155952.png" alt="image-20200626185155952" style="zoom:50%;"></p>
<p>如果attacker知道了我们是使用算法A来进行模拟，那么attacker可以使用算法B来进行attack，那么我们的network并不能抵御这种attack</p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Attack and Defense</tag>
      </tags>
  </entry>
  <entry>
    <title>Backpropagation</title>
    <url>/2020/06/06/Backpropagation/</url>
    <content><![CDATA[<h4 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h4><p>这里我们再回顾一下gradient descent，对于网络中的参数$w_1,w_2,…,b_1,b_2,…$，通过求出相对应的梯度$\Delta L(\theta)$，再根据梯度更新网络结构中的参数</p>
<script type="math/tex; mode=display">
\theta ^i=\theta^{i-1}-\eta\Delta L(\theta^{i-1})</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606200029374.png" alt="image-20200606200029374" style="zoom:50%;"></p>
<h4 id="Chain-Rule"><a href="#Chain-Rule" class="headerlink" title="Chain Rule"></a>Chain Rule</h4><p>backpropagation的核心思想就是链式法则</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606200855314.png" alt="image-20200606200855314" style="zoom:50%;"></p>
<h4 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h4><p>这里我们先定义了一个loss函数，$l^n(\theta)$表示training data中$y^n$和$\hat y ^n$之间的loss，这个loss可以通过cross entropy或者MSE计算，再将所有的loss进行求和，得到$L(\theta)$</p>
<script type="math/tex; mode=display">
L(\theta)=\sum_{n=1}^Nl^n(\theta)</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606200827557.png" alt="image-20200606200827557" style="zoom:50%;"></p>
<p>对w求导，可得$\frac{\partial l^n}{\partial w}=\frac{\partial z}{\partial w}\frac{\partial l}{\partial z}$，这里我们截取了网络中的部分结构，神经网络的forward过程计算$\frac{\partial z}{\partial w}$，backward过程计算$\frac{\partial l}{\partial z}$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606201320644.png" alt="image-20200606201320644" style="zoom:50%;"></p>
<h4 id="Forward-pass"><a href="#Forward-pass" class="headerlink" title="Forward pass"></a>Forward pass</h4><p>forward过程计算$\frac{\partial z}{\partial w}$，即为权重所对应的上一层神经元的值(x1,x2)</p>
<script type="math/tex; mode=display">
z=x_1w_1+x_2+w_2+b \\
\frac{\partial z}{\partial w_1}=x_1,\quad\frac{\partial z}{\partial w_2}=x_2</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606201854092.png" alt="image-20200606201854092" style="zoom:50%;"></p>
<p>对于forward过程，计算出上层神经元的值后，才可以继续计算下一层神经元的梯度$\frac{\partial z}{\partial w}$，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606202207055.png" alt="image-20200606202207055" style="zoom:50%;"></p>
<h4 id="Backward-pass"><a href="#Backward-pass" class="headerlink" title="Backward pass"></a>Backward pass</h4><h5 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h5><p>backward过程计算$\frac{\partial l}{\partial z}$，z为激活函数$\sigma (z)$的输入值。这里我们令$a=\sigma(z)$，简化表达式的形式，根据chain rule</p>
<script type="math/tex; mode=display">
\frac{\partial l}{\partial z}=\frac{\partial a}{\partial z}\frac{\partial l}{\partial a}</script><p>其中$\frac{\partial a}{\partial z}=\sigma’(z)$，对激活函数求一阶导数，可以很轻松地表示出来</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606203500895.png" alt="image-20200606203500895" style="zoom:50%;"></p>
<p>接下来我们开始$\frac{\partial l}{\partial a}$的计算，a会影响$z’,z’’$，因此可以再次使用链式法则</p>
<script type="math/tex; mode=display">
\frac{\partial l}{\partial a}=\frac{\partial z'}{\partial a}\frac{\partial l}{\partial z'}
+ \frac{\partial z''}{\partial a}\frac{\partial l}{\partial z''}</script><p>由于$z’=aw_3+…,\quad z’’=aw_4+…$，那么我们可以得出</p>
<script type="math/tex; mode=display">
\frac{\partial z'}{\partial a}=w_3,\quad\frac{\partial z''}{\partial a}=w_4</script><p>代入原式，可得</p>
<script type="math/tex; mode=display">
\frac{\partial l}{\partial z}=\frac{\partial a}{\partial z}\frac{\partial l}{\partial a} =\sigma'(z)\left[w_3\frac{\partial l}{\partial z'}+ w_4\frac{\partial l}{\partial z''}\right]</script><p>对于$\frac{\partial l}{\partial z’},\frac{\partial l}{\partial z’’}$，我们假设可以通过某种方式求得他们的值</p>
<h5 id="另一个观点"><a href="#另一个观点" class="headerlink" title="另一个观点"></a>另一个观点</h5><p>我们可以从这个图中更加直观地了解backpropagation的过程，这里我们假设有一个神经元（图中的三角形），它不在原来的网络结构中，可以通过$w_3\frac{\partial l}{\partial z’}+ w_4\frac{\partial l}{\partial z’’}$来计算，<u>前面再乘上一个放大系数$\sigma’(z)$，这个放大系数的值是根据forward过程计算的，是一个常数</u>，就可以得出$\frac{\partial l}{\partial z}$的值</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606204707606.png" alt="image-20200606204707606" style="zoom:50%;"></p>
<h5 id="两种情况"><a href="#两种情况" class="headerlink" title="两种情况"></a>两种情况</h5><p>公式内的其他项都已经计算出来，还有$\frac{\partial l}{\partial z’},\frac{\partial l}{\partial z’’}$没有得出具体的表达式，此步骤是为了求解$\frac{\partial l}{\partial z’},\frac{\partial l}{\partial z’’}$的表达式</p>
<p><strong>Case1：Output Layer</strong></p>
<p>为了求出$\frac{\partial l}{\partial z’},\frac{\partial l}{\partial z’’}$，这里再次使用了chain rule</p>
<script type="math/tex; mode=display">
\frac{\partial l}{\partial z'}=\frac{\partial y_1}{\partial z'}\frac{\partial l}{\partial y_1}</script><p>其中$\frac{\partial y_1}{\partial z’}$就是激活函数的输出值再对$z’$求导；</p>
<p>$\frac{\partial l}{\partial y_1}$为相对应的loss函数对y1求导，这个loss函数可以是cross entropy，也可以是MSE，对于不同的loss函数，计算出来的导数也不同</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606213905635.png" alt="image-20200606213905635" style="zoom:50%;"></p>
<p><strong>Case2: Not output layer</strong></p>
<p>如果现在假设后层不是output layer，而是hidden layer中的其中一层，计算方式就发生了一些小小的变化</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606215916597.png" alt="image-20200606215916597" style="zoom:50%;"></p>
<p>根据Case1的推导，知道了$\frac{\partial l}{\partial z’},\frac{\partial l}{\partial z’’}$之后，就可以对$\frac{\partial l}{\partial z’}$进行求解</p>
<p>同理可得，知道了$\frac{\partial l}{\partial z_a},\frac{\partial l}{\partial z_b}$，就可以对$\frac{\partial l}{\partial z’}$求解，如下图所示，$\frac{\partial l}{\partial z_a},\frac{\partial l}{\partial z_b}$分别乘上对应的权重$w_5,w_6$，前面再乘一个放大系数，就可得出$\frac{\partial l}{\partial z’}$的表达式</p>
<script type="math/tex; mode=display">
\frac{\partial l}{\partial z'}=\sigma'(z')\left[w_5\frac{\partial l}{\partial z_a}+ w_6\frac{\partial l}{\partial z_b}\right]</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606221608597.png" alt="image-20200606221608597" style="zoom:50%;"></p>
<p>知道了$z’,z’’$的值后，就可以根据公式计算z的值；知道了$z_a,z_b$之后，就可以计算$z’$ 的值；……一直循环这个步骤，直到到达output layer为止</p>
<p>下图中有6个neural，分别是$z_1,z_2,z_3,z_4,z_5,z_6$，为激活函数的输入值，现在要分别求得$l$对这些函数的偏微分$\frac{\partial l}{\partial z_i}$。按照我们之前的做法，要求$\frac{\partial l}{\partial z_1}$，就必须要求$\frac{\partial l}{\partial z_3},\frac{\partial l}{\partial z_4}$，而要求$\frac{\partial l}{\partial z_3}，\frac{\partial l}{\partial z_4}$的值，就必须要分别求两次$\frac{\partial l}{\partial z_5},\frac{\partial l}{\partial z_6}$的值，计算效率很低</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606222718315.png" alt="image-20200606222718315" style="zoom:50%;"></p>
<p>如果我们先计算$\frac{\partial l}{\partial z_5},\frac{\partial l}{\partial z_6}$，就可以接着计算出$\frac{\partial l}{\partial z_3},\frac{\partial l}{\partial z_4}$的值，再计算出$\frac{\partial l}{\partial z_1},\frac{\partial l}{\partial z_2}$的值，计算效率可以提高很多</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606222732356.png" alt="image-20200606222732356" style="zoom:50%;"></p>
<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>神经网络的forward过程计算$\frac{\partial z}{\partial w}$，backward过程计算$\frac{\partial l}{\partial z}$，再相乘，就可以得出loss函数对每个hidden layer神经元的梯度，</p>
<script type="math/tex; mode=display">
\frac{\partial l}{\partial w}=\frac{\partial z}{\partial w}\frac{\partial l}{\partial z}</script><p>代入每次参数更新的公式，就可以得出每次的参数更新结果</p>
<script type="math/tex; mode=display">
w ^i=w^{i-1}-\eta\Delta L(w^{i-1})</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606224217277.png" alt="image-20200606224217277" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>backpropagation</tag>
      </tags>
  </entry>
  <entry>
    <title>CCF-JSON查询</title>
    <url>/2017/12/02/CCF-JSON%E6%9F%A5%E8%AF%A2/</url>
    <content><![CDATA[<p>试题编号：   201709-3<br>试题名称：   JSON查询<br>时间限制：   1.0s<br>内存限制：   256.0MB<br>问题描述：<br>　　JSON (JavaScript Object Notation) 是一种轻量级的数据交换格式，可以用来描述半结构化的数据。JSON 格式中的基本单元是值 (value)，出于简化的目的本题只涉及 2 种类型的值：<br>　　&#42; 字符串 (string)：字符串是由双引号 “ 括起来的一组字符（可以为空）。如果字符串的内容中出现双引号 “，在双引号前面加反斜杠，也就是用 \” 表示；如果出现反斜杠 \，则用两个反斜杠 \\ 表示。反斜杠后面不能出现 “ 和 \ 以外的字符。例如：””、”hello”、”\”\\”。<br>　　&#42; 对象 (object)：对象是一组键值对的无序集合（可以为空）。键值对表示对象的属性，键是属性名，值是属性的内容。对象以左花括号 { 开始，右花括号 } 结束，键值对之间以逗号 , 分隔。一个键值对的键和值之间以冒号 : 分隔。键必须是字符串，同一个对象所有键值对的键必须两两都不相同；值可以是字符串，也可以是另一个对象。例如：{}、{“foo”: “bar”}、{“Mon”: “weekday”, “Tue”: “weekday”, “Sun”: “weekend”}。<br>　　除了字符串内部的位置，其他位置都可以插入一个或多个空格使得 JSON 的呈现更加美观，也可以在一些地方换行，不会影响所表示的数据内容。例如，上面举例的最后一个 JSON 数据也可以写成如下形式。<br>　　{<br>　　“Mon”: “weekday”,<br>　　“Tue”: “weekday”,<br>　　“Sun”: “weekend”<br>　　}<br>　　给出一个 JSON 格式描述的数据，以及若干查询，编程返回这些查询的结果。<br>输入格式<br>　　第一行是两个正整数 n 和 m，分别表示 JSON 数据的行数和查询的个数。<br>　　接下来 n 行，描述一个 JSON 数据，保证输入是一个合法的 JSON 对象。<br>　　接下来 m 行，每行描述一个查询。给出要查询的属性名，要求返回对应属性的内容。需要支持多层查询，各层的属性名之间用小数点 . 连接。保证查询的格式都是合法的。<br>输出格式<br>　　对于输入的每一个查询，按顺序输出查询结果，每个结果占一行。<br>　　如果查询结果是一个字符串，则输出 STRING <string>，其中 <string> 是字符串的值，中间用一个空格分隔。<br>　　如果查询结果是一个对象，则输出 OBJECT，不需要输出对象的内容。<br>　　如果查询结果不存在，则输出 NOTEXIST。</string></string></p>
<p>样例输入<br>10 5<br>{<br>“firstName”: “John”,<br>“lastName”: “Smith”,<br>“address”: {<br>“streetAddress”: “2ndStreet”,<br>“city”: “NewYork”,<br>“state”: “NY”<br>},<br>“esc\\aped”: “\”hello\””<br>}<br>firstName<br>address<br>address.city<br>address.postal<br>esc\aped<br>样例输出<br>STRING John<br>OBJECT<br>STRING NewYork<br>NOTEXIST<br>STRING “hello”<br>评测用例规模与约定<br>　　n ≤ 100，每行不超过 80 个字符。<br>　　m ≤ 100，每个查询的长度不超过 80 个字符。<br>　　字符串中的字符均为 ASCII 码 33-126 的可打印字符，不会出现空格。所有字符串都不是空串。<br>　　所有作为键的字符串不会包含小数点 .。查询时键的大小写敏感。<br>　　50%的评测用例输入的对象只有 1 层结构，80%的评测用例输入的对象结构层数不超过 2 层。举例来说，{“a”: “b”} 是一层结构的对象，{“a”: {“b”: “c”}} 是二层结构的对象，以此类推。<br>  <strong>思路</strong>：加一些标识位，用vector数据类型来存储键值</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">string</span> json = <span class="string">""</span>;</span><br><span class="line">    <span class="keyword">int</span> n, m;</span><br><span class="line">    <span class="built_in">cin</span>&gt;&gt;n&gt;&gt;m;</span><br><span class="line">    getchar();</span><br><span class="line">    <span class="keyword">while</span> (n--) &#123;</span><br><span class="line">        <span class="built_in">string</span> temp;</span><br><span class="line">        getline(<span class="built_in">cin</span>, temp);</span><br><span class="line">        json += temp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt; all;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; pre;</span><br><span class="line">    <span class="built_in">string</span> preHost;</span><br><span class="line">    <span class="keyword">int</span> iskey = <span class="literal">true</span>, isvalue = <span class="literal">false</span>, isstring = <span class="literal">false</span>, blank = <span class="literal">false</span>;</span><br><span class="line">    <span class="built_in">string</span> key, value;</span><br><span class="line">    <span class="built_in">string</span> now_key;</span><br><span class="line">    <span class="built_in">string</span> temp_key;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; json.length(); i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (json[i] == <span class="string">' '</span>) blank = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">else</span> blank = <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">if</span> (json[i]==<span class="string">':'</span>) &#123;</span><br><span class="line">            isvalue = <span class="literal">true</span>;</span><br><span class="line">            iskey = <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (isvalue &amp;&amp; json[i]==<span class="string">'&#123;'</span>) &#123;</span><br><span class="line">            isstring = <span class="literal">false</span>;</span><br><span class="line">            pre.push_back(key);</span><br><span class="line">            temp_key = <span class="string">""</span>;</span><br><span class="line">            <span class="keyword">if</span> (pre.size()&gt;<span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; pre.size()<span class="number">-1</span>; j++) &#123;</span><br><span class="line">                    temp_key += pre[j];</span><br><span class="line">                    temp_key += <span class="string">"."</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                temp_key += pre[pre.size()<span class="number">-1</span>];</span><br><span class="line">            &#125;</span><br><span class="line">            all[temp_key] = <span class="string">"&#123;"</span>;</span><br><span class="line">            preHost += key;</span><br><span class="line">            preHost += <span class="string">"."</span>;</span><br><span class="line">            key = <span class="string">""</span>;</span><br><span class="line">            iskey = <span class="literal">true</span>;</span><br><span class="line">            isvalue = <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (json[i]==<span class="string">'&#125;'</span>) &#123;</span><br><span class="line">            isvalue = <span class="literal">false</span>;</span><br><span class="line">            iskey = <span class="literal">true</span>;</span><br><span class="line">            i++;</span><br><span class="line">            temp_key = <span class="string">""</span>;</span><br><span class="line">            <span class="keyword">if</span> (pre.size()&gt;<span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; pre.size(); j++) &#123;</span><br><span class="line">                    temp_key += pre[j];</span><br><span class="line">                    temp_key += <span class="string">"."</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            temp_key+=key;</span><br><span class="line">            all[temp_key] = value;</span><br><span class="line"></span><br><span class="line">            preHost = <span class="string">""</span>;</span><br><span class="line">            key = <span class="string">""</span>; value = <span class="string">""</span>;</span><br><span class="line">            <span class="keyword">if</span> (pre.size()&gt;<span class="number">0</span>)</span><br><span class="line">                pre.pop_back();</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (isvalue &amp;&amp; json[i]==<span class="string">','</span>) &#123;</span><br><span class="line">            isvalue = <span class="literal">false</span>;</span><br><span class="line">            iskey = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">            temp_key = <span class="string">""</span>;</span><br><span class="line">            <span class="keyword">if</span> (pre.size()&gt;<span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; pre.size(); j++) &#123;</span><br><span class="line">                    temp_key += pre[j];</span><br><span class="line">                    temp_key += <span class="string">"."</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            temp_key+=key;</span><br><span class="line">            all[temp_key] = value;</span><br><span class="line"></span><br><span class="line">            key = <span class="string">""</span>; value = <span class="string">""</span>;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (isvalue &amp;&amp; json[i]==<span class="string">'"'</span>) &#123;</span><br><span class="line">            isstring = <span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (iskey &amp;&amp; json[i]==<span class="string">'"'</span>) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">if</span> ((json[i]==<span class="string">'\\'</span>&amp;&amp;json[i+<span class="number">1</span>]==<span class="string">'"'</span>) || (json[i]==<span class="string">'\\'</span>&amp;&amp;json[i+<span class="number">1</span>]==<span class="string">'\\'</span>)) i++;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (iskey &amp;&amp; !blank) key+=json[i];</span><br><span class="line">        <span class="keyword">if</span> (isvalue &amp;&amp; isstring &amp;&amp; !blank) value+=json[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">        <span class="built_in">string</span> temp;</span><br><span class="line">        getline(<span class="built_in">cin</span>, temp);</span><br><span class="line">        <span class="keyword">if</span> (all[temp]==<span class="string">""</span>) <span class="built_in">cout</span>&lt;&lt;<span class="string">"NOTEXIST"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (all[temp]==<span class="string">"&#123;"</span>) <span class="built_in">cout</span>&lt;&lt;<span class="string">"OBJECT"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="built_in">cout</span>&lt;&lt;<span class="string">"STRING "</span>&lt;&lt;all[temp]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>CCF Answer</category>
      </categories>
      <tags>
        <tag>CCF</tag>
      </tags>
  </entry>
  <entry>
    <title>Conditional GAN (CGAN)</title>
    <url>/2020/07/05/CGAN/</url>
    <content><![CDATA[<p>对于一般的GAN，都是随机生成一个vector，然后输入GAN，再生成一张image，但我们并不能控制output。对于本文要讲述的CGAN，我们则可以操控其输出的结果。</p>
<h4 id="Text-to-Image"><a href="#Text-to-Image" class="headerlink" title="Text-to-Image"></a>Text-to-Image</h4><p>对于传统的监督学习的方法，训练数据集是一些带有描述的图片，网络的input是一段文字，output则是一张图片，我们希望输出的图片和target越接近越好。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705145505825.png" alt="image-20200705145505825" style="zoom:60%;"></p>
<p>如果现在网络的输入文字是“train”，网络会觉得正面的火车是对的，侧面的火车也是对的，网络最后会取这些值的一个平均值，因此会得到一个非常模糊的火车图片。</p>
<h4 id="Conditional-GAN"><a href="#Conditional-GAN" class="headerlink" title="Conditional GAN"></a>Conditional GAN</h4><p>现在我们使用CGAN来完成这个任务。CGAN现在的generator的输入不仅有z，还有一个condition c（“train”），G的输出为$x=G(z,c)$；</p>
<p>这时我们还使用原来的Discriminator，把x输入D，D可以对其进行评价分数scalar，对真实的图像输出为1，生成的图像输出为0。</p>
<p>那么现在就出现了一个新问题，G可以完全不管输入的condition，只生成高质量的、接近真实的图像即可。比如我们把condition设置为“dog”，G如果生成一只猫的图像，这个图像很接近真实图像，那么就可以骗过discriminator。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705150242620.png" alt="image-20200705150242620" style="zoom:60%;"></p>
<p>因此，这里的discriminator也要做出改变，需要把condition也作为D的其中一个输入。</p>
<p>那么现在D输出的分数就有两部分组成：（1）x的真实性；（2）x是不是满足condition的条件。如果这个图片和文字是match的，而且图片很接近真实图像，D就会给这个图像一个高分。</p>
<p>给低分0的两个case：（1）如果给出了正确的文字，但生成了模糊的图像；（2）虽然生成了清晰的图像，但和随机输入的文字（condition）是不匹配的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705151419726.png" alt="image-20200705151419726" style="zoom:60%;"></p>
<p>这里是具体的算法。</p>
<p>Learning D：输入是文字和图像的pair，即$\{(c^1,x^1),…,(c^m,x^m)\}$，从（高斯）分布中取出noise $\{z^1,z^2…,z^m\}$，输入G，得到生成的图像 $\tilde x^i=G(c^i,z^i)$；从database中取出m个真实的图像数据 $\{\hat x^1,…,\hat x^m\}$，再输入discriminator D，不断调整$\theta_d$，使得得到的分数越大越好，</p>
<script type="math/tex; mode=display">
\tilde V =\frac{1}{m}\sum_{i=1}^m logD(c^i,x^i)+\frac{1}{m}\sum_{i=1}^mlog(1-D(c^i,\tilde x^i))
+ \frac{1}{m}\sum_{i=1}^mlog(1-D(c^i,\hat x^i))</script><p>其中$D(c^i,x^i)$表示真实图像所得到的分数，D的目标就是使真实图像获得的分数越大越好；而$D(c^i,\tilde x^i)$表示G生成的图像所得到的分数，应该越小越好，所以前面加了负号；而$D(c^i,\hat x^i)$表示生成了清晰的图像，但和condition不匹配，所以前面加了负号。后两个case都是给低分的情况。</p>
<p>其他过程和之前的GAN差别不大，主要是计算低分数的情况多了一个case。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705153051152.png" alt="image-20200705153051152" style="zoom:67%;"></p>
<h4 id="Conditional-GAN-Discriminator"><a href="#Conditional-GAN-Discriminator" class="headerlink" title="Conditional GAN - Discriminator"></a>Conditional GAN - Discriminator</h4><p>对于一般的discriminator架构，输入为图像和文字，输出为两部分的分数（x的真实性、x和condition是不是match）。如果这个图片和文字是match的，而且图片很接近真实图像，D就会给这个图像一个高分。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705154525712.png" alt="image-20200705154525712" style="zoom:60%;"></p>
<p>有学者在其他论文中也提出了其他架构，而且效果还不错。首先有object输入<span style="color: green">network</span>，得出第一个分数，表示x是否真实；<span style="color: green">network</span>还会输出一个embedding，再和condition结合，输入<span style="color: blue">network</span>后得到另外一个分数，表示x和c是否match。</p>
<p>选择第一种架构方法有一个缺点。前文我们提到CGAN会给低分的两种case：x和condition是match的，但生成的图像不好；x和condition是不match的，但生成的图像质量高。如果我们使用第一种架构，网络会比较confused，网络并不知道分数低的原因到底是哪一种case，有可能是图像不够realistic，也有可能是和condition不够match。但<span style="color: red">对于第二种架构，网络则可以清楚地知道到底是因为哪个原因导致的低分。</span></p>
<p>下面是结果的展示，输入文字condition，输出对应的图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705191337832.png" alt="image-20200705191337832" style="zoom:60%;"></p>
<h4 id="Stack-GAN"><a href="#Stack-GAN" class="headerlink" title="Stack GAN"></a>Stack GAN</h4><p>首先输入一段文字，这些文字先经过embedding的过程，再经过conditioning augmentation的过程（加入噪声），输入generator之后，会生成一张图像（$64\times 64$）；discriminator再来判断这个生成的image和输入的文字是不是match的；</p>
<p>这里还有第二个generator，输入为刚才$64\times64$的图像和文字的embedding结果；再生成一张$256\times256$的图像；再把新生成的图像输入discriminator，看到底是不是realistic。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705191647515.png" alt="image-20200705191647515" style="zoom: 100%;"></p>
<h4 id="Image-to-image"><a href="#Image-to-image" class="headerlink" title="Image-to-image"></a>Image-to-image</h4><h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>CGAN不仅可以输入一段文字产生一张图像，也可以是image-to-image，比如把简单的几何模型转化为真实的房屋模型，把灰度图转化为彩色图，</p>
<p><img src="/2020/07/05/CGAN/image-20200705193817063.png" alt="image-20200705193817063"></p>
<p>如果使用传统的监督学习的方法来完成这个任务，首先需要收集训练数据（几何模型图、对应的真实房屋图），训练network，output是一张图片，我们希望输出的图片和target越接近越好。由于network会产生多种多样的房子，最后的结果会取一个平均值，因此会产生一个非常模糊的图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705194028821.png" alt="image-20200705194028821" style="zoom:60%;"></p>
<p>我们可以使用GAN来完成这个任务。首先把从distribution中sample出来的z和结构图作为G的输入，G会生成一张新的Image；再把Image和原来的结构图（一个pair）输入D，会输出一个分数scalar。可以发现GAN生成的图要相对清晰很多。</p>
<p>但我们这时发现了一个新问题，GAN产生了一些原来的结构图中没有的东西，比如在图的左上角，产生了一个像是窗户或者天线的东西。这时我们可以加入一个新的constrain（真实的房屋图），希望generator产生的image和训练数据集中对应的图像也越靠近越好。</p>
<p>这时generator的目标就有两个：产生出足够清晰可以骗过D的图像；产生的新图像和原来的target要接近。这样就会产生结果很好的图（GAN+close），图足够清晰，也不会产生一些奇怪的东西。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705195049882.png" alt="image-20200705195049882" style="zoom:60%;"></p>
<h5 id="Patch-GAN"><a href="#Patch-GAN" class="headerlink" title="Patch GAN"></a>Patch GAN</h5><p>现在要生成的是一张很大的图像，如果这时discriminator的输入还是一整张大的图像，那么D要对其进行评分，就肯定需要更多的参数，很有可能产生overfiting或者训练所需的时间会很长。</p>
<p>在这篇论文中，作者也对discriminator的设计进行了变化。discriminator只需要检查图中的一小部分，对这一小块图片来输出评价分数 。区域的大小也是需要调整的参数之一。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705200803367.png" alt="image-20200705200803367" style="zoom:60%;"></p>
<h4 id="Speech-Enhancement"><a href="#Speech-Enhancement" class="headerlink" title="Speech Enhancement"></a>Speech Enhancement</h4><p>如果我们使用传统的深度学习方法来做语音增强，首先需要把纯净语音加上一些noise再输入CNN，不断地训练CNN，使其能够输出去噪后的纯净语音。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705201419793.png" alt="image-20200705201419793" style="zoom:60%;"></p>
<p>直接产生同样会有语音频谱图比较模糊的情况，我们在这里也可以使用CGAN算法。</p>
<p>输入为带noise的语音信号，G的输出为增强语音，增强语音和纯净语音之间应该越接近越好。discriminator的输入为增强语音和带噪语音，输出评价分数，看这个output是不是clean的，还要看output和noise这个pair是不是match的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705201744963.png" alt="image-20200705201744963" style="zoom:60%;"></p>
<h4 id="Video-Generation"><a href="#Video-Generation" class="headerlink" title="Video Generation"></a>Video Generation</h4><p>输入一段video，让generator预测下一步会发生什么，产生对应的video；discriminator要同时考虑generator的input和output，可以把它们接到一起，变成一段完整的影片，让discriminator来判断到底是不是一个合理的影片。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705202357625.png" alt="image-20200705202357625" style="zoom:60%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Conditional GAN</tag>
        <tag>CGAN</tag>
        <tag>Speech Enhancement</tag>
        <tag>Video Generation</tag>
        <tag>Patch GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>Convolutional Neural Network</title>
    <url>/2020/06/08/CNN/</url>
    <content><![CDATA[<h4 id="Why-CNN-for-Image"><a href="#Why-CNN-for-Image" class="headerlink" title="Why CNN for Image"></a>Why CNN for Image</h4><p>可以使用相同的参数来提取不同图像中的同一种特征，这样就可以降低网络需要学习的参数</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608170112931.png" alt="image-20200608170112931" style="zoom:50%;"></p>
<p>CNN还有另外一种操作可以减少网络需要训练的参数，即下采样（subsampling），subsampling可以减少图像的大小。采样层就是使用pooling的技术来实现的，可以用max pooling或average pooling，获取某个像素点及其周围区域的最大值或平均值，将这些像素都用一个像素来表示，就可以缩小图像的大小。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608170313635.png" alt="image-20200608170313635" style="zoom:50%;"></p>
<p>一个完整的CNN网络结构，由多个convolution和pooling层、以及全连接层组成。输入图像先经过多次的convolution、pooling，提取图像中的特征，再把这些特征flatten成一个一维的向量，即全连接层，最后再得出分类结果 cat or dog</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608170601092.png" alt="image-20200608170601092" style="zoom:50%;"></p>
<h4 id="convolution"><a href="#convolution" class="headerlink" title="convolution"></a>convolution</h4><p>对于我们想要提取图像中的两个特征，我们使用filter1和filter2这两个过滤器，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608172139320.png" alt="image-20200608172139320" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608172200059.png" alt="image-20200608172200059" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608172256317.png" alt="image-20200608172256317" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608172311889.png" alt="image-20200608172311889" style="zoom:50%;"></p>
<h4 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h4><h5 id="local-field"><a href="#local-field" class="headerlink" title="local field"></a>local field</h5><p>下一层的neural 3，只与前层的9个neural相连接，而不是和前层的全部neural连接</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608172513373.png" alt="image-20200608172513373" style="zoom:50%;"></p>
<h5 id="weight-sharing"><a href="#weight-sharing" class="headerlink" title="weight sharing"></a>weight sharing</h5><p>对于图像的同一种特征，只需要一个过滤器filter即可，这样就可以进一步减少图像中的参数</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608173532837.png" alt="image-20200608173532837" style="zoom:50%;"></p>
<h4 id="Flatten"><a href="#Flatten" class="headerlink" title="Flatten"></a>Flatten</h4><p>每经过一次convolution和pooling操作，都会生成一张新的图片，这张新图片的大小比原图要小很多，减少了需要训练的参数</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608173901231.png" alt="image-20200608173901231" style="zoom:50%;"></p>
<p>再把经过多次convolution和pooling的图像flatten，展开成一维的向量，再输入全连接层</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608174020284.png" alt="image-20200608174020284" style="zoom:50%;"></p>
<h4 id="CNN-in-Keras"><a href="#CNN-in-Keras" class="headerlink" title="CNN in Keras"></a>CNN in Keras</h4><p>首先先介绍一下convolution和pooling对图像大小变化的公式，设输入图像的宽度和高度分别为w和h，卷积核大小为$F\times F$，步长stride大小为S，如果再加入Padding操作，经过卷积或池化操作后图像的大小为W和H</p>
<script type="math/tex; mode=display">
H/W=\frac{(h/w - F+2P)}{S}+1</script><p>这里引入了keras，先介绍几个主要函数的参数。</p>
<p>我们可以先通过以下函数来生成一个convolution layer</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model2.add(Convolution2D(<span class="number">25</span>,<span class="number">3</span>,<span class="number">3</span>, input_shape=(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)))</span><br></pre></td></tr></table></figure>
<p>其中25表示filter的数量，$3\times3$表示filter的大小，<code>model2.add()</code>其中的<code>Convolution2D</code>还有一个参数input_shape，表示函数输入的图像大小，例子中表示一个单通道的$28\times28$图像，如果需要输入彩色图像，则是$28\times28\times3$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608174611511.png" alt="image-20200608174611511" style="zoom:50%;"></p>
<p>下面我们将继续介绍通过convolution和pooling操作后参数数量的变化。对于下图中的例子，我们先考虑输入$28\times28$大小的图像，即$w=28,h=28$，这里默认步长为1，padding为0，经过$3\times3$卷积核大小的过滤，图像的大小就变为$26\times26$</p>
<script type="math/tex; mode=display">
\frac{28-3+2*0}{1}+1=26</script><p>由于输入的图像只有1个channel，卷积核大小为$3\times3$，因此第一层只有9个参数</p>
<p>由于前一层有25个filter，经过一次convolution操作后，图像大小为$26\times26$，本层的neural个数为$25\times26\times26$；还需要再进行一次pooling操作，经过pooling操作之后图像大小为$13\times13$，max pooling操作没有filter，不算参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model2.add(MaxPooling2D((<span class="number">2</span>,<span class="number">2</span>)))</span><br></pre></td></tr></table></figure>
<p>pooling操作后图像宽度的大小为，</p>
<script type="math/tex; mode=display">
\frac{26-2+2*0}{2}+1=13</script><p>经过一次convolution和pooling操作之后，neural个数为$25\times13\times13$；</p>
<p>对于第二次的convolution操作，有50个大小为$3\times3$的filter，输出的neural数量为$50\times11\times11$；一次pooling操作所包含的区域大小为$2\times2$，输出的neural数量为$50\times5\times5$</p>
<script type="math/tex; mode=display">
\frac{13-3+2*0}{1}+1=11\\
\frac{11-2+2*0}{2}+1=5</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608175330480.png" alt="image-20200608175330480" style="zoom:50%;"></p>
<p>经过两次的convolution和pooling操作之后，neural的数量为$50\times5\times5$，下一步操作就是将这些神经元flatten，展开成一维向量；再输入激活函数和全连接层</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200608202515115.png" alt="image-20200608202515115" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Classification</title>
    <url>/2020/06/05/Classification/</url>
    <content><![CDATA[<p>考虑到宝可梦的两个属性（<strong>Defense</strong>、<strong>SP Defense</strong>），将输入的宝可梦进行属性分类（Water、Normal）</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605204445258.png" alt="image-20200605204445258" style="zoom:50%;"></p>
<h4 id="Gaussian-Distribution"><a href="#Gaussian-Distribution" class="headerlink" title="Gaussian Distribution"></a>Gaussian Distribution</h4><p>假设图中的点服从高斯分布，由于只考虑了两个属性，$\mu$为一个二维向量，则有高斯分布公式，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605202837571.png" alt="image-20200605202837571" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605204129951.png" alt="image-20200605204129951" style="zoom:50%;"></p>
<h4 id="Maximum-Likelihood"><a href="#Maximum-Likelihood" class="headerlink" title="Maximum Likelihood"></a>Maximum Likelihood</h4><p>第i个example的概率密度函数为$f_{\mu,\sum}(x^i)$，可得出最大似然函数$L(\mu,\sum)$的表达式，其中$\mu=\mu^<em>,\sum=\sum^</em>$时，函数L得到最大值</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605203041766.png" alt="image-20200605203041766" style="zoom:50%;"></p>
<p>这里我们假设有两个类别，其参数和分布如下，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605210600803.png" alt="image-20200605210600803" style="zoom:50%;"></p>
<p>输入样例x是Class 1:Water的概率为，</p>
<script type="math/tex; mode=display">
P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x)}=\frac{P(x|C_1)P(C_1)}
{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}</script><p>再代入相应的表达式，其中$P(x|C_1)=f_{\mu^1,\sum^1},P(x|C_2)=f_{\mu^2,\sum^2}$，即可算出x为C1的概率，如果算出这个概率大于0.5，我们就可以认为x的属性为C1</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605212347236.png" alt="image-20200605212347236" style="zoom:50%;"></p>
<p>但这样子算出来的分类精确度很低，只有47%，就算加入其他的属性，精确度也只提高到了54%</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605214028329.png" alt="image-20200605214028329" style="zoom:50%;"></p>
<h4 id="Modifying-Model"><a href="#Modifying-Model" class="headerlink" title="Modifying Model"></a>Modifying Model</h4><p>由于上面模型的精确度都不高，所以在此我们对模型进行了修改，模型的参数就只有三个，$\mu^1,\mu^2,\sum=\sum^1=\sum^2$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605214245755.png" alt="image-20200605214245755" style="zoom:50%;"></p>
<p>对于Water属性的宝可梦对应参数为$\mu^1$，Normal属性的宝可梦为$\mu^2$，共同属性为$\sum$，这时对应的最大似然函数为$L(\mu^1,\mu^2,\sum)$，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605214446675.png" alt="image-20200605214446675" style="zoom:50%;"></p>
<p>修改后的模型准确率可以达到54%，如果加入更多的属性，准确率可以提高到73%</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605215021833.png" alt="image-20200605215021833" style="zoom:50%;"></p>
<h4 id="Three-Steps"><a href="#Three-Steps" class="headerlink" title="Three Steps"></a>Three Steps</h4><p>这里再回忆一下三个步骤：<br>（1）Function Set，计算分类为该类的概率$P(C_1|x)$，如果大于0.5，则认为类别为1，否则为类别2<br>（2）找出相对应的$\mu,\sum$，使得似然函数L取得最大值；<br>（3）得出使似然函数最大化的参数，$\mu^<em>,\sum^</em>$，</p>
<script type="math/tex; mode=display">
u^*=\frac{1}{79}\sum_{n=1}^{79}x^n,\quad\sum^*=\frac{1}{79}\sum_{n=1}^{79}(x^n-\mu^*)(x^n-\mu^*)^T</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605220705405.png" alt="image-20200605220705405" style="zoom:50%;"></p>
<h4 id="Probability-Distribution"><a href="#Probability-Distribution" class="headerlink" title="Probability Distribution"></a>Probability Distribution</h4><p>这里我们所使用的概率分布是</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605221617855.png" alt="image-20200605221617855" style="zoom:50%;"></p>
<p>下面开始公式推导，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605221733838.png" alt="image-20200605221733838" style="zoom:50%;"></p>
<p>得出了我们的Sigmoid函数，$\sigma(z)$的函数图像为s型，值域范围为[0,1]，将z化简，并代入$P(C_i)=\frac{N_i}{N_1+N_2}$ (i=1,2)，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605221911457.png" alt="image-20200605221911457" style="zoom:50%;"></p>
<p>将$P(x|C_1),P(x|C_2)$的表达式代入$ln\frac{P(x|C_1)}{P(x|C_2)}$，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605222200879.png" alt="image-20200605222200879" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605223207505.png" alt="image-20200605223207505" style="zoom:50%;"></p>
<p>再进行进一步化简，带入$\sum^1=\sum^2=\sum$，我们可以得出z的简易表达式$z=w\cdot x + b$，可得出$P(C_1|x)=\sigma(z)=\sigma(w\cdot x+b)$。当得出$N_1,N_2,\mu^1,\mu^2,\sum$时，就可以计算出w和b的值。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200605223410530.png" alt="image-20200605223410530" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>classification</tag>
        <tag>sigmoid function</tag>
      </tags>
  </entry>
  <entry>
    <title>Tips for Training DNN</title>
    <url>/2020/06/07/DNN-tip/</url>
    <content><![CDATA[<blockquote>
<p>本文的主要思路：针对training set和testing set上的performance分别提出针对性的解决方法 1、在training set上准确率不高：   new activation function：ReLU、Maxout   adaptive learning rate：Adagrad、RMSProp、Momentum、Adam<br>2、在testing set上准确率不高：Early Stopping、Regularization or Dropout</p>
</blockquote>
<h3 id="Recipe"><a href="#Recipe" class="headerlink" title="Recipe"></a>Recipe</h3><h4 id="three-steps-of-deep-learning"><a href="#three-steps-of-deep-learning" class="headerlink" title="three steps of deep learning"></a>three steps of deep learning</h4><p>做深度学习也遵循这三个步骤：</p>
<ul>
<li>define a set of function，</li>
<li>goodness of function，找到loss function</li>
<li>pick the best function，找到使loss最小化的参数</li>
</ul>
<p>overfitting是指模型在训练集上表现良好，但在测试集上表现却很差的现象。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607122016217.png" alt="image-20200607122016217" style="zoom:50%;"></p>
<h4 id="Do-not-always-blame-Overfitting"><a href="#Do-not-always-blame-Overfitting" class="headerlink" title="Do not always blame Overfitting"></a>Do not always blame Overfitting</h4><p>在下图中，我们展示了一个20层和56层的network，在训练集和测试集上的error。黄色表示20层network，红色表示56层network。</p>
<p>由于模型在训练集上的表现，20层的network表现得比较好，有同学就认为这是overfitting，但其实这并不是overfitting问题，因为这个模型在训练集上的表现，也是20层的network表现好</p>
<p>之所以出现这个20层和56层network表现（都是20层network表现好），是由于模型的训练没有训练好</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607123330505.png" alt="image-20200607123330505" style="zoom:50%;"></p>
<h4 id="Different-approaches-for-different-problems"><a href="#Different-approaches-for-different-problems" class="headerlink" title="Different approaches for different problems."></a>Different approaches for different problems.</h4><p>在网络的训练过程中，我们要针对网络的不同问题提供不同的解决方法，主要有两个问题</p>
<ul>
<li>在training data上表现不好</li>
<li>在testing data上表现不好</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607123926844.png" alt="image-20200607123926844" style="zoom:50%;"></p>
<h3 id="Good-Results-on-Training-Data"><a href="#Good-Results-on-Training-Data" class="headerlink" title="Good Results on Training Data?"></a>Good Results on Training Data?</h3><p>要在training data上获得好的结果，可以使用new activation function和adaptive learning rate</p>
<h4 id="New-Activation-Function"><a href="#New-Activation-Function" class="headerlink" title="New Activation Function"></a>New Activation Function</h4><h5 id="deeper-is-better-？"><a href="#deeper-is-better-？" class="headerlink" title="deeper is better ？"></a>deeper is better ？</h5><p>在1980年代，network中主要使用sigmoid funcion作为激活函数，从下图中我们可以看出，使用sigmoid function并不能保证网络结构越深，训练结果越好</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607124417000.png" alt="image-20200607124417000" style="zoom:50%;"></p>
<h5 id="Vanishing-Gradient-Problem"><a href="#Vanishing-Gradient-Problem" class="headerlink" title="Vanishing Gradient Problem"></a>Vanishing Gradient Problem</h5><p>出现上面这个问题的原因并不是overfitting，而是vanishing gradient（梯度消失）</p>
<p>当网络层数很深的时候，在靠近input layer的位置，常常会有很小的 gradient，学习速度也很慢；而在靠近output layer的地方，常常会有更大的gradient，学习速度也会很快，很快就到了converge（收敛）了</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607125209255.png" alt="image-20200607125209255" style="zoom:50%;"></p>
<p>下面将叙述出现这个问题的原因。对于下图的中sigmoid function，输出范围为[0,1]，对于很大的输入，输出往往会被压缩成一个较小的值</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607125550128.png" alt="image-20200607125550128" style="zoom: 50%;"></p>
<p>对于loss function对其中一个参数w的微分$\frac{\partial l}{\partial w}$，这里我们将其表达式写为$\Delta w$，可以表示当前参数w对结果loss的影响，当把这个参数w进行变化时，对loss的影响会有多大</p>
<p>如下图所示，如果我们输入一个很大的$\Delta w$，在经过sigmoid function运算之后，其值就缩小一次；当经过后面多层的压缩之后，$\Delta w$的值就变得越来越小;……；因此gradient在input layer附近的值会很大，但在output layer附近的值经过多次的压缩就变得很小了。</p>
<p>当缩小的gradient达到我们设置的那个临界值，即gradient接近于0，就发生了梯度消失问题</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607125910525.png" alt="image-20200607125910525" style="zoom:50%;"></p>
<p>要解决这个问题，可以修改一下network中用到的激活函数</p>
<h5 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h5><p>我们选取ReLU函数的原因有以下几个：</p>
<ul>
<li>可以快速计算，不管是函数值还是对应的梯度</li>
<li>结合了生物上的一些观察</li>
<li>无穷多个不同bias的sigmoid function叠加的结果可以变成ReLU</li>
<li><u>可以解决梯度消失问题</u>；</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607131402741.png" alt="image-20200607131402741" style="zoom:50%;"></p>
<p>使用ReLU函数之后，代入具体的网络结构</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607131816149.png" alt="image-20200607131816149" style="zoom:50%;"></p>
<p>对于input为0的值，network将不再计算其相对应的weight，而对于input不为0的值，就相当于一个线性函数$y=x$。这样做可以简化网络结构，网络结构中也就不存在gradient很小的neural</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607131920750.png" alt="image-20200607131920750" style="zoom:50%;"></p>
<p>Q：但这时出现了一个新问题，ReLU函数在z=0这一点是不可导的，那么我们在根据loss function如何来计算gradient呢？</p>
<p>A：这里我们将输入z<0的数的gradient看作0，相当于从network中抹去了这部分神经元；对于z>=0的数，gradient=1</0的数的gradient看作0，相当于从network中抹去了这部分神经元；对于z></p>
<h5 id="ReLU-variant"><a href="#ReLU-variant" class="headerlink" title="ReLU - variant"></a>ReLU - variant</h5><p>对于ReLU，当x&lt;=0时，函数的输出值就为0了，网络中的参数也没办法更新。因此，就有学者提出了Leaky ReLU，当x&lt;=0时，函数的输出值不是0，而是乘以一个系数0.01，这时的函数就称作<strong>Leaky ReLU</strong></p>
<p>还有另外一种ReLU函数的变体，<strong>Parametric ReLU</strong>，前面乘上的系数也可以进行训练</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607143129117.png" alt="image-20200607143129117" style="zoom:50%;"></p>
<h5 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h5><blockquote>
<p>ReLU is a special cases of Maxout</p>
</blockquote>
<p>Maxout的主要思想是：让network自己去学习对应的activation function，可以学习出ReLU，也可以是其他的activation function</p>
<p>Maxout激活函数是对前几个神经元取最大值，再输出相应的最大值</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607144717796.png" alt="image-20200607144717796" style="zoom:50%;"></p>
<p><strong>Maxout-&gt;ReLU</strong></p>
<p>在下图中，对于左图中的ReLU function</p>
<ul>
<li>input为蓝色直线，表示$z=wx+b$，</li>
<li><p>output：当z<0时，relu也输出为0；当z>0时，ReLU的图像和z是一致的</0时，relu也输出为0；当z></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607144943655.png" alt="image-20200607144943655" style="zoom:50%;"></p>
</li>
</ul>
<p>对于右图中的Maxout function，$z_1$对应的权重是$w,b$，而$z_2$对应的权重则是0，</p>
<ul>
<li>input为$z_1,z_2$，<ul>
<li>蓝色直线表示$z_1=wx+b$，</li>
<li>红色表示$z_2=0$</li>
</ul>
</li>
<li>这时neural的output为$max\{z_1,z_2\}$，输出则是和ReLU一致的（图中绿色直线）</li>
</ul>
<p><strong>Maxout-&gt;more than ReLU</strong></p>
<p>maxout不仅可以学习ReLU，也可以学习其他的activation function</p>
<p>对于右图中的新的输入，$z_2$对应的权重则变成了$w’,b’$，那么相应的input和output为</p>
<ul>
<li>input，$z_1=wx+b,z_2=w’x+b’$，分别对应图中蓝色和绿色直线；</li>
<li>output为$max\{z_1,z_2\}$，表现为图中绿色直线</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607150829520.png" alt="image-20200607150829520" style="zoom:50%;"></p>
<p>这时我们得到的activation function就是图中的绿色直线，是通过网络training出来的，训练的参数为$w,w’,b,b’$，训练结束即可得出我们的activation function</p>
<p><strong>Summary</strong></p>
<p>这里我们先对maxout做一个总结，maxout是一个可学习的activation function，可以学习出任何分段线性凸函数（piecewise linear convex function），具体的分段数取决于在group中的元素个数</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607151653515.png" alt="image-20200607151653515" style="zoom:50%;"></p>
<p>对于maxout函数的训练，如果是下图中的网络结构，我们假设已经知道$z_1^1,z_4^1,z_2^2,z_3^2$为对应的最大值，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607152054887.png" alt="image-20200607152054887" style="zoom:50%;"></p>
<p>那么network可以再次被化简，neural也可以变少</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607152242384.png" alt="image-20200607152242384" style="zoom:50%;"></p>
<h4 id="Adaptive-Learning-Rate"><a href="#Adaptive-Learning-Rate" class="headerlink" title="Adaptive Learning Rate"></a>Adaptive Learning Rate</h4><h5 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h5><p><strong>Adagrad</strong></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607152520774.png" alt="image-20200607152520774" style="zoom:50%;"></p>
<p><strong>RMSprop</strong></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607153455066.png" alt="image-20200607153455066" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607153507651.png" alt="image-20200607153507651" style="zoom:50%;"></p>
<p>gradient为0的点可以是local minimum，也可以是saddle point，还可以是在很平缓的plateau中的某些点</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607153541752.png" alt="image-20200607153541752" style="zoom:50%;"></p>
<p>而在物理世界，物体本身是带有momentum的，再加上gradient的作用，就很可能可以跳出saddle point，继续训练</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607153719322.png" alt="image-20200607153719322" style="zoom:50%;"></p>
<h5 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h5><p>图中蓝色箭头表示Movement（前进方向），红色箭头表示gradient的方向，绿色虚线表示上一次movement对本次的影响（惯性）</p>
<p>再$\theta^0$处，movement为$v^0=0$；对于在$\theta^1$处的前进方向，先计算出在$\theta^0$处的梯度$\Delta L(\theta^0)$，我们要移动的方向是由上一个时间点的gradient为$\Delta L(\theta^0)$和前进方向$v_0$决定的，即</p>
<script type="math/tex; mode=display">
v^1=\lambda v^0-\eta \Delta L(\theta^0)=-\eta \Delta L(\theta^0)</script><p>其中$\lambda$也是一个可以手动调整的参数</p>
<p>对于下一个时间点的移动方向$v^2$，是和当前时间节点的移动方向和梯度$v^1,\Delta L(\theta^1)$决定的，即</p>
<script type="math/tex; mode=display">
\begin{aligned}
v^2&=\lambda v^1-\eta \Delta L(\theta^1)\\
&=-\lambda\eta \Delta L(\theta^0)-\eta \Delta L(\theta^1)
\end{aligned}</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607154125395.png" alt="image-20200607154125395" style="zoom:50%;"></p>
<p>再回到之前的例子，红色箭头表示gradient的反方向，绿色表示momentum的方向，蓝色箭头表示受到gradient和momentum影响后的真实运动方向</p>
<p>初始点的momentum值为0；在下一个plateau上的点，虽然gradient的值很小很小，但由于受到上一个很大的momentum的影响，真实的movement还是向前的，步长也没有因为gradient的变小而变得很小；</p>
<p>如果我们现在走到了local minimum，此时gradient=0，此时由于momentum的影响，如果momentum的值足够大，还会继续向前运动</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607161327324.png" alt="image-20200607161327324" style="zoom:50%;"></p>
<h5 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h5><blockquote>
<p>RMSProp + Momentum</p>
<p>Adam其实就是结合了RMSProp和Momentum思想的方法</p>
</blockquote>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607162646245.png" alt="image-20200607162646245" style="zoom: 67%;"></p>
<p>先将动量momentum和初始移动方向初始化为0，即$m_0=0,v_0=0$，$v_0$表示RMSProp中分母上的参数$\sigma$</p>
<p>计算在t时的梯度$g_t$，</p>
<script type="math/tex; mode=display">
g_t=\Delta_\theta f_t(\theta_{\theta-1})</script><p>根据上一个时间点的要走的方向$m_t$和gradient，因此t时的移动方向为$m_t$———Momentum</p>
<script type="math/tex; mode=display">
m_t=\beta_1\cdot m_{t-1}+(1-\beta_1)\cdot g_t</script><p>根据上一个时间点的移动方向$v_{t-1}$和gradient，则此时的真实移动方向$v_t$为———-RMSprop</p>
<script type="math/tex; mode=display">
v_t=\beta_1\cdot v_{t-1}+(1-\beta_2)\cdot g_t</script><p>该算法还进行了bias corrected，</p>
<script type="math/tex; mode=display">
\hat m_t=\frac{m_t }{(1-\beta_1^t)},\quad
\hat v_t=\frac{v_t}{(1-\beta_2^t)}</script><p>将进行了bias corrected的参数再输入公式，更新参数</p>
<script type="math/tex; mode=display">
\theta_t=\theta_{t-1} -\alpha \cdot \hat m _t/\sqrt{\hat v _t}+\epsilon</script><h3 id="Good-Results-on-Testing-Data"><a href="#Good-Results-on-Testing-Data" class="headerlink" title="Good Results on Testing Data?"></a>Good Results on Testing Data?</h3><h4 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607163018382.png" alt="image-20200607163018382" style="zoom:50%;"></p>
<h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><h5 id="L2-Regularization"><a href="#L2-Regularization" class="headerlink" title="L2 Regularization"></a>L2 Regularization</h5><p>正则化就引入了一个新的loss function，加上了一个新的正则项，这个正则项将所有需要training的参数都包括进来了，通常不包括bias</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607163135769.png" alt="image-20200607163135769" style="zoom:50%;"></p>
<p>这个新的loss function再对w求偏微分，对参数进行更新</p>
<script type="math/tex; mode=display">
w^{t+1}=(1-\eta\lambda)w^t-\eta \frac{\partial L}{\partial w}</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607163452933.png" alt="image-20200607163452933" style="zoom:50%;"></p>
<p>与原来的参数更新公式相比，可以发现$w^t$前面多了一项$(1-\eta\lambda)$，通常这个$\eta,\lambda$都是很小的值，这里我们假设$(1-\eta\lambda)$是很接近于1的值，约等于0.99; regularization所做的事就是，在每次更新参数时，全都在前面乘上了一个小于1的数，在经过若干次的训练之后，$(1-\eta\lambda)w^t$的值就很接近0了</p>
<p>虽然$(1-\eta\lambda)w^t$的值每次都会变得越来越小，但参数更新的公式中，后面还有另外一项$\eta \frac{\partial L}{\partial w}$，会使得梯度的值不会变成0，达到平衡</p>
<h5 id="L1-Regularization"><a href="#L1-Regularization" class="headerlink" title="L1 Regularization"></a>L1 Regularization</h5><p>既然L2可以作为正则项，L1也可以作为正则项，正则项为参数的绝对值相加。对这些参数求导，当$w_i$大于0时，gradient=1，当$w_i$小雨0时，gradient=-1，即为$sgn(w)$函数</p>
<p>L1的参数更新公式为</p>
<script type="math/tex; mode=display">
w^{t+1}=w^t-\eta \frac{\partial L}{\partial w} - \eta\lambda\ \rm{sgn}(w^t)</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607171442670.png" alt="image-20200607171442670" style="zoom:50%;"></p>
<p>与原来的参数更新公式相比较，可以发现后面多了一项$- \eta\lambda\ \rm{sgn}(w^t)$，表示参数在原来的基础上都要减去一个小于1的数</p>
<h5 id="L1-vs-L2"><a href="#L1-vs-L2" class="headerlink" title="L1 vs L2"></a>L1 vs L2</h5><p>参数更新公式分别如下，</p>
<script type="math/tex; mode=display">
L1:\ w^{t+1}=w^t-\eta \frac{\partial L}{\partial w} - \eta\lambda\ \rm{sgn}(w^t)\\
L2:\ w^{t+1}=(1-\eta\lambda)w^t-\eta \frac{\partial L}{\partial w}</script><ul>
<li>L1参数更新公式每次都多<strong>减去了一个小于1的固定值（constant）</strong></li>
<li>L2参数更新公式中每次都将<strong>前一次的参数乘上一个小于1的值</strong></li>
</ul>
<h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173403475.png" alt="image-20200607173403475" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173419426.png" alt="image-20200607173419426" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173455150.png" alt="image-20200607173455150" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173629804.png" alt="image-20200607173629804" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173828093.png" alt="image-20200607173828093" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173729715.png" alt="image-20200607173729715" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173943611.png" alt="image-20200607173943611" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>DNN</tag>
        <tag>Momentum</tag>
        <tag>Adam</tag>
        <tag>Regularization</tag>
      </tags>
  </entry>
  <entry>
    <title>Unsupervised Conditional Generation</title>
    <url>/2020/07/06/CycleGAN/</url>
    <content><![CDATA[<p>在以前的文章中，我们提到过<a href="https://scarleatt.github.io/2020/07/05/CGAN/" target="_blank" rel="noopener">CGAN</a>，训练数据包括图像和其对应的文字描述，是一种监督学习的方法。本文将叙述一种使用CGAN进行无监督学习的方法，主要包括两大类的方法：Direct Transformation和Projection to Common Space。</p>
<h4 id="Unsupervised-Conditional-Generation"><a href="#Unsupervised-Conditional-Generation" class="headerlink" title="Unsupervised Conditional Generation"></a>Unsupervised Conditional Generation</h4><p>如果现在有一张真实的风景照X，还有一张图像是梵谷的画作Y，那么我们就可以训练一个generator，使其输出像梵谷画作的图像。现在我们要完成的任务是对图像进行风格转换，我们可以收集很多真实的风景照，也可以收集很多梵谷的画作，但我们很难收集到这两者之间的联系，相当于训练数据是没有label的，因此就需要进行无监督的学习。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706144703530.png" alt="image-20200706144703530" style="zoom:60%;"></p>
<p>这种技术不仅可以用到图像领域，也可以用到其他领域，比如语音处理领域。</p>
<p>根据收集到的论文，Unsupervised CGAN可以分为两大类的方法：</p>
<ol>
<li>Direct Transformation：学习一个generator，直接将Domain X的图像转化为Domain Y的图像；这种处理方式不会对input进行太大的改变，如果是影像的话，通常只会修改一下颜色、质地之类的；</li>
<li>Projection to Common Space：input和output差距很大，不仅仅是只有颜色和纹理的变化；这时就需要先使用encoder，得出input的特征，比如这是个男生、还戴着眼镜，再用decoder生成对应的动漫角色</li>
</ol>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706151909380.png" alt="image-20200706151909380" style="zoom:60%;"></p>
<h4 id="Direct-Transformation"><a href="#Direct-Transformation" class="headerlink" title="Direct Transformation"></a>Direct Transformation</h4><h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>现在是无监督的学习，generator如何得知自己是不是产生了类似Domain Y的图像呢？这时我们就需要训练一个Domain Y的discriminator，这个discriminator看过很多属于domain Y的图像；对于给定的图像，就可以判断出到底属于哪个domain的图像。</p>
<p>对于generator，这时的训练目标就是生成能骗过discriminator的图像。如果generator能生成骗过discriminator的图像，那么我们就可以认为generator生成了和domain Y类似的图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706153136496.png" alt="image-20200706153136496" style="zoom:60%;"></p>
<p>generator可以产生很像梵谷画作的图像，但这个图像可以是和input毫无关系的，这并不是我们想要的结果。因此我们并不能只要求generator生成的图像能骗过discriminator就好。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706154211838.png" alt="image-20200706154211838" style="zoom:60%;"></p>
<p>实际上，在generator不加额外限制的条件下，generator的input和output通常差别不会特别大（比如input是风景图，output是梵谷的自画像），generator通常只希望改一小部分内容能骗过discriminator就好，并不希望进行太大的改变。因此如果不加额外的constrain，这个GAN也是可以work的。</p>
<p>有学者在论文中提出了其他的解决方法。</p>
<p>(1) 如果generator比较shallow，不那么deep，不用加额外的constrain，就可以使input和output差距不大；如果generator很深，就可以使input和output差距很大，这时就需要一些额外的constrain。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706154355533.png" alt="image-20200706154355533" style="zoom:60%;"></p>
<p>(2) 还可以使用另外一种方法。现在有一个pre-trained的network（VGG等），把generator的input和output输入这个network，会输出一个embedding（<a href="https://scarleatt.github.io/2020/06/11/Word-Embedding/" target="_blank" rel="noopener">word embedding</a>）。那么generator的目标就有两个：输出和梵谷画作类似的图像；其input和output之间的差距也不能太大。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706155513844.png" alt="image-20200706155513844" style="zoom:60%;"></p>
<h5 id="Cycle-GAN"><a href="#Cycle-GAN" class="headerlink" title="Cycle GAN"></a>Cycle GAN</h5><p>(3) <strong>Cycle GAN</strong>，现在有一个<span style="color: blue">generator</span> $G_{X\rightarrow Y}$可以生成domain X到Y的图像，还有另外一个<span style="color: orange">generator</span> $G_{Y\rightarrow X}$可以生成domain Y到X的图像output，生成的图像应与原来的input越接近越好。现在的generator有了两个目标：产生能骗过discriminator的图像；使对应的input和output越接近越好。</p>
<p>那么现在就不可能在中间产生一个像梵谷自画像的图像，因为这时第二个generator就不可能从这个自画像返回原来的自画像，不满足两个限制条件。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706160720337.png" alt="image-20200706160720337" style="zoom:60%;"></p>
<p>Cycle GAN也可以是双向的。原来的网络是使domain $X\rightarrow Y,Y\rightarrow X$，现在我们加入了另外一个GAN网络，使输入domain Y的图input转化为X的图，再使domain X的图转化为Y的图output，input和output之间的差距也应该越小越好。这时还对应了两个discriminator。现在我们就可以同时train这两个网络。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706162008139.png" alt="image-20200706162008139" style="zoom:60%;"></p>
<h5 id="Issue-of-Cycle-Consistency"><a href="#Issue-of-Cycle-Consistency" class="headerlink" title="Issue of Cycle Consistency"></a>Issue of Cycle Consistency</h5><p>CycleGAN会把input的一些信息藏起来，output的时候会把这些信息又呈现出来。</p>
<p>下图中的网络是使输入的真实图像转化为类似卫星图的图像。第一个generator把input转化为卫星图，第二个generator再转化为原来的真实图像output。我们可以在input的红色方框内有一些黑点，中间卫星图的部分却没有黑点，再output的红色方框内又出现了这些黑点。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706162852240.png" alt="image-20200706162852240" style="zoom:60%;"></p>
<p>Q：那么为什么generator可以从第二张图像中生成output呢？</p>
<p>A：答案是第一个generator把这些关键信息隐藏了，只是我们人眼并不能看到这些藏起来的信息。</p>
<p>下图中还有一些其他GAN网络（Dual GAN，Disco GAN），但核心思想都是和Cycle GAN差不多的，只是论文提交到了不同期刊上。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706163814535.png" alt="image-20200706163814535" style="zoom:60%;"></p>
<h5 id="StarGAN"><a href="#StarGAN" class="headerlink" title="StarGAN"></a>StarGAN</h5><p>如果我们现在不是在两个Domain之间互转，而是在4个Domain之间，在理论上就需要12个GAN网络才可以实现。但StarGAN只学习了1个generator，就可以实现在多个Domain之间互转。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706164130509.png" alt="image-20200706164130509" style="zoom:60%;"></p>
<p>下面简要叙述StarGAN的算法：</p>
<p>(a) 首先训练了一个discriminator，需要来鉴别输入的图像是real/fake，还需要得出图像到底是属于哪一个domain；</p>
<p>(b) 还需要学习一个generator，输入是一张图像和目标domain，即你想让input转化成哪一个domain，生成一个Fake Image；</p>
<p>(c) 把生成的图像再输入同一个generator，目标domain也作为输入，生成一张新的图像（reconstructed image），我们希望reconstructed image和input image之间越接近越好；</p>
<p>(d) 把Fake Image再输入discriminator，看到底是不是符合要求的图像。</p>
<p><img src="/Users/liufang/Library/Application Support/typora-user-images/image-20200706164451665.png" alt="image-20200706164451665"></p>
<p>下图是一个更加realistic的展示。domain可以有多个，所以用一串编码表示，比如现在输入图像的domain是Brown、Young，对应的编码为00101，记作CelevA label。</p>
<p>(a) 现在把一张图像输入discriminator，来判断到底是不是真实的图像，且输出domain对应的代号；(b) 把input image和target domain label（10011，Black、Male、Young）输入generator；(c) 把上一部生成的图像再输入同一个generator，让其生成00101（brown、young）的图像output，我们希望input和output越接近越好；(d) 把第一次generator生成的图像输入discriminator，看是不是realistic的图像，且输出对应的domain代号。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706165503010.png" alt="image-20200706165503010"></p>
<h4 id="Projection-to-Common-Space"><a href="#Projection-to-Common-Space" class="headerlink" title="Projection to Common Space"></a>Projection to Common Space</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706182808106.png" alt="image-20200706182808106" style="zoom:60%;"></p>
<h5 id="Target"><a href="#Target" class="headerlink" title="Target"></a>Target</h5><p>现在Domain X是真实人物图像，Domain Y是动漫人物，X和Y之间差距很大，就不能用之前的direct transformation。</p>
<p>这里我们可以先使用encoder $EN_X$提取出X的特征，用另一个encoder $EN_Y$提取Y的特征，即图像输入encoder会输出一个vector；把vector输入decoder，如果输入domain X的decoder，就产生真实人物的图像，如果输入domain Y的decoder，就产生动漫人物的图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706183118051.png" style="zoom:60%;"></p>
<p>再回顾一下我们的问题，我们希望输入真实人物的图像，网络输出动漫人物的图像。这里vector，表示人脸的特征，其每一个维度对应人脸的某个属性，比如戴不戴眼镜。即我们希望decoder能够根据这些attribute生成对应的动漫人物。</p>
<p>如果我们知道X和Y之间的关系，这个问题用supervise学习可以很简单地解决，但现在这是一个unsupervised问题，我们可以收集domain X的很多数据，也可以收集domain Y的很多数据，但这两者之间的联系我们却很难收集。</p>
<p>那么我们怎么来训练这个encoder和decoder呢？</p>
<h5 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h5><p>我们可以先把属于domain X的encoder和decoder结合起来，组成一个auto-encoder，输入一张domain X的图，经过encode-decode的过程，使其reconstruct成原来输入的图，使这两者之间的reconstruction error最小化。属于domain Y的encoder和decoder也使用类似的操作。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706190811253.png" alt="image-20200706190811253" style="zoom:60%;"></p>
<p>这样做会造成一个新的问题，这两个encoder和decoder之间是没有关联的。</p>
<p>我们可以再多加discriminator进来，让输入domain X的decoder的输出更像X。如果我们只是来学习这个auto-encoder，使reconstruction error最小化，会使decoder的output非常模糊。</p>
<p>现在这个属于domain X的encoder和decoder，以及discriminator和起来，就相当于一个VAE GAN；属于domain Y的encoder和decoder，discriminator相当于另外一个VAE GAN。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706191458656.png" alt="image-20200706191458656" style="zoom:60%;"></p>
<p>由于这两个auto-encoder是分开学习的，如果现在输入一张真实的人脸，属于domain Y的decoder很可能输出一张截然不同的人脸。两个encoder是分开训练的，很可能encoder $EN_X$输出的vector的第一维代表性别、第二维代表戴不戴眼镜，encoder  $EN_Y$输出的vector的第二维代表性别、第三维代表戴不戴眼镜。</p>
<p><u>Solution 1:</u> 为了解决这个问题，有学者提出了新的解法。对于不同domain的encoder和decoder，我们可以让其tie到一起。具体做法是：属于domain X和Y的网络结构都有多个hidden layer，我们可以让这两者的后面某几层hidden layer的参数是共用的；对应的decoder，可以前面几个hidden layer是共用的，后面几个不是共用的。</p>
<p>如果我们共用encoder的后面几个hidden layer，属于domain X和Y的encoder所输出的vector，都是属于同一个latent space的，用同样的dimensions来表示人脸的同一个特征，即第一维都表示男性，第二维都表示戴眼镜。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706193608486.png" alt="image-20200706193608486" style="zoom:60%;"></p>
<p><u>Solution 2:</u> 加一个domain discriminator，可以对domain X和Y的encoder所输出的vector进行判断，看到底是属于哪一个domain的图像。如果这个domain discriminator不能进行判断，那么我们就可以认为这两个encoder所生成的vector其distribution都是一样的，从而这两个distribution中相同的维度表示相同的意思。</p>
<p>假设domain X和Y中男女比例、戴不戴眼镜的比例都是一样的，现在domain discriminator可以强迫让这个embedding的latent feature是一样的，因此就会用同样的dimension来表示这个vector（The domain discriminator forces the output of 𝐸𝑁x and 𝐸𝑁y have the same distribution. ）。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706194620613.png" alt="image-20200706194620613" style="zoom:60%;"></p>
<p><u>Solution 3</u>: 还可以用cycle consistency。真实人物的图像input输入domain X的encoder，生成对应的code再输入domain Y的decoder，重建输入的图像；再输入domain Y的encoder，对应的code输入domain X的decoder，得到output，目标是使input和output之间的error越小越好。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706200901735.png" alt="image-20200706200901735" style="zoom:60%;"></p>
<h4 id="Voice-Conversion"><a href="#Voice-Conversion" class="headerlink" title="Voice Conversion"></a>Voice Conversion</h4><p>把一个人的声音转化成另一个人的声音。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200706202208008.png" alt="image-20200706202208008" style="zoom:60%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Cycle GAN</tag>
        <tag>StarGAN</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Autoencoder</title>
    <url>/2020/06/29/Deep-autoencoder/</url>
    <content><![CDATA[<h4 id="Auto-encoder"><a href="#Auto-encoder" class="headerlink" title="Auto-encoder"></a>Auto-encoder</h4><p>对于输入大小为$28\times28$的图像，先经过encoder进行编码，得到的结果通常比784维要小，code表示比原来的image更加精简（compact）的特征；</p>
<p>但现在我们进行的是unsupervised learning，这个code到底长什么样子也不知道；</p>
<p>那么我们现在就先来做一个decoder，把code恢复成一张image；</p>
<p>decoder和encoder单独是没办法训练的，因为是无监督学习，因此现在我们将这两者一起学习</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629094326636.png" alt="image-20200629094326636" style="zoom:50%;"></p>
<h4 id="Recap-PCA"><a href="#Recap-PCA" class="headerlink" title="Recap: PCA"></a>Recap: PCA</h4><p>这里先回顾一下<a href="https://scarleatt.github.io/2020/06/28/dimension-reduction/" target="_blank" rel="noopener">PCA</a>，对于输入的图像$x$为$784$维，经过PCA降维，可以降低到324维的图像$\hat x$，在encoder和decoder的学习过程中，应达到的目标是最小化$(x-\hat x)^2$，这个结构可以看成是具有一个hidden layer的network</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629100232021.png" alt="image-20200629100232021" style="zoom:50%;"></p>
<h4 id="Deep-Auto-encoder"><a href="#Deep-Auto-encoder" class="headerlink" title="Deep Auto-encoder"></a>Deep Auto-encoder</h4><h5 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h5><p>中间的hidden layer也可以不止一个，也可以是多个的hidden layer，这种结构就称作deep auto-encoder</p>
<p><span style="color: red">注：encoder和decoder的结构不一定非得是对称的。</span></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629101531636.png" alt="image-20200629101531636" style="zoom:50%;"></p>
<h5 id="PCA-vs-Deep-Auto-encoder"><a href="#PCA-vs-Deep-Auto-encoder" class="headerlink" title="PCA. vs Deep Auto-encoder"></a>PCA. vs Deep Auto-encoder</h5><p>对于下图中的图像(0,1,2,3,4)，如果我们使用PCA，只有中间一个hidden layer，先从784维降到30维，再从30维恢复到784维，可以发现图像变得比较模糊；</p>
<p>如果使用deep auto-encoder，先从784到1000，1000到500，500到250，250到300，再使用类似的encoder恢复图像，可以发现结果图像非常清晰</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629101748853.png" alt="image-20200629101748853" style="zoom:50%;"></p>
<p>如果我们先使用PCA进行降维，把原图从784降到2维，对二维的数据进行可视化，可以发现不同的digit（不同的颜色代表不同的数字）都叠在一起了；</p>
<p>如果使用deep autoencoder，可以发现这个数字都是分开的</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629102817764.png" alt="image-20200629102817764" style="zoom:50%;"></p>
<h4 id="Text-Retrieval"><a href="#Text-Retrieval" class="headerlink" title="Text Retrieval"></a>Text Retrieval</h4><p>下图中蓝色的圆点都表示一个document，我们将输入的query也加入这个document，再计算查询的词汇query和每个document之间的inner product或similarity等，距离最近的document，similarity的值是最大的，因此会retrieval距离红色箭头最近的其他两个蓝色箭头</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629103259176.png" alt="image-20200629103259176" style="zoom:50%;"></p>
<p>还有另外一种方法，使用一个vector来表示所有的词汇，vector对应的值就是每个character出现的次数；但这种方式没有考虑原来的语义顺序，每个character都是independent的</p>
<p>在下图中，假设bag里面有2000个词汇，把输入的document或query变成对应的vector，再输入相应的encoder，降维成2维，对这二维的数据进行可视化，如右图所示，不同的颜色代表不同的document；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629104843900.png" alt="image-20200629104843900" style="zoom:50%;"></p>
<p>如果我们现在输入一个query，再降维到二维，可以发现和图中对应的document是有关的，我们就可以看到query所属的类别是Energy market</p>
<p>但用LSA就得不到对应的结果</p>
<h4 id="Similar-Image-Search"><a href="#Similar-Image-Search" class="headerlink" title="Similar Image Search"></a>Similar Image Search</h4><p>以图找图，如果我们只是做pixel上的相似程度，那么我们可以得到以下的结果，迈克杰克逊和马蹄铁也很像，这显然不符合常理</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629110009994.png" alt="image-20200629110009994" style="zoom:50%;"></p>
<p>我们可以把输入的image经过deep auto-encoder，变成一个code，再去做搜寻；由于auto-encoder是unsupervised learning，收集多少数据都行，不缺数据</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629110134069.png" alt="image-20200629110134069" style="zoom:50%;"></p>
<p>用deep auto-encoder来找类似迈克杰克逊，也可以得到更好的结果（至少全是人脸 ）</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629110527622.png" alt="image-20200629110527622" style="zoom:50%;"></p>
<h4 id="Pre-training-DNN"><a href="#Pre-training-DNN" class="headerlink" title="Pre-training DNN"></a>Pre-training DNN</h4><p>autoencoder也可以用到network的pre-training过程，来初始化所需要的weight；在下图中，如果我们要对第一个hidden layer的weight进行初始化，那么我们可以使用autoencoder，先将784维到1000维，再进行reconstruct，使1000维降到784维，来使$x,\hat x$之间的差值最小</p>
<p>但这样可能会出现一个问题，由于1000维比784维更高维，autoencoder很可能将input直接embed到1000维里，再进行恢复，这样很可能network什么都没有学到；</p>
<p>对于这个问题，我们可以把1000维再加上一个regularization，比如L1，这1000维的数据中有某些维必须是为0的，这样就可以避免autoencoder直接将input进行embed</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629110742780.png" alt="image-20200629110742780" style="zoom:50%;"></p>
<p>我们可以学习这样的一个autoencoder，先将input转化成一个1000维的vector，再把这个vector转化为<span style="color: green">1000维的code</span>，再把这个code转化为1000维的vector；使input和output越接近越好，<strong>把$W^2$的值保存下来</strong>，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629111953318.png" alt="image-20200629111953318" style="zoom:50%;"></p>
<p>再继续下一个layer，使用第三个autoencoder</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629112400780.png" alt="image-20200629112400780" style="zoom:50%;"></p>
<p>学习到$W^1,W^2,W^3$之后，就把这个weight作为初始值，$W^4$则进行random init，再使用<span style="color: red">back propagation</span>进行fine-tune</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629112513885.png" alt="image-20200629112513885" style="zoom:50%;"></p>
<p><span style="color: red">如果我们有大量的unlabeled data，labeled data只有很小一部分，那么我们就可以通过这种无监督学习的方式来初始化weight，再使用剩下的label data来对network进行fine-tune即可</span></p>
<h4 id="De-noising-auto-encoder"><a href="#De-noising-auto-encoder" class="headerlink" title="De-noising auto-encoder"></a>De-noising auto-encoder</h4><p>对于原来的input x，我们先加入一些noise得到$x’$，进行encode之后再进行decode，使之和最早的input之间的差值最小；encoder现在不仅学习到了encode这件事，还学习到了把noise过滤掉这件事</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629113151447.png" alt="image-20200629113151447" style="zoom:50%;"></p>
<h4 id="Auto-encoder-for-CNN"><a href="#Auto-encoder-for-CNN" class="headerlink" title="Auto-encoder for CNN"></a>Auto-encoder for CNN</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629114519998.png" alt="image-20200629114519998" style="zoom:50%;"></p>
<h5 id="CNN-Unpooling"><a href="#CNN-Unpooling" class="headerlink" title="CNN -Unpooling"></a>CNN -Unpooling</h5><p>在pooling时，会选择四个方框中的最大值，并记住最大值的location；</p>
<p>在upooling时，就会用到上面的location</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629114730398.png" alt="image-20200629114730398" style="zoom:50%;"></p>
<h5 id="Deconvolution"><a href="#Deconvolution" class="headerlink" title="Deconvolution"></a>Deconvolution</h5><p>Deconvolution其实就是在做convolution，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629114919151.png" alt="image-20200629114919151" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Unsupervised Learning</tag>
        <tag>Deep Autoencoder</tag>
        <tag>Autoencoder</tag>
      </tags>
  </entry>
  <entry>
    <title>Explainable Machine Learning</title>
    <url>/2020/06/12/Explainable-AI/</url>
    <content><![CDATA[<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>机器不仅要告诉我们结果cat，还要告诉我们为什么 </p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611112308714.png" alt="image-20200611112308714" style="zoom:50%;"></p>
<h5 id="Why-we-need-Explainable-ML"><a href="#Why-we-need-Explainable-ML" class="headerlink" title="Why we need Explainable ML?"></a>Why we need Explainable ML?</h5><p><u>我们不仅需要机器结果的精确度，还需要进行模型诊断，看机器学习得怎么样；有的任务精确度很高，但实际上机器什么都没学到</u></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611112911240.png" alt="image-20200611112911240" style="zoom:50%;"></p>
<p>有模型诊断后，我们就可以根据模型诊断的结果再来调整我们的模型</p>
<h5 id="Interpretable-v-s-Powerful"><a href="#Interpretable-v-s-Powerful" class="headerlink" title="Interpretable v.s. Powerful"></a>Interpretable v.s. Powerful</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611195115774.png" alt="image-20200611195115774" style="zoom:50%;"></p>
<p>那么有没有model是Interpretable，也是powerful的呢 ？</p>
<p>决策树可以interpretable，也是比较powerful的；对于第一个分支节点，“这些动物呼吸空气吗？”，就包含了interpretable的信息</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611195220007.png" alt="image-20200611195220007" style="zoom:50%;"></p>
<p>当分支特别多的时候，决策树的表现也会很差</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611195426930.png" alt="image-20200611195426930" style="zoom:50%;"></p>
<h4 id="Local-Explanation"><a href="#Local-Explanation" class="headerlink" title="Local Explanation"></a>Local Explanation</h4><h5 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h5><p>对于输入的x，我们将其分成components $\{x_1,…,x_n,…x_N\}$，每个component由一个像素，或者一小块组成</p>
<p>我们现在的目标是知道每个component对making the decision的重要性有多少，那么我们可以通过remove或者modify其中一个component的值，看此时的decision会有什么变化</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611200230303.png" alt="image-20200611200230303" style="zoom:50%;"></p>
<p>把灰色方块放到图像中，覆盖图像的一小部分；如果我们把灰色方块放到下图中的红色区域，那么对解释的结果影响不大，第一幅图还是一只狗</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611200829405.png" alt="image-20200611200829405" style="zoom:50%;"></p>
<p>还有另一种方法</p>
<p>对于输入的$\{x_1,…,x_n,..,x_N\}$，对于其中的某个关键的pixel $x_n$加上$\Delta x$，这个pixel对我们识别这是不是一只狗具有很重要的作用</p>
<p>那么我们就可以用$\frac{\Delta y}{\Delta x}$来表示这个小小的扰动对y的影响，可以通过$\frac{\partial y_k}{\partial x_n}$来进行计算，表示$y_k$对$x_n$的偏微分，最后取绝对值，表示某一个pixel对现在y影响的大小</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611214135944.png" alt="image-20200611214135944" style="zoom:50%;"></p>
<p>在上图中，下半部分由3幅图saliency map，亮度越大，绝对值就越大，亮度越大的地方就表示该pixel对结果的影响越大</p>
<h5 id="Limitation-of-Gradient-based-Approaches"><a href="#Limitation-of-Gradient-based-Approaches" class="headerlink" title="Limitation of Gradient based Approaches"></a>Limitation of Gradient based Approaches</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611220206992.png" alt="image-20200611220206992" style="zoom:50%;"></p>
<h5 id="Attack-Interpretation"><a href="#Attack-Interpretation" class="headerlink" title="Attack Interpretation"></a>Attack Interpretation</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611221032938.png" alt="image-20200611221032938" style="zoom:50%;"></p>
<h4 id="Global-Explanation"><a href="#Global-Explanation" class="headerlink" title="Global Explanation"></a>Global Explanation</h4><p>Interprete the whole Model</p>
<h5 id="Activation-Minimization-review"><a href="#Activation-Minimization-review" class="headerlink" title="Activation Minimization (review)"></a><strong>Activation Minimization</strong> (review)</h5><p>让我们先review一下activation minimization，现在我们的目标是找到一个$x^*$，使得输出的值$y_i$最大</p>
<p>我们可以加入一些噪声，加上噪声后人并不能识别出来，但机器可以识别出来，看出来下图中的噪声是0 1 2 3 4 5 6 7 8</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611221428166.png" alt="image-20200611221428166" style="zoom:50%;"></p>
<p>之前我们的目标是找到一个image，使得输出的y达到最大值；现在我们的目标不仅是找到x使输出y达到最大值，还需要把image变得更像是一个digit，不像左边那个图，几乎全部的像素点都是白色，右边的图只有和输出的digit相关的pixel才是白色</p>
<p>这里我们通过加入了一个新的限制$R(x)$来实现，可以表示图像和digit的相似度</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611223152859.png" alt="image-20200611223152859" style="zoom:50%;"></p>
<h5 id="Constraint-from-Generator"><a href="#Constraint-from-Generator" class="headerlink" title="Constraint from Generator"></a>Constraint from Generator</h5><p>如下图所示，我们输入一个低维的vector z到generator里面，输出Image x；</p>
<p>现在我们将生成的Image x再输入Image classifier，输出分类结果$y_i$，那么我们现在的目标就是找到$z^*$，使得属于那个类别的可能性$y_i$最大</p>
<script type="math/tex; mode=display">
z^*=arg \ max y_i</script><p>找到最好的$z^<em>$，再输入Generator，根据$x^</em>=G(z^<em>)$得出$x^</em>$，产生一个好的Image</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200612105833703.png" alt="image-20200612105833703" style="zoom:50%;"></p>
<p>结果展示。现在你问机器蚂蚁长什么样子呢？机器就会给你画一堆蚂蚁的图片出来，再放到classifier里面，得出分类结果到底是火山还是蚂蚁</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200612113355921.png" alt="image-20200612113355921" style="zoom:50%;"></p>
<h4 id="Using-a-model-to-explain-another"><a href="#Using-a-model-to-explain-another" class="headerlink" title="Using a model to explain another"></a>Using a model to explain another</h4><p>现在我们使用一个interpretable model来模仿另外一个uninterpretable model；下图中的Black Box为uninterpretable model，比如Neural Network，蓝色方框是一个interpretable model，比如Linear model；现在我们的目标是使用相同的输入$x^1,x^2,…,x^N$，使linear model和Neural Network有相近的输出</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200612155736839.png" alt="image-20200612155736839" style="zoom:50%;"></p>
<p>实际上并不能使用linear model来模拟整个neural network，但可以用来模拟其中一个local region</p>
<h5 id="Local-Interpretable-Model-Agnostic-Explanations-LIME"><a href="#Local-Interpretable-Model-Agnostic-Explanations-LIME" class="headerlink" title="Local Interpretable Model-Agnostic Explanations (LIME)"></a>Local Interpretable Model-Agnostic Explanations (LIME)</h5><p><strong>General</strong></p>
<p>下图中input为x，output为y，都是一维的，表示Black Box中x和y的关系，由于我们并不能用linear model来模拟整个neural network，但可以用来模拟其中一个local region</p>
<ol>
<li><p>首先给出要explain的point，代入black box里面</p>
</li>
<li><p>在第三个蓝色point（我们想要模拟的区域）周围sample附近的point，nearby的区域不同，结果也会不同</p>
</li>
<li><p>使用linear model来模拟neural network在这个区域的行为</p>
</li>
<li><p>得知了该区域的linear model之后，我们就可以知道在该区域x和y的关系，即x越大，y越小，也就interpret了原来的neural network在这部分区域的行为</p>
</li>
</ol>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200612161607923.png" alt="image-20200612161607923" style="zoom:50%;"></p>
<p>那么到底什么算是nearby呢？<u>用不同的方法进行sample，结果不太一样。</u>对于下图中的region，可以看到离第三个蓝色point的距离很远，取得的效果就非常不好了</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200612163829515.png" alt="image-20200612163829515" style="zoom:50%;"></p>
<p><strong>LIME-Image</strong></p>
<p>刚才说了general的情况，下面我们讲解LIME应用于image的情况</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200612164116470.png" alt="image-20200612164116470" style="zoom:50%;"></p>
<ol>
<li>首先需要一张需要解释的image；为什么这张图片可以被classify为树蛙？</li>
<li>sample at the nearby：首先把image分成多个segment，再随机去掉图中的一些segment，就得到了不同的新图片，这些新的图片就是sample的结果；再把这些新生成的图片输入black box，得到新图片是frog的可能性；</li>
<li>fit with linear model：即找到一个linear model来fit第3步输出的结果；先extract生成的新图片的特征，再把这些特征输入linear model；</li>
</ol>
<p>Q：那么如何将image转化为一个vector呢？</p>
<p>A：这里我们将image中的每个segment使用$x_i$来表示，其中$i=1,…,m,…,M$，M为segment的数量；$x_i$为1，表示当前segment被deleted，如果为0，表示exist；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200612165520599.png" alt="image-20200612165520599" style="zoom:50%;"></p>
<p>4.$\ $Interpret the model：对于学习出来的linear model，我们就可以对其进行interpret；首先需要将$x_i$和y的关系用一个公式表示出来，即</p>
<script type="math/tex; mode=display">
y=w_1x_1+..+w_mx_m+...+w_Mx_M</script><p>对于$w_m$的值，有以下三种情况：</p>
<ul>
<li>$w_m\approx 0$，segment $x_m$被认为对分类为frog没有影响；</li>
<li>$w_m&gt; 0$， $x_m$对图片分类为frog是有正面的影响的；</li>
<li>$w_m&lt;0$， 看到这个segment，反而会让机器认为图片不是frog</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200612170336857.png" alt="image-20200612170336857" style="zoom:50%;"></p>
<h5 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h5><p>如果我们用不限制深度的decision tree，那么我们就可以使用decision tree来模拟black box（neural network），使两者的输出相近</p>
<p>但decision tree的深度不可能是没有限制的。这里我们设neural network的参数为$\theta$，decision tree的参数为$T_\theta$，使用$O(T_\theta)$来表示$T_\theta$的复杂度，复杂度可以用$T_\theta$的深度来表示，也可以用neural的个数来表示；<u>现在我们的目标不仅是使两者输出相近，还需要使$O(T_\theta)$的值最小化</u></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200612171916382.png" alt="image-20200612171916382" style="zoom:50%;"></p>
<p>那么我们如何来实现使$O(T_\theta)$越小越好呢？</p>
<p>如下图所示，我们首先训练一个network，这个network可以很容易地被decision tree解释，使decision tree的复杂度没有那么高；这里我们加入了一个正则项$\lambda O(T_\theta)$，在训练network的同时，不仅要最小化loss function，还需要使$O(T_\theta)$的值尽量小，这时需要找到的network参数为$\theta^*$，</p>
<script type="math/tex; mode=display">
\theta^*=arg\ {\rm min}\ L(\theta) + \lambda O(T_\theta)</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200612172838929.png" alt="image-20200612172838929" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>explainable machine learning</tag>
        <tag>LIME</tag>
      </tags>
  </entry>
  <entry>
    <title>Theory behind GAN</title>
    <url>/2020/07/08/GAN-theory/</url>
    <content><![CDATA[<p>本文主要介绍了GAN的基础理论。还对似然函数和KL散度的关系进行了推导。</p>
<h4 id="Generation"><a href="#Generation" class="headerlink" title="Generation"></a>Generation</h4><p>现在我们用x来表示一张图像，是一个高维的vector，比如图像大小是$64\times64$维的，那么vector的维数就是$64\times64$。每张图像都是这个高维空间中的一个点。为了方便展示，下图中我们假设图像是二维空间中的一个点。</p>
<p>对于我们要产生的图像，有一个固定的distribution $P_{data}(x)$。在整个图像所构成的高维空间中，只有一小部分sample出来的图像和人脸接近，其他部分都不像人脸。比如我们从下图中蓝色的distribution中进行sample，看起来就很像是人脸，在其他区域就不像人脸。</p>
<p>那么我们现在的目标就是找到这个distribution。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708134708130.png" alt="image-20200708134708130" style="zoom:60%;"></p>
<p>在有GAN之前，我们通常用Maximum Likelihood Estimation来做这件事。</p>
<h4 id="Maximum-Likelihood-Estimation"><a href="#Maximum-Likelihood-Estimation" class="headerlink" title="Maximum Likelihood Estimation"></a>Maximum Likelihood Estimation</h4><ol>
<li>我们可以从这个distribution中sample图像，但我们并不知道其对应的formula长什么样子；</li>
<li>那么我们现在就可以找到另外一个distribution $P_G(x;\theta)$，比如其对应的参数可以是$\mu,\sum$，来使这个distribution的参数和原来的相接近。</li>
</ol>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708141336553.png" alt="image-20200708141336553" style="zoom:60%;"></p>
<p>具体做法如下，</p>
<ul>
<li>先从原来的distribution中sample出$\{x^1,x^2,…x^m\}$；</li>
<li>把$x^i$代入现在的已知的distribution $P_G(x^i;\theta)$，表示$x^i$是从现在这个distribution中sample出来的概率；</li>
<li>把这些概率相乘，得到似然函数L；最后找到对应的参数$\theta$，使似然函数取得最大值。</li>
</ul>
<script type="math/tex; mode=display">
L=\prod_{i=1}^mP_G(x^i;\theta)</script><h4 id="Minimize-KL-Divergence"><a href="#Minimize-KL-Divergence" class="headerlink" title="Minimize KL Divergence"></a>Minimize KL Divergence</h4><p>最大似然估计也就等同于来最小化KL divergence。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708142636137.png" alt="image-20200708142636137" style="zoom:60%;"></p>
<p>现在我们的问题是找到参数$\theta^*$，使得$E_{x\sim P_{data}}[logP_G(x;\theta)]$可以取得最大值。$\{x^1,x^2,…x^m\}$是从distribution $P_{data}$中sample出来的，我们把这里的$E_{x\sim P_{data}}$展开，从离散值变到连续值，即</p>
<script type="math/tex; mode=display">
E_{x\sim P_{data}}[logP_G(x;\theta)]=\int_xP_{data}(x)[logP_G(x;\theta)]dx</script><p>由于我们的目标是找到$P_G$分布对应的参数，现在加入一个常数项$\int_xP_{data}(x)[logP_{data}(x;\theta)]$，对最大化的问题也不会产生影响，即</p>
<script type="math/tex; mode=display">
\begin{aligned}
&arg\mathop{\rm  max}_{\theta} E_{x\sim P_{data}}[logP_G(x;\theta)]\\
=&arg\mathop{\rm  max}_{\theta}\int_xP_{data}(x)[logP_G(x;\theta)]dx\\
=&arg\mathop{\rm  max}_{\theta}\int_xP_{data}(x)[logP_G(x;\theta)]dx-\int_xP_{data}(x)[logP_{data}(x;\theta)]dx\\
=&arg\mathop{\rm  max}_{\theta}\int_xP_{data}(x)[logP_G(x;\theta)-logP_{data}(x;\theta)]dx\\
=&arg\mathop{\rm  max}_{\theta}\int_xP_{data}(x)[log\frac{P_G(x;\theta)}{P_{data}(x;\theta)}]dx\\
=&arg \mathop{\rm  max}_{\theta}-\int_xP_{data}(x)[log\frac{P_{data}(x;\theta)}{P_{G}(x;\theta)}]dx\\
=&arg \mathop{\rm  min}_{\theta}\int_xP_{data}(x)[log\frac{P_{data}(x;\theta)}{P_{G}(x;\theta)}]dx\\
=&arg \mathop{\rm  min}_{\theta}KL(P_{data}||P_G)
\end{aligned}</script><p>就把这个最大化似然函数问题转化为了最小化KL divergence的问题。</p>
<p>那么我们如何来定义$P_G$的表达式呢？</p>
<p>首先$P_G$是类似于高斯分布这样的distribution，很容易计算出其对应的likelihood；但如果是neural network这样的distribution，就很难算出这个likelihood。</p>
<h4 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h4><p>现在分布是neural network，如果我们还是用高斯分布的公式来进行调整，那么我们不管怎么变化mean和variance，其分布都不可能和neural network相接近。因此我们需要一个更加general的方式来学习generative这件事。</p>
<p>现在我们有一个generator G，input z是从normal distribution中sample出来的，output为$x=G(z)$，不同的z就会有不同的x，x就组成了一个新的distribution $P_G(x)$。这个distribution可以非常复杂，比如neural network。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708154052356.png" alt="image-20200708154052356" style="zoom:60%;"></p>
<p>我们希望通过G得出的这个distribution $P_G(x)$，与目标$P_{data}(x)$可以越接近越好，即$P_G,P_{data}$之间的divergence可以越小越好，可以是KLdivergence，也可以是其他的divergence，即</p>
<script type="math/tex; mode=display">
G^*=arg\mathop{\rm  min}_{\theta}Div(P_G,P_{data})</script><p>那么我们怎么来minimize这个divergence呢？</p>
<p>如果我们知道$P_G,P_{data}$的formulation，那么我们就可以计算出divergence，再使用gradient descent算法。但现在我们并不知道他们的formulation，就不能使用gradient descent算法。</p>
<h4 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h4><p>虽然我们并不知道这两者的distribution，但我们可以从这两个分布sample 很多data出来。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708154159852.png" alt="image-20200708154159852" style="zoom:60%;"></p>
<p>从这两个分布中sample很多data出来，又如何来计算分布之间的divergence呢？</p>
<p>我们可以使用GAN的discriminator来完成这个任务。</p>
<p>在下图中，我们使用蓝色星星表示从$P_{data}$中sample出来的数据，红色星星表示从$P_G$中sample出来的数据。再来训练我们的discriminator D，D对$P_{data}$中sample出来的数据会给高分，从$P_G$中sample出来的数据给低分。这个训练的结果就可以告诉我们$P_G,P_{data}$之间的divergence。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708161546891.png" alt="image-20200708161546891" style="zoom:60%;"></p>
<p>先fix掉G的参数，再来训练discriminator D，D得出的分数越大越好。找到$D^*$，使得$V(G,D)$最大化。</p>
<script type="math/tex; mode=display">
V(G,D)=E_{x\sim P_{data}}[log(D(x))]+E_{x\sim P_{G}}[1-log(D(x))]</script><p>其中$E_{x\sim P_{data}}[log(D(x))]$表示真实图像所得到的分数，D的目标就是使真实图像获得的分数越大越好；而$E_{x\sim P_{G}}[log(D(x))]$表示G生成的图像所得到的分数，应该越小越好，所以前面加了负号。</p>
<p>这个$V(D,G)$的表达式其实和二分类的问题是一样的，红色星星表示class 1，蓝色星星表示class 2，discriminator的任务就是对这两个class进行分类，来最小化cross entropy，也就相当于在解这个问题</p>
<script type="math/tex; mode=display">
D^*=arg\mathop{\rm  max}_{G}V(D,G)</script><p>我们最后找到的$D^*$，能使objective function $V(D,G)$取得最大值。这个objective function和divergence是有一定关系的。如果这两个类别之间很接近、很难区分，分类器训练的时候loss就会很大，对应的V的值就会很低，对应的divergence的值也会很低；如果这两个类别很好区分，discriminator就很容易找到$D$，使得V取得很大的值，从而divergence的值就会很大。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708163901367.png" alt="image-20200708163901367" style="zoom:60%;"></p>
<h4 id="V-G-D-和divergence之间的关系"><a href="#V-G-D-和divergence之间的关系" class="headerlink" title="V(G,D)和divergence之间的关系"></a>V(G,D)和divergence之间的关系</h4><p>根据大数定律，</p>
<script type="math/tex; mode=display">
\begin{aligned}
V(G,D)&=E_{x\sim P_{data}}[log(D(x))]+E_{x\sim P_{G}}[1-log(D(x))]\\
&=\int_x[P_{data}(x)logD(x)+P_G(x)log(1-D(x))]dx
\end{aligned}</script><p>其中我们假设$D(x)$可以是任意函数，现在我们的目标是找到其中某个D使V最大化。我们可以把积分中的x分开来算，对于其中的任意一个x，都可以分配一个最好的D函数。那么现在的问题就变成：对于给定的x，来找到最优值$D^*$使V最大化，即最大化</p>
<script type="math/tex; mode=display">
P_{data}(x)logD(x)+P_G(x)log(1-D(x))</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708164929216.png" alt="image-20200708164929216" style="zoom:60%;"></p>
<p>现在我们用$a=P_{data},b=P_G,D=D(x)$，来简化式子，即找到$D^*$来最大化</p>
<script type="math/tex; mode=display">
f(D)={\rm a}\ log(D)+{\rm b}\ log(1-D)</script><p>对D求导，并令成0，可以得到</p>
<script type="math/tex; mode=display">
D^*=\frac{a}{a+b}\quad\rightarrow\quad D^*(x)=\frac{P_{data}}{P_{data}+P_G}</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708170505053.png" alt="image-20200708170505053" style="zoom:60%;"></p>
<p>把$D^*(x)$代入objective function，可以得到</p>
<script type="math/tex; mode=display">
\begin{aligned}
&maxV(G,D)=V(G,D^*)\\&=E_{x\sim P_{data}}[log\frac{P_{data}}{P_{data}+P_G}]+E_{x\sim P_{G}}[log\frac{P_{G}}{P_{data}+P_G}]\\
&=\int_x[P_{data}(x)log\frac{P_{data}}{P_{data}+P_G}+P_G(x)log\frac{P_{G}}{P_{data}+P_G}]dx
\end{aligned}</script><p>对式子中log部分的分子分母同时除以2，把分子上的1/2提出来，可得</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\quad maxV(G,D)\\
&=-2log2+\int_x[P_{data}(x)log\frac{P_{data}}{(P_{data}+P_G)/2}+P_G(x)log\frac{P_{G}}{(P_{data}+P_G)/2}]dx\\
&=-2log2+KL(P_{data}||\frac{P_{data}+P_G}{2})+KL(P_{G}||\frac{P_{data}+P_G}{2})\\
&=-2log2+2JSD(P_{data}||P_G)\quad\quad(1)
\end{aligned}</script><p>也就得到了我们的<span style="color: red"> Jensen-Shannon divergence</span>，即JSD。</p>
<p>我们希望通过G得出的这个distribution $P_G(x)$，与目标$P_{data}(x)$可以越接近越好，即$P_G,P_{data}$之间的divergence可以越小越好，即</p>
<script type="math/tex; mode=display">
G^*=arg\mathop{\rm  min}_{G}Div(P_G,P_{data})</script><p>那么我们到底怎么算$P_G,P_{data}$之间的divergence $Div(P_G,P_{data})$呢？</p>
<p>根据公式(1)，我们知道了JS divergence和$maxV(G,D)$之间的关系，是成正比的。那么现在我们找到D，使objective function取得最大值，这个最大值就是divergence。那么现在的式子就变成了，</p>
<script type="math/tex; mode=display">
G^*=arg\mathop{\rm  min}_{G}\mathop{\rm  max}_DV(G,D)</script><p>其中要最大化discriminator得出的分数，最小化generator生成的数据与$P_{data}$之间的差距。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708192757596.png" alt="image-20200708192757596" style="zoom:60%;"></p>
<p>假设现在只有三个generator $G_1,G_2,G_3$可供选择，对应的三个objective function变化如上图所示，横坐标表示选择了不同的discriminator，纵坐标表示$V(G_i,D)$的值。第一幅图表示选择固定G1，变化discriminator，V的值的变化曲线。</p>
<p>图中红色圆点所在的横坐标，表示$V(G_i,D)$值最大的位置，一共有三个。现在已经找到了使V最大的discriminator，接下来需要找使$max V(G,D)$最小的generator（$G_1,G_2,G_3$）。毫无疑问是第三幅图中的$G_3$，是可以使得$max V(G,D)$最小的generator。</p>
<p>红色圆点的纵坐标$V(G_i,D)$表示$P_{G_i},P_{data}$之间的divergence，也是第三幅图中的divergence最小。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708192923990.png" alt="image-20200708192923990" style="zoom:60%;"></p>
<p>其实GAN的两个训练步骤就是在解决这个最大最小化问题。</p>
<h4 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h4><p>我们把目标式子简化一下，现在D是一个给定的值，可以让$V(G,D)$的值最大化，$\mathop{\rm  max}_DV(G,D)$可以表示为$L(G)$，即找到$G^*$，</p>
<script type="math/tex; mode=display">
G^*=arg\mathop{\rm  min}_{G}L(G)</script><p>先计算出gradient  $\frac{\partial L(G)}{\partial \theta_G}$，再来更新$\theta_G$的参数。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708201616936.png" alt="image-20200708201616936" style="zoom:60%;"></p>
<p>Q：其中$L(G)=\mathop{\rm  max}_DV(G,D)$，我们可以对这个函数求微分吗 ？</p>
<p>A：可以。如果现在有一个函数$f(x)=max\{f_1(x),f_2(x),f_3(x)\}$，其对应的函数图像如上图所示，是个分段函数。我们假设$f_1(x)$的函数值是最大的，那么$\frac{df(x)}{dx}=\frac{df_1(x)}{dx}$，梯度对应的是在该区域内，函数值最大的那个梯度。更次参数更新都要注意自己在哪个region内，不同的region求导的函数不一样。</p>
<p>具体的算法流程如下：</p>
<ul>
<li>给定一个初始的generator $G_0$；</li>
<li>找到$D_0^*$，使得$V(G_0,D)$的值最大化；</li>
<li>得到$L(G)$之后，就可以对整个式子求微分，得出对应的梯度，更新generator的参数，就得到新的generator $G_1$；</li>
</ul>
<p>得到新的generator后，很可能已经进入了下一个region，因此还需要重新计算discriminator，</p>
<ul>
<li>此时objective function为$V(G_1,D)$，现在是$D_1^*$使V取得最大值；</li>
<li>$L(G)=V(G_1,D_1^*)$，再更新generator的参数。</li>
</ul>
<p>……</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708203440933.png" alt="image-20200708203440933" style="zoom:60%;"></p>
<p>更新参数这个过程到底是不是在减小JS divergence呢？</p>
<p><span style="color: blue">（此处公式渲染错误，可去<a href="https://scarleatt.gitee.io/ML-notes/ML-notes-html/22_theory_behind_GAN.html" target="_blank" rel="noopener">这个网站</a>查看完整文章。）</span></p>
<p>首先我们找到了$D_0^<em>$，使V取得最大值$V(G_0,D_0^</em>)$，也就是JS divergence取得最大值；在generator更新参数之后，objective function也发生了变化$V(G_0,D)\rightarrow V(G_1,D)$ ，$D_0^<em>$对应的$V(G_0,D_0^</em>)$并不是现在的最大值，而$D_1^<em>$对应的$V(G_0,D_1^</em>)$才是最大值。从图中可以看出，$V(G_0,D_1^<em>)&lt;V(G_0,D_0^</em>)$，对应的JS divergence反而减小了。</p>
<p>如果generator的参数变化不大，即$D_0^<em>\approx D_1^</em>$，我们就可以把这个过程看作是在减小divergence。在实际的操作中，我们应该使G迭代的次数减少， 使D迭代的次数增加。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708205829068.png" alt="image-20200708205829068" style="zoom:60%;"></p>
<h4 id="In-practice-…"><a href="#In-practice-…" class="headerlink" title="In practice …"></a>In practice …</h4><p>fix G的参数，得到G生成的图像$\{\tilde x^1,\tilde x^2,..,\tilde x^m\}$，再输入discriminator D，不断调整$\theta_d$，使得得到的分数越大越好，</p>
<script type="math/tex; mode=display">
\tilde V =\frac{1}{m}\sum_{i=1}^m logD(x^i)+\frac{1}{m}\sum_{i=1}^mlog(1-D(\tilde x^i))</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708212957397.png" alt="image-20200708212957397" style="zoom:60%;"></p>
<p>这个discriminator其实就是在做binary classifier；</p>
<p>这是整个算法的步骤：</p>
<p>Learning D：首先从数据库中取出m个真实图片，再根据一个分布随机产生m个vector作为输入$\{z^1,z^2,..,z^m\}$，此时fix G的参数，得到G生成的图像$\{\tilde x^1,\tilde x^2,..,\tilde x^m\}$，再输入discriminator D，不断调整$\theta_d$，使得得到的分数越大越好。V的值最大的时候，divergence的值才越小。</p>
<script type="math/tex; mode=display">
\tilde V =\frac{1}{m}\sum_{i=1}^m logD(x^i)+\frac{1}{m}\sum_{i=1}^mlog(1-D(\tilde x^i))</script><p>其中$D(x^i)$表示真实图像所得到的分数，D的目标就是使真实图像获得的分数越大越好；而$D(\tilde x^i)$表示G生成的图像所得到的分数，应该越小越好，所以前面加了负号。求出梯度$\Delta \tilde V(\theta_d)$，再更新$\theta_d$的值，</p>
<script type="math/tex; mode=display">
\theta_d\leftarrow \theta_d+\eta\Delta \tilde V(\theta_d)</script><p>Learning G：把D训练好之后，我们就可以fix D，来训练generator G的参数。首先也需要从分布中随机生成一些噪声z，再输入G，$G(z^i)$再输入D，得到相对应的分数，G的目标是想办法骗过D，不断调整参数$\theta_g$，使下面的objective function最小化，<u>generator不能train太多次，通常update一次就好</u>。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\tilde V &=\frac{1}{m}\sum_{i=1}^m log(1-D(G(z^i)))
\end{aligned}</script><p>求出梯度$\Delta \tilde V(\theta_g)$，再更新$\theta_g$的值，</p>
<script type="math/tex; mode=display">
\theta_g\leftarrow \theta_g-\eta\Delta \tilde V(\theta_g)</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708222236749.png" alt="image-20200708222236749" style="zoom:60%;"></p>
<h4 id="Objective-Function-for-Generator-in-Real-Implementation"><a href="#Objective-Function-for-Generator-in-Real-Implementation" class="headerlink" title="Objective Function for Generator in Real Implementation"></a>Objective Function for Generator in Real Implementation</h4><p>实际上，我们使用$V=E_{x\sim P_G}[-log(D(x))]$，可以更方便进行code。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200708223556261.png" alt="image-20200708223556261" style="zoom:60%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>GAN</tag>
        <tag>Maximum Likelihood Estimation</tag>
        <tag>Minimize KL Divergence</tag>
        <tag>似然函数和KL散度的关系</tag>
      </tags>
  </entry>
  <entry>
    <title>Introduction of Generative Adversarial Network (GAN)</title>
    <url>/2020/07/04/GAN-intro/</url>
    <content><![CDATA[<p>有很多种不同类型GAN，可以在这里查看<a href="https://github.com/hindupuravinash/the-gan-zoo" target="_blank" rel="noopener">https://github.com/hindupuravinash/the-gan-zoo</a></p>
<h4 id="Generation"><a href="#Generation" class="headerlink" title="Generation"></a>Generation</h4><p>对于Image generation，要实现的是输入一个vector，输出一个image；而对于Sentence generation，则实现的是输入一个vector，输出为一个sentence。那么GAN就是用来实现这个中间的NN Generator。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704140831850.png" alt="image-20200704140831850" style="zoom:60%;"></p>
<h4 id="Basic-Idea-of-GAN"><a href="#Basic-Idea-of-GAN" class="headerlink" title="Basic Idea of GAN"></a>Basic Idea of GAN</h4><h5 id="Generator-vs-Discriminator"><a href="#Generator-vs-Discriminator" class="headerlink" title="Generator vs Discriminator"></a>Generator vs Discriminator</h5><p>这个generator可以是一个神经网络，也可以是一个函数f。</p>
<p>输入的vector表示我们要generate图像的某种特征，比如vector的第一维如果代表头发的长度，我们现在把这个值设得很大，那么就会generate一张头发很长的图像；如果我们改变了vector倒数第二维（头发为蓝色）的值，可以发现generate的新图头发变蓝了，由于此时只改变了头发的颜色，其他特征都是类似的，所以只有头发的颜色发生了变化。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704141328771.png" alt="image-20200704141328771" style="zoom:60%;"></p>
<p>Discriminator可以是一个神经网络，也可以是一个函数f。输入是一张image，输出为scalar，数值越大，表示这个image越接近真实图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704141804394.png" alt="image-20200704141804394" style="zoom:60%;"></p>
<p>在大自然中，某种鸟类以枯叶蝶为食。枯叶蝶必须不断地进化，使其看起来越来越像一个枯叶；它的天敌也在不断地进化，如果枯叶蝶看起来并不像叶子，那么它就会被捕食。其中枯叶蝶就相当于一个generator，天敌就相当于discriminator。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704142147503.png" alt="image-20200704142147503" style="zoom:60%;"></p>
<p>第一代的generator不知道怎样产生二次元的头像，所以会产生一些看起来很像是杂讯的图像，再把这个图像输入discriminator，来判断这是不是一张真实的图像，第一代的discriminator可以根据图像是否有颜色，来正确分辨真实图像和生成的图像；</p>
<p>那么第二代的generator的目标就是想办法骗过第一代的discriminator，生成了有颜色的图像，随之discriminator也会发生进化，学习了真实图像和生成图像之间的差异（真实图像是有嘴的）；</p>
<p>第三代的generator生成的图像可以骗过第二代的discriminator，然后discriminator也会继续进化，……</p>
<p>generator和discriminator都会不断地进化，因此generator会产生越来越像真实图片的图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704142643957.png" alt="image-20200704142643957" style="zoom:60%;"></p>
<p>这个过程可以看作是一个对抗的过程，也看一用另外一个和平的比喻来进行解释。generator相当于一个学生，discriminator相当于一个老师，学生并不知道真实的图像长什么样，但老师看过很多真实的图像，就知道真实的图像应该长什么样子。</p>
<p>第一代的generator相当于一年级的学生，重复着上述的过程。学生会画得越来越好，老师会越来越严格，那么学生最后就可以画出很想二次元人物的图像了。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704145218224.png" alt="image-20200704145218224" style="zoom:60%;"></p>
<h5 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h5><p>首先需要随机初始化G和D的参数，</p>
<p>Step 1：我们需要先调整D的参数，就必须先把G的参数固定。首先把随机产生的vector输入G（fix），生成新的图像之后与从database中sampled出来的图像进行比较。要实现D如果输入真实图像，就会得高分，与1越接近越好，如果输入生成的图像，就会得低分，与0越接近越好。有了这个标准，我们就可以来训练这个discriminator D；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704145820900.png" alt="image-20200704145820900" style="zoom:60%;"></p>
<p>Step2：训练好discriminator D之后，我们就可以fix D，来调整generator G。一个vector输入第一代G之后，会生成一张图像，再输入D（fix），就可以得到一个很低的分数（0.13）。那么G训练的目标就是使生成的图片可以“骗”过D，即生成的图片使D给出一个比较高的分数。由于D看过真实的图像，如果给出了很高的分数，就可以说明G生成的图像和真实图像是很接近的。</p>
<p>在真实的代码实现中，我们通常会把generator和discriminator当成是一个大的network，其中generator的输出就可以看作是一个hidden layer，discriminator所在的层参数是fix的，不用调整，只需要根据整个网络的输出来调整generator的参数。</p>
<p>由于我们希望使discriminator的输出分数值越大越好，因此这里使用了梯度上升算法 <strong>Gradient Ascent</strong>，也就是梯度下降算法前面多乘了一个负号。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704150612063.png" alt="image-20200704150612063" style="zoom:60%;"></p>
<p>现在来叙述一下总的算法流程，$\theta_d,\theta_g$分别表示discriminator和generator的参数。</p>
<p>Learning D：首先从数据库中取出m个真实图片，再根据一个分布随机产生m个vector作为输入$\{z^1,z^2,..,z^m\}$，此时fix G的参数，得到G生成的图像$\{\tilde x^1,\tilde x^2,..,\tilde x^m\}$，再输入discriminator D，不断调整$\theta_d$，使得得到的分数越大越好，</p>
<script type="math/tex; mode=display">
\tilde V =\frac{1}{m}\sum_{i=1}^m logD(x^i)+\frac{1}{m}\sum_{i=1}^mlog(1-D(\tilde x^i))</script><p>其中$D(x^i)$表示真实图像所得到的分数，D的目标就是使真实图像获得的分数越大越好；而$D(\tilde x^i)$表示G生成的图像所得到的分数，应该越小越好，所以前面加了负号。为了方便求梯度，在式子前面介入了log，求出梯度$\Delta \tilde V(\theta_d)$，再更新$\theta_d$的值，</p>
<script type="math/tex; mode=display">
\theta_d\leftarrow \theta_d+\eta\Delta \tilde V(\theta_d)</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704152035892.png" alt="image-20200704152035892" style="zoom: 67%;"></p>
<p>Learning G：把D训练好之后，我们就可以fix D，来训练generator G的参数。首先也需要从分布中随机生成一些噪声z，再输入G，$G(z^i)$再输入D，得到相对应的分数，G的目标是想办法骗过D，不断调整参数$\theta_g$，使生成的图像所得到的分数越高越好，</p>
<script type="math/tex; mode=display">
\tilde V =\frac{1}{m}\sum_{i=1}^m logD(G(z^i))</script><p>求出梯度$\Delta \tilde V(\theta_g)$，再更新$\theta_g$的值，</p>
<script type="math/tex; mode=display">
\theta_g\leftarrow \theta_g+\eta\Delta \tilde V(\theta_g)</script><p>在每个iteration里，都会进行这个步骤，先训练discriminator，再训练generator；这两个步骤会反复进行。</p>
<h5 id="Anime-Face-Generation"><a href="#Anime-Face-Generation" class="headerlink" title="Anime Face Generation"></a>Anime Face Generation</h5><p>结果展示</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704154439615.png" alt="image-20200704154439615" style="zoom:60%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704154503164.png" alt="image-20200704154503164" style="zoom:60%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704154528242.png" alt="image-20200704154528242" style="zoom:60%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704154544426.png" alt="image-20200704154544426" style="zoom:60%;"></p>
<h4 id="GAN-as-structured-learning"><a href="#GAN-as-structured-learning" class="headerlink" title="GAN as structured learning"></a>GAN as structured learning</h4><h5 id="Structed-learning"><a href="#Structed-learning" class="headerlink" title="Structed learning"></a>Structed learning</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704155106998.png" alt="image-20200704155106998" style="zoom:60%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704155202016.png" alt="image-20200704155202016" style="zoom:60%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704155216943.png" alt="image-20200704155216943" style="zoom:60%;"></p>
<h5 id="Why-Structured-Learning-Challenging"><a href="#Why-Structured-Learning-Challenging" class="headerlink" title="Why Structured Learning Challenging?"></a>Why Structured Learning Challenging?</h5><p>One-shot/Zero-shot Learning，如果有的类别都没有范例，或者只有很少一部分的范例。</p>
<p>而structured learning是一种极端的One-shot learning，由于output为一个structure，比如一个句子，可能这些句子在training data中从来没出现过，那么如何学习去输出一个从来没看到的structure，machine必须学会去创造。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704155430757.png" alt="image-20200704155430757" style="zoom:60%;"></p>
<p>machine还必须学会如何去planing，有全局观；比如sentence generation中，如果只看第一句话，会认为是负面的，但如果你把整句话都看完，就会发现这整句话在表达一个正面的意思。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704160320919.png" alt="image-20200704160320919" style="zoom:60%;"></p>
<h5 id="Structured-Learning-Approach"><a href="#Structured-Learning-Approach" class="headerlink" title="Structured Learning Approach"></a>Structured Learning Approach</h5><p>structured learning有两套方法：</p>
<ul>
<li>Bottom up，机器在生成一个部件时，会先生成多个component，这种方法一个很大的问题就是容易失去大局观；</li>
<li>Top down，产生一个完整的物件之后，再去从整体上看产生物件好不好。</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704160908741.png" alt="image-20200704160908741" style="zoom:60%;"></p>
<p>把这两种方法结合起来就是Generator。</p>
<h4 id="Can-Generator-learn-by-itself"><a href="#Can-Generator-learn-by-itself" class="headerlink" title="Can Generator learn by itself?"></a>Can Generator learn by itself?</h4><h5 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h5><p>对于Generator，首先输入不同的vector，就可以输出不同的图片。如果我们现在输入1对应的vetor，generator会生成一张image，目标是使image和真实的图像越接近越好，这个真实图像现在generator能看到，那么这不就和一般的supervised learning一摸一样了吗？</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704161403974.png" alt="image-20200704161403974" style="zoom:60%;"></p>
<p>那么我们怎么知道输入的那些vector的数值呢？</p>
<p>我们可以用一个Encoder来表示，把image输入这个NN Encoder，就会输出对应的特征，把图像的特征用vector来表示即可。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704162515395.png" alt="image-20200704162515395" style="zoom:60%;"></p>
<h5 id="Auto-encoder"><a href="#Auto-encoder" class="headerlink" title="Auto-encoder"></a>Auto-encoder</h5><p>Auto-encoder分为encoder和decoder。对于输入的$28\times 28$图像，先用encoder使得输入的图像变成code，decoder把这个code再恢复成原来的图像，这两者会一起进行学习。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704162914235.png" alt="image-20200704162914235" style="zoom:60%;"></p>
<p>目标是使得input和output越接近越好。这里的decoder就相当于一个generator，我们可以随机输入一个vector，使decoder（generator）生成一张对应的图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704163120815.png" alt="image-20200704163120815" style="zoom:60%;"></p>
<p>这里的code可以训练成二维的。如果code是$(-1.5\ \ 0)^T$，输入decoder会生成对应的图像，即图中的0；如果code是$(1.5\ \ 0)^T$，输入decoder会生成对应的图像，即图中的1.</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704163400098.png" alt="image-20200704163400098" style="zoom:60%;"></p>
<p>如果我们在这个范围内等距地sample，就有以下结果，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704163657239.png" alt="image-20200704163657239" style="zoom:60%;"></p>
<p>但使用auto-encoder也会产生一些问题，如果输入vector为a，那么会产生1的图像，如果输入vector为b，会产生斜着的1；</p>
<p>Q：那么0.5a+0.5b的vector会产生什么样的结果呢？</p>
<p>A：由于NN Generator是network，不是线性的，很可能产生不是数字的图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704163846888.png" alt="image-20200704163846888" style="zoom:60%;"></p>
<h5 id="VAE（Variational-Auto-encoder）"><a href="#VAE（Variational-Auto-encoder）" class="headerlink" title="VAE（Variational Auto-encoder）"></a>VAE（<strong>Variational Auto-encoder</strong>）</h5><p>我们可以用VAE来解决这个问题，输入vector a之后，generator不仅产生code m，还会产生每一维的方差$\sigma$，还有一个额外的vector e（noise），最后根据$c_i=exp(\sigma_i)\times e_i+m_i$得出需要的code；decoder需要根据这个带有noise的code，来还原出和input类似的图像。</p>
<p>那么现在machine不仅看到vector a会产生数字，看到vector b也会产生数字；看到vector a+noise也会产生数字，看到vector b+noise也会产生数字。</p>
<p>因此，<span style="color: red">对于现在的generator，如果input是在训练的时候从来没见过的vector，也可能output出合理的object。</span></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704164316549.png" alt="image-20200704164316549" style="zoom:60%;"></p>
<h5 id="What-do-we-miss"><a href="#What-do-we-miss" class="headerlink" title="What do we miss?"></a>What do we miss?</h5><p>现在有一张真实图像Target，通过generator生成的图像为generated image，目标是使这两者之间的差距越小越好。通常这两者之间的差距，可以通过每个像素点之间的差值来进行计算，比如可以将两张图像都表示成一个vector，再通过L1/L2计算两者之间的distance。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704165316903.png" alt="image-20200704165316903" style="zoom:60%;"></p>
<p>在一种理想的条件下，generator可以生成和Target完全一样的图像。但实际上，generator通常会犯一些错误，在生成的时候会有一些取舍，在一些地方会不得不做出妥协，选择在什么地方做妥协，对结果也会产生至关重要的影响。</p>
<p>在下图中，有一个Target，和4个Generated image。上面两张图像和target之间的差距只有一个pixel，下面两张图像和target之间的差距有6个pixel。但如果根据我们自己的观点，前两张图效果并不好，看起来并不像人写的数字，后两张图效果要好很多，因为后两张图只是对其中的一些笔画进行了延长。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704171245512.png" alt="image-20200704171245512" style="zoom:60%;"></p>
<p>因此，<span style="color: red">我们并不能单纯地只是让我们的output和target越像越好，否则就会产生上述的生成效果。</span></p>
<p>Structure  learning的输出是一个structure，由多个component组成，每个component之间的关系是非常重要的。对于下图中生成的数字图像，左边的图像有个pixel所在的位置很奇怪，不像是人手写的；但在右图中，我们添加了一些深色的pixel（和原来的pixel相邻）进去，就看起来很像是人手写的了。</p>
<p>但在neural network中，我们很难把component之间的关系放进去。在下图的网络结构中，layer L是output，表示颜色的深浅。如果layer L-1是给定的，那么输出的neural之间其实是independent的，值不会互相影响，也不会互相配合来产生一个相同的颜色。<span style="color: red">为了把不同component之间的关系也考虑进去，我们可以加入多个hidden layer</span>。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704172041505.png" alt="image-20200704172041505" style="zoom:60%;"></p>
<h5 id="GAN-vs-VAE"><a href="#GAN-vs-VAE" class="headerlink" title="GAN vs VAE"></a>GAN vs VAE</h5><p><span style="color: red">如果现在有相同的network，我们用GAN和VAE都可以用来生成图像，但VAE需要用更大的network（更多的hidden layer）才能达到和GAN接近的结果。</span></p>
<p>蓝色的点为实验结果，绿色的点表示target，VAE最好能训练的结果也就是图中的蓝色分布了，因为VAE很难考虑不同的dimension之间的关系。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704174238271.png" alt="image-20200704174238271" style="zoom:60%;"></p>
<h4 id="Can-Discriminator-generate"><a href="#Can-Discriminator-generate" class="headerlink" title="Can Discriminator generate?"></a>Can Discriminator generate?</h4><h5 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h5><p>我们再来回顾一下discriminator的概念，input为一张图像x，output为分数$D(X)$，数值越大，表示这个image越接近真实图像。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704174542805.png" alt="image-20200704174542805" style="zoom:60%;"></p>
<p>我们也可以用discriminator来生成图像。</p>
<p>我们之前提到用generator也可以用来生成图像，但考虑每个component之间的关系是非常困难的。对于discriminator，考虑每个component之间的关系，就相对而言比较容易了。</p>
<p>Q：为什么使用discriminator来生成图像就有优势？</p>
<p>A：<u>在生成的时候，来考虑每个component之间的关系是很麻烦的；但在整个图片都生成完的时候，再来判断图像每个component之间的关系就很容易了。</u></p>
<p>在下图中，如果用discriminator来生成图像，就可以根据分数来判断生成的图像到底好不好，比如左边的数字2，就可以给一个很低的分数，右边的数字2，就可以给一个很高的分数。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704174830772.png" alt="image-20200704174830772" style="zoom:60%;"></p>
<p>Q：那么我们怎么来实际来判断分数呢？</p>
<p>A：discriminator很可能是convolution neural network，对于其中的一个CNN filter（上图所示），可以来判断某个pixel周围是不是都是empty的，如果是的话，就给他低分。</p>
<p>如果我们现在有一个discriminator，我们可以通过以下方式来生成图像：穷举所有可能的输入x，看哪一个输入可以得到最高的分数。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704175856938.png" alt="image-20200704175856938" style="zoom:60%;"></p>
<h5 id="Discriminator-Training"><a href="#Discriminator-Training" class="headerlink" title="Discriminator - Training"></a>Discriminator - Training</h5><p>Discriminator的训练不仅需要真实的图像，也需要一些negative example。如果只有真实的图像，从来没有看过negative example，也不知道negative example长什么样子，D就有可能对所有的输入都输出一个很好的分数1.</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704180319560.png" alt="image-20200704180319560" style="zoom:60%;"></p>
<p>如何选择negative example也至关重要。在下图中，（左）如果negative example可以让机器很容易就分别出来（0，fake），那么如果我们输入一个相对比较清晰的图像，D就会得出一个很好的分数0.9，其实这个图像是fake的，这并不是我们想看到的结果；（右）如果输入的是非常真实的negative example，D才可以很好地鉴别real or fake image。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704180657495.png" alt="image-20200704180657495" style="zoom:60%;"></p>
<p>我们可以通过以下步骤来生成realistic negative examples，首先给出positive和negative examples。再进行不断地循环。</p>
<p>在循环中，D要给positive example很高的分数，给negative example很低的分数；学习出这个discriminator D之后，再用D来做generation（比如穷举找出$\tilde x$，使得分数取得最大值）；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704181511902.png" alt="image-20200704181511902" style="zoom:60%;"></p>
<p>找出这些D觉得还不错的图像$\tilde x$之后，进入下一次循环，将$\tilde x$与positive examples进行比较，学习出新的discriminator，……</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704181540562.png" alt="image-20200704181540562" style="zoom:60%;"></p>
<p>我们现在假设object分布在一维空间中，把object输入D，得到分数$D(x)$。在下图中，$D(x)$再real example区域取得了很不错的分数，在其他区域则分数相对较低。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704182329306.png" alt="image-20200704182329306" style="zoom:60%;"></p>
<p>但实际上，x所在的空间是非常高维的，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704183029426.png" alt="image-20200704183029426" style="zoom:60%;"></p>
<h5 id="Generator-v-s-Discriminator"><a href="#Generator-v-s-Discriminator" class="headerlink" title="Generator v.s. Discriminator"></a>Generator v.s. Discriminator</h5><p>Generator：</p>
<ul>
<li>优点：很容易做生成；</li>
<li>缺点：不容易考虑component之间的关系。</li>
</ul>
<p>Discriminator：</p>
<ul>
<li>优点：可以很容易考虑每个component之间的关系；</li>
<li>缺点：生成很慢。</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704205720871.png" alt="image-20200704205720871" style="zoom:60%;"></p>
<h5 id="Generator-Discriminator"><a href="#Generator-Discriminator" class="headerlink" title="Generator + Discriminator"></a>Generator + Discriminator</h5><p>现在我们将Generator和Discriminator结合起来，使用G来生成negative examples，现在D就不用那么费力去寻找对应的negative examples了。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704212740716.png" alt="image-20200704212740716" style="zoom:60%;"></p>
<h5 id="Benefit-of-GAN"><a href="#Benefit-of-GAN" class="headerlink" title="Benefit of GAN"></a>Benefit of GAN</h5><ol>
<li>从Discriminator的角度来看，现在我们只需要使用generator来生成negative examples，相比于之前的方法，可以更加高效；</li>
<li>从Generator的角度来看，虽然还是生成component-by-component的object，但此时G得到的feedback不再是L1或者L2 loss了，不用再去计算pixel之间的相似度，而是更具有全局观的discriminator给出的分数评价。</li>
</ol>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704213005168.png" alt="image-20200704213005168" style="zoom:60%;"></p>
<h5 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h5><p>这里的数据还使用的上文VAE的数据，只是训练方法调整为了GAN。蓝色点点是generator要学习的目标，前文的VAE没有discriminator，可以发现现在GAN比VAE的效果要好很多。在真实的环境中，VAE产生的人脸会比较模糊，GAN生成的人脸就没那么模糊。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200704213601037.png" alt="image-20200704213601037" style="zoom:60%;"></p>
<h4 id="A-little-bit-theory"><a href="#A-little-bit-theory" class="headerlink" title="A little bit theory"></a>A little bit theory</h4><p><img src="/2020/07/04/GAN-intro/image-20200704220645444.png" alt="image-20200704220645444"></p>
<p><img src="/2020/07/04/GAN-intro/image-20200704220652727.png" alt="image-20200704220652727"></p>
<p><img src="/2020/07/04/GAN-intro/image-20200704220700604.png" alt="image-20200704220700604"></p>
<p><img src="/2020/07/04/GAN-intro/image-20200704220709098.png" alt="image-20200704220709098"></p>
<p><img src="/2020/07/04/GAN-intro/image-20200704220719432.png" alt="image-20200704220719432"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>GAN</tag>
        <tag>Auto-Encoder</tag>
        <tag>VAE</tag>
      </tags>
  </entry>
  <entry>
    <title>Logistic Regression</title>
    <url>/2020/06/06/Logistic-Regression/</url>
    <content><![CDATA[<h4 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h4><p>我们可以得出z的简易表达式$z=w\cdot x + b$，可得出</p>
<script type="math/tex; mode=display">
P(C_1|x)=\sigma(z)=\sigma(w\cdot x+b)</script><p>当得出$N_1,N_2,\mu^1,\mu^2,\sum$时，就可以计算出w和b的值。</p>
<h4 id="Three-Steps"><a href="#Three-Steps" class="headerlink" title="Three Steps"></a>Three Steps</h4><h5 id="Step1-Function-Set"><a href="#Step1-Function-Set" class="headerlink" title="Step1: Function Set"></a>Step1: Function Set</h5><p>把所有的w和b都要包括进来，这里使用的function set就是sigmoid函数，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606111122013.png" alt="image-20200606111122013" style="zoom:50%;"></p>
<h5 id="Step2-Goodness-of-a-Function"><a href="#Step2-Goodness-of-a-Function" class="headerlink" title="Step2: Goodness of a Function"></a>Step2: Goodness of a Function</h5><p>对于给定的一组w和b，得出似然函数L(w,b)的表达式，对于一个二分类问题，类别C1的概率为$f_{w,b}(x^i),\ i=1,2,4,…N$，而类别C2的概率则为$1-f_{w,b}(x^3)$。找出相对应的$w^{<em>},b^{</em>}$，使得L取得最大值。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606111627142.png" alt="image-20200606111627142" style="zoom:50%;"></p>
<p>对于训练数据集，我们设C1的$\hat y=1$，C2的$\hat y =0$，服从Bernoulli distribution。在函数前面加-号就可以使原来的最大化函数，转化为对目标的最小化。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606112102628.png" alt="image-20200606112102628" style="zoom:50%;"></p>
<p>这时原来的似然函数L转化为了一个新形式，把原来的乘法变成了ln项相加，可以方便后边对w的求导</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606112315406.png" alt="image-20200606112315406" style="zoom:50%;"></p>
<p>现在我们的目标就转化为了找出$w^<em>,b^</em>=argmin-lnL(w,b)$，交叉熵的形式为</p>
<script type="math/tex; mode=display">
-lnL(w,b)=\sum_n -[\hat y^nlnf_{w,b}(x^n)+(1-\hat y^n)ln(1-f_{w,b}(x^n))</script><h5 id="Step3-Find-the-best-function"><a href="#Step3-Find-the-best-function" class="headerlink" title="Step3: Find the best function"></a>Step3: Find the best function</h5><p>为了找出那组使得$-lnL(w,b)$最小化的参数$w^<em>,b^</em>$，这里我们使用了Gradient Descent方法</p>
<script type="math/tex; mode=display">
f_{w,b}(x)=\sigma (x)=\frac{1}{1+e^{-z}},\quad z=w\cdot x+b=\sum_i w_ix_i+b</script><p>对wi求导，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606113919943.png" alt="image-20200606113919943" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606113959065.png" alt="image-20200606113959065" style="zoom:50%;"></p>
<p>分别得出$\frac{\partial lnf_{w,b}(x)}{\partial w_i},\frac{\partial ln(1-f_{w,b}(x))}{\partial w_i}$，代入原式子，化简可得</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606114230182.png" alt="image-20200606114230182" style="zoom:50%;"></p>
<p>得出梯度$\frac{\partial (-ln L(w,b))}{\partial w_i}=\sum_n -\left(\hat y^n-f_{w,b}(x^n)\right)x_i^n$，代入每次的梯度更新公式，</p>
<script type="math/tex; mode=display">
w_i\leftarrow w_i -\eta \frac{\partial (-ln L(w,b))}{\partial w_i}=w_i -\eta \sum_n -\left(\hat y^n-f_{w,b}(x^n)\right)x_i^n</script><h5 id="Logistic-Regression-Square-error是否可行"><a href="#Logistic-Regression-Square-error是否可行" class="headerlink" title="Logistic Regression + Square error是否可行"></a>Logistic Regression + Square error是否可行</h5><p>按照之前的步骤，先得出$f_{w,b}(x),L(f)$的表达式，第三步再求导，可以发现一个问题，代入训练数据集的$\hat y$后，梯度总是为0，模型最后无法训练，所以这样的结合是不可行的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606115052273.png" alt="image-20200606115052273" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606115706601.png" alt="image-20200606115706601" style="zoom:50%;"></p>
<h5 id="Cross-Entropy-v-s-Square-Error"><a href="#Cross-Entropy-v-s-Square-Error" class="headerlink" title="Cross Entropy v.s. Square Error"></a>Cross Entropy v.s. Square Error</h5><p>下图我们将Cross entropy和square error进行了对比，黑色网格线表示cross entropy，红色表示square error</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606115544160.png" alt="image-20200606115544160" style="zoom:50%;"></p>
<p>对于cross entropy，loss变化较大，曲线比较sharp，相应的微分也较大，每次跨越的步长也较长</p>
<p>对于square error，loss曲线变化比较平缓，微分值很小，每次跨越的步长也小，当gradient接近于0的时候，参数就很有可能不再更新，训练也会停下来。就算将gradient设置为很小的值，使训练不那么容易停下来，但由于每次跨越的步长很小很小，也会出现训练非常缓慢的问题</p>
<h4 id="Logistic-vs-Linear-Regression"><a href="#Logistic-vs-Linear-Regression" class="headerlink" title="Logistic vs Linear Regression"></a>Logistic vs Linear Regression</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606121346151.png" alt="image-20200606121346151" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606121324734.png" alt="image-20200606121324734" style="zoom:50%;"></p>
<h4 id="Discriminative-v-s-Generative"><a href="#Discriminative-v-s-Generative" class="headerlink" title="Discriminative v.s. Generative"></a>Discriminative v.s. Generative</h4><p>logistic regression我们称之为Discriminative方法；而我们将gaussian来描述posterior probability，称之为Generative方法。虽然都使用了相同的函数表达式，但需要找到的参数却是不同的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606122040582.png" alt="image-20200606122040582" style="zoom:50%;"></p>
<p>logistic regression<strong>没有实质性的假设</strong>，要求直接找出对应的w和b。但generative model<strong>做出了假设</strong>，假设输入的数据是服从Gaussian分布的，需要先找出$\mu^1,\mu^2,\sum^{-1}$，再根据这些值得出相对应的w和b。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606143847987.png" alt="image-20200606143847987" style="zoom:50%;"></p>
<h5 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606143950210.png" alt="image-20200606143950210" style="zoom:50%;"></p>
<p>对于包含13个example 的训练数据，对于图中所示的测试数据，我们可以明显看出测试example属于Class1，那么通过Naive Bayes（朴素贝叶斯）计算的结果也是这样吗？下面我们将开始验证，</p>
<script type="math/tex; mode=display">
P(x|C1)=P(x_1=1|C_1)\times P(x_2=1|C_1)=1\times1\\
P(x|C2)=P(x_1=1|C_2)\times P(x_2=1|C_2)=\frac{1}{3}\times\frac{1}{3}</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200606144006375.png" alt="image-20200606144006375" style="zoom:50%;"></p>
<p>根据这个计算结果可知，属于Class1的概率是小于0.5的，因此可以看出根据朴素贝叶斯算法算出，测试的example是属于Class2，和我们的直觉是相反的。==这是由于训练数据集中属于Class1的数量太少了，==比例只有1/13。在实际生活中的模型训练中，我们也必须要避免数据集的差异对实验结果造成的影响，数据集中每个类别所占的比例应该是差别不大的。</p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Logistic Regression</tag>
      </tags>
  </entry>
  <entry>
    <title>Mac安装Pytorch</title>
    <url>/2020/03/29/Mac%E5%AE%89%E8%A3%85Pytorch/</url>
    <content><![CDATA[<p>先创建一个名为<code>pytorch</code>的虚拟环境，</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda create -n pytorch python=3.7 numpy matplotlib pandas jupyter notebook</span><br></pre></td></tr></table></figure>
<p>激活虚拟环境</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda activate pytorch</span><br></pre></td></tr></table></figure>
<p>去官网选择合适的pytorch版本的安装命令<a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener">https://pytorch.org/get-started/locally/</a></p>
<p><img src="/2020/03/29/Mac安装Pytorch/image-20200329101220290.png" alt="image-20200329101220290"></p>
<p>复制到命令行下，回车运行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda install pytorch torchvision -c pytorch</span><br></pre></td></tr></table></figure>
<p>会出现以下报错信息：</p>
<p><strong>CondaHTTPError: HTTP 000 CONNECTION FAILED for url</strong></p>
<p><strong>解决方法：添加镜像站到Anaconda</strong></p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda config --add channels http://mirror.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --add channels http://mirror.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --set show_channel_urls yes</span><br><span class="line"></span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ </span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/ </span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/ </span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/ </span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br></pre></td></tr></table></figure>
<p>再把官网安装命令的<code>-c pytorch</code>去掉，运行以下命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda install pytorch torchvision</span><br></pre></td></tr></table></figure>
<p>检测pytorch是否安装成功，命令行运行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">jupyter notebook</span><br></pre></td></tr></table></figure>
<p>运行测试代码，出现以下结果，则说明安装成功</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.FloatTensor([<span class="number">5</span>, <span class="number">3</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/29/Mac安装Pytorch/image-20200329102655979.png" alt="image-20200329102655979"></p>
]]></content>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Recurrent Neural Network</title>
    <url>/2020/06/10/RNN/</url>
    <content><![CDATA[<h4 id="Example-Application"><a href="#Example-Application" class="headerlink" title="Example Application"></a>Example Application</h4><p>将Taipei作为向量的输入，由于RNN网络是有记忆的，因此可以根据输入来预测目的地dest，以及离开的时间</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609165306993.png" alt="image-20200609165306993" style="zoom:50%;"></p>
<h4 id="RNN-Example"><a href="#RNN-Example" class="headerlink" title="RNN Example"></a>RNN Example</h4><p>RNN是一个有记忆的网络。对于输入的向量$(x_1,x_2)$，hidden neural的输入就包含store和input的neural，本次hidden neural的输出又作为下一次hidden neural的部分输入</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609165404347.png" alt="image-20200609165404347" style="zoom:50%;"></p>
<p>在下图中，对于给定的input sequence为(1,1),(1,1),(2,2)，所有的weights都出实话为1，没有bias项，所有的activation function都是linear的</p>
<p>如果input sequence的第一个输入为(1,1)，store neural的初始值为0，则hidden neural的输出为2，把hidden neural的再放到store neural里（红色箭头），output neural的输出为$y_1=4,y_2=4$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609170004564.png" alt="image-20200609170004564" style="zoom:50%;"></p>
<p>第二个input为(1,1)，store neural的值为(2,2)，那么hidden layer的输出为$2+2+1+1=6$，再把hidden neural 的输出作为store neural的值，对应的$y_1=2,y_2=12$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609170504030.png" alt="image-20200609170504030" style="zoom:50%;"></p>
<p>第三个input为(2,2)，store neural的值为(6,6)，那么hidden neural的输出为$6+6+2+2=16$，对应的$y_1=32,y_2=32$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609170819485.png" alt="image-20200609170819485" style="zoom:50%;"></p>
<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><h5 id="Basic-Concept"><a href="#Basic-Concept" class="headerlink" title="Basic Concept"></a>Basic Concept</h5><p>下面将在具体叙述RNN网络的原理。</p>
<p>如果input sequence为”arrive Taipei on November 2nd”，那么第一个input记为$x^1=arrive$，再通过hidden neural计算出$a^1$，这里需要把$a^1$存入到store neural（作为下一次hidden neural计算的部分输入），再通过output neural计算出$y^1$;</p>
<p>那么第二个input记为$x^2=Taipei$，其中$x^2,a^1$都是hidden neural的输入，再通过hidden neural计算出$a^2$，这里需要把$a^2$存入到store neural（作为下一次hidden neural计算的部分输入），再通过output neural计算出$y^2$;</p>
<p>…</p>
<p>RNN的训练并不是多个网络的叠加，即在下图中，RNN不是有三个不同的network，只有一个network，每次更新相对应的input和其他neural的值即可</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609171227640.png" alt="image-20200609171227640" style="zoom:50%;"></p>
<p>这里还有一个需要注意的点，即使是input sequence中相同的单词输入(Taipei)，由于input sequence的顺序不同，store neural内存的值也不同，因此output neural的输出值也不同</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609171916224.png" alt="image-20200609171916224" style="zoom:50%;"></p>
<h5 id="Of-course-it-can-be-deep-…"><a href="#Of-course-it-can-be-deep-…" class="headerlink" title="Of course it can be deep …"></a>Of course it can be deep …</h5><p>和其他的神经网络一样，RNN也可以加深他的网络层次</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609172301903.png" alt="image-20200609172301903" style="zoom:50%;"></p>
<h5 id="Elman-Network-amp-Jordan-Network"><a href="#Elman-Network-amp-Jordan-Network" class="headerlink" title="Elman Network &amp; Jordan Network"></a>Elman Network &amp; Jordan Network</h5><p>下面将叙述两种不同的RNN网络。</p>
<p>Elman Network类似于前文讲的rnn network，将hidden neural的输出作为store neural的值；而Jordan Network则是将output neural的值存入store neural；</p>
<p>其中Jordan Network的performance比较好，因为Jordan是直接将更清晰的值target存入store neural，而Elman是将学习到的值（hidden neural的输出）存入store neural</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609172444143.png" alt="image-20200609172444143" style="zoom:50%;"></p>
<h5 id="Bidirectional-RNN"><a href="#Bidirectional-RNN" class="headerlink" title="Bidirectional RNN"></a>Bidirectional RNN</h5><p>RNN不仅可以是单向的，也可以是双向的，可以train一个正向的network，也同时是train一个逆向的network。如下图所示，input sequence为$…,x^t,x^{t+1},x^{t+2},…$，正向和逆向hidden neural的输出同时作为output neural的输入，从而输出target的值$…,y^t,y^{t+1},y^{t+2},…$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609173058539.png" alt="image-20200609173058539" style="zoom:50%;"></p>
<p>这种双向的RNN对我们的训练是有好处的。对于图中的output $y^{t+1}$，如果只是单向RNN，那么$y^{t+1}$只可以看到$x^1,…,x^{t+1}$的值；但现在是双向的RNN，$y^{t+1}$不仅可以看到$x^1,…,x^{t+1}$的值，还可以看到$x^{t+2},…$到句尾的值，<u>即整个input sequence的值都可以被双向的RNN看到，从而得出更好的训练结果</u></p>
<h4 id="Long-Short-term-Memory-LSTM"><a href="#Long-Short-term-Memory-LSTM" class="headerlink" title="Long Short-term Memory (LSTM)"></a>Long Short-term Memory (LSTM)</h4><h5 id="three-gates"><a href="#three-gates" class="headerlink" title="three gates"></a>three gates</h5><p>下图中的neural有4个input和1个output，一共有三个gate：</p>
<ul>
<li>Input Gate：决定其他neural的值是否可以输入这个neural；是否可以输入由neural的学习情况而定</li>
<li>Forget Gate：什么时候要把memory里面的东西forget，<u>打开表示memory，关闭表示forget</u>；具体什么时候要自己学习</li>
<li>Output Gate：其他的neural可不可以到这个neural把值取出来，打开的时候才可以取出来，关闭的时候则不可以；打不打开可以自己学习，自己决定</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609175328153.png" alt="image-20200609175328153" style="zoom:50%;"></p>
<h5 id="formula"><a href="#formula" class="headerlink" title="formula"></a>formula</h5><p>上面只是对LSTM做了一个简要的叙述，下面将开始详细叙述。下图中的activation function f为sigmoid function，黑色圆点表示multiply；$z_i,z_o,z_f$分别操控input、output、forget gate；z表示外界要存入的值，</p>
<p><u>如果$f(z_f)$的输出值为1，表示打开forget gate（memory）；输出值为0，表示关闭forget gate（forget）</u></p>
<p>对于输入的z值，经过activation function g运算后为$g(z)$，相应的$z_i$经过运算得$f(z^i)$，c为Memory的初始值，当forget gate开启的时候，c’表示经过一次forget gate打开时进行计算后更新的值</p>
<script type="math/tex; mode=display">
c'=g(z)f(z_i)+cf(z_f)</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609181527674.png" alt="image-20200609181527674" style="zoom:50%;"></p>
<p>memory cell的输出再输入activation function h，输出$h(c’)$，即可得出该neural的output为a</p>
<script type="math/tex; mode=display">
a=h(c')f(z_0)</script><h5 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609223053105.png" alt="image-20200609223053105" style="zoom:50%;"></p>
<p>如下图所示，箭头上的数字表示weight，activation function g和h都是linear function，LSTM初始状态如下</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609225029699.png" alt="image-20200609225029699" style="zoom:50%;"></p>
<p>第一个input vector为$z=(x_1,x_2,x_3,bias)^T=(3,1,0,1)^T$，则$g(z)=3$，Input Gate为$z_i=100\rightarrow f(z_i)\approx1$，Forget Gate为$z_f=110\rightarrow f(z_f)\approx 1$，Memory Cell内的$c=0$，那么memory cell更新后的值为c’</p>
<script type="math/tex; mode=display">
\begin{aligned}
c'&=g(z)f(z_i)+cf(z_f) \\
&=3*1+0*1\\
&=3
\end{aligned}</script><p>memory cell的输出再输入activation function h (linear)，输出$h(c’)=3$，Output Gate为$z_o=0\rightarrow f(z_0)\approx0$，即可得出该neural的output为a</p>
<script type="math/tex; mode=display">
a=h(c')f(z_0)=0</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609224201819.png" alt="image-20200609224201819" style="zoom:50%;"></p>
<p>第二个input vector为$z=(x_1,x_2,x_3,bias)^T=(4,1,0,1)^T$，…..,其中Memory保存了上一次更新的值3，同理可得a=0</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609225345356.png" alt="image-20200609225345356" style="zoom:50%;"></p>
<p>第三个input vector为$z=(x_1,x_2,x_3,bias)^T=(2,0,0,1)^T$，…..,同理可得a=0</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609225442395.png" alt="image-20200609225442395" style="zoom:50%;"></p>
<p>第四个input vector为$z=(x_1,x_2,x_3,bias)^T=(1,0,1,1)^T$，则$g(z)=1$，Input Gate为$z_i=0\rightarrow f(z_i)\approx0$，Forget Gate为$z_f=10\rightarrow f(z_f)\approx 1$，Memory Cell内的$c=7$，那么memory cell更新后的值为c’</p>
<script type="math/tex; mode=display">
\begin{aligned}
c'&=g(z)f(z_i)+cf(z_f) \\
&=1*0+7*1\\
&=7
\end{aligned}</script><p>memory cell的输出再输入activation function h (linear)，输出$h(c’)=7$，Output Gate为$z_o=90\rightarrow f(z_0)\approx1$，即可得出该neural的output为a=7</p>
<script type="math/tex; mode=display">
a=h(c')f(z_0)=7</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200609225546396.png" alt="image-20200609225546396" style="zoom:50%;"></p>
<h5 id="Original-Network-vs-LSTM"><a href="#Original-Network-vs-LSTM" class="headerlink" title="Original Network vs LSTM"></a>Original Network vs LSTM</h5><p>LSTM就是把原来的hidden layer中的neural替换为LSTM cell</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610095121581.png" alt="image-20200610095121581" style="zoom:50%;"></p>
<p>将现在的 $(x_1,x_2)$乘以不同的weight，从而可以作为4个不同输入，即input、input gate、forgte gate、output gate</p>
<p>一般的neural network只需要操控$(x_1,x_2)$，但LSTM需要操控4个不同的input，因此参数量多了4倍</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610095605339.png" alt="image-20200610095605339" style="zoom:50%;"></p>
<h5 id="LSTM-with-neural-network"><a href="#LSTM-with-neural-network" class="headerlink" title="LSTM with neural network"></a>LSTM with neural network</h5><p>z的维数则表示LSTM中memory cell（LSTM中红色方框）的个数，z的第一维表示第一个LSTM cell的input，第二维表示第二个LSTM cell的input，……</p>
<p>因此一共有4个不同的vector $z_f,z_iz,z_o$，作为network的input，操控整个LSTM的运转</p>
<p>输入到第一个LSTM cell的值是vector $z_f,z_iz,z_o$中的第一维数据，每个cell的输入值都是不一样的</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610101406355.png" alt="image-20200610101406355" style="zoom:50%;"></p>
<p>$x^t$进行不同的transform，可以得出4个不同的input $z_f,z_iz,z_o$</p>
<p>首先对于输入的z值，经过activation function g运算后为$g(z)$，相应的$z_i$经过运算得$f(z_i)$，$g(z)$再和 $f(z_i)$相乘（element-wise），memory cell的初始值为$c^{t-1}$，经过运算可得当前的值$c^t=g(z)f(z_i)+c^{t-1}f(z_f)$，经过activation function h计算出$h(c^t)$，再和output gate的输出值$f(z_o)$结合，得出当前LSTM cell的输出值$y^t=h(c^t)f(z_0)$，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610102846836.png" alt="image-20200610102846836" style="zoom:50%;"></p>
<h5 id="LSTM-Extension"><a href="#LSTM-Extension" class="headerlink" title="LSTM Extension"></a>LSTM Extension</h5><p>下图展示了两个LSTM cell的extension，在时间点t进行的计算，不仅要包括 $x^{t+1}$，还要包括上个时间点中lstm cell算出的值，即memory cell的输出$c^t$，以及$c^t$再输入activation function的输出值$h(c^t)$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610103840723.png" alt="image-20200610103840723" style="zoom:50%;"></p>
<h4 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h4><p>Training Sentences为”arrive Taipei on November 2nd”，当Taipei输入时，必须要先计算出前一层hidden layer的输出值</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610112127379.png" alt="image-20200610112127379" style="zoom:50%;"></p>
<p>RNN的训练也是前向和逆向传播，其中逆向传播算法为Backpropagation through time（BPTT），也有一个参数更新公式</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610112423397.png" alt="image-20200610112423397" style="zoom:50%;"></p>
<p>但基于RNN的network并不是很容易训练，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610112627265.png" alt="image-20200610112627265" style="zoom:50%;"></p>
<h5 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h5><p>下图中的纵轴表示total loss，横轴表示两个weight w1和w2。error surface非常的rough，图像要么很flat，要么非常steep</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610112709861.png" alt="image-20200610112709861" style="zoom:50%;"></p>
<p>假设图中最右方的橙色点表示初始值，由于刚开始error surface比较flat，gradient也比较小，learning rate比较大；经过多次的训练后，如果这时候刚好到达悬崖和平原的交接处，这一次的gradient就超级大，再加上之前很大的learning rate，loss可能就会直接飞出去了</p>
<h5 id="Clipping"><a href="#Clipping" class="headerlink" title="Clipping"></a>Clipping</h5><p>改进措施是clipping，当gradient大于15的时候，就看做是15，不再继续增加，这时gradient的移动方向就是图中的蓝色虚线部分，就不会再飞出去了，仍然可以继续做RNN training </p>
<h5 id="Why？"><a href="#Why？" class="headerlink" title="Why？"></a>Why？</h5><p>Q：由于之前的文章中提到sigmoid function会产生gradient vanshing问题，那么造成RNN训练困难的原因是因为activation function吗 ？</p>
<p>A：有学者将sigmoid function 替换为ReLU，发现RNN网络给出了更差的performance，因此这个问题的产生并不是因为activation function的问题</p>
<p>最直观的解决方式是：将某一个参数的gradient进行变化，观察这个变化会对output产生的影响</p>
<p>对于下图中的toy example，input sequence为1,0,0,0,…，hidden layer中的neural都是线性的，在上一个时间点的hidden layer的输出也会作为当前时间点的部分input，设其权重为w，那么如果输入了1000次，那么整个network的output $y^{1000}=w^{999}$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610115048161.png" alt="image-20200610115048161" style="zoom:50%;"></p>
<p>如果$w=1$，那么对应的输出$y^{1000}=1$；如果$w=1.01$，那么对应的输出$y^{1000}\approx20000$；w虽然只有小小的变化，但由于蝴蝶效应，对output的影响却很大，因此有很large的gradient $\frac{\partial L}{\partial w}$，这时就需要把learning rate设置小一点</p>
<p>如果$w=0.99$，那么对应的输出$y^{1000}\approx0$；如果$w=0.01$，那么对应的输出$y^{1000}\approx0$；w虽然有很大的变化，但对output的影响却很小，因此有很small的gradient $\frac{\partial L}{\partial w}$，这时就需要把learning rate设置大一点</p>
<p>在很小的区域内，gradient就会有很大的变化，learning rate的变化也很大</p>
<p>因此，RNN不好训练的原因并不是因为activation function的影响，而是由于weight在high frequency地被使用，weight在不同的位置、不同的时间点是会被反复地使用的</p>
<h5 id="Helpful-Techniques"><a href="#Helpful-Techniques" class="headerlink" title="Helpful Techniques"></a>Helpful Techniques</h5><p><strong>LSTM</strong>可以让error surface不那么崎岖，可以把一些很平坦的地方拿掉，使error surface不再有那么平坦的地方，这时可以把learning rate设置得小一些，<u>可以解决gradient vanishing问题</u></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610120736515.png" alt="image-20200610120736515" style="zoom:50%;"></p>
<p>传统的RNN和LSTM的区别：</p>
<ul>
<li>传统的RNN通常是直接覆盖的形式，当前时间点的计算结果直接覆盖上一个时间节点的计算结果；</li>
<li>LSTM通常采用了“累加”的形式，上一个时间节点的计算结果会影响之后的计算结果，并不是直接覆盖。</li>
</ul>
<p>LSTM可以解决gradient vanishing的原因：</p>
<ul>
<li>当前的input和memory里的值是“累加”的；</li>
<li>如果forget gate是打开的状态，上一个时间点的memory会对当前时间点造成影响，这个影响会一直持续下去</li>
</ul>
<p>LSTM的第一个版本是为了解决gradient vanishing问题，forget gate是后面才加上去的。因此在LSTM的训练中，要给forget gate非常大的bias，要保证forget gate大部分情况下都是forget的</p>
<p><strong>Gated Recurrent Unit（GRU）</strong>：比LSTM更简单，gate的数量只有俩，即forget gate和input gate，这两者有一个联动：</p>
<ul>
<li>当input gate打开时，forget gate关闭，forget掉menory里的值；</li>
<li>当forget gate打开时，记住memory里的值，input gate关闭，不再进行输入</li>
</ul>
<p>还有一些其他的技术可以解决这个问题，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610123018202.png" alt="image-20200610123018202" style="zoom:50%;"></p>
<h4 id="More-Applications-……"><a href="#More-Applications-……" class="headerlink" title="More Applications ……"></a>More Applications ……</h4><h5 id="Many-to-one"><a href="#Many-to-one" class="headerlink" title="Many to one"></a>Many to one</h5><p>input是一些vector的sequence，output可以只是一个vector</p>
<p>比如sentiment analysis，输入电影评价的sequence，输出为电影评论的类别</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610123509761.png" alt="image-20200610123509761" style="zoom: 50%;"></p>
<h5 id="Many-to-Many-Output-is-shorter"><a href="#Many-to-Many-Output-is-shorter" class="headerlink" title="Many to Many (Output is shorter)"></a>Many to Many (Output is shorter)</h5><p>input和output也可以都是一个sequence，比如语音识别speech recognition</p>
<p>对于原来的RNN，每个vector对应到某一个character，input的vector通常是很短的，比如0.01s，因此通常是很多个vector对应到同一个character</p>
<p>有一种解决方式是trimming，去掉重复的汉字，但这时就识别不出“好棒棒”，只能识别出“好棒”</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610123754697.png" alt="image-20200610123754697" style="zoom:50%;"></p>
<p>有学者提出了CTC，添加了表示null的符号进来</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610154820892.png" alt="image-20200610154820892" style="zoom:50%;"></p>
<p>当输入“机”字，根据上一个时间点的memory和“机”，RNN就可以学习到输出为“器”，…..</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610155051371.png" alt="image-20200610155051371" style="zoom:50%;"></p>
<p>如果不停止的话，rnn可能会一直生成下去，因此要加入一个“断“字，停止生成</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610155210452.png" alt="image-20200610155210452" style="zoom:50%;"></p>
<h5 id="Sequence-to-sequence-Auto-encoder-Text"><a href="#Sequence-to-sequence-Auto-encoder-Text" class="headerlink" title="Sequence-to-sequence Auto-encoder - Text"></a>Sequence-to-sequence Auto-encoder - Text</h5><p>虽然这两句话有相同的单词，但这两句话中的单词却有不同的顺序，因此表示的含义也不一样，一个是positive，一个是negative</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610155820922.png" alt="image-20200610155820922" style="zoom:50%;"></p>
<p>在考虑单词原有顺序的情况下，我们可以通过sequence2sequence auto-encoder来将一个document转化为vector，那么具体怎么做呢？</p>
<p>现在有一个input sequence，“mary was hungry. She didn’t find any food”，先通过encoder找到一个vector，再把这个vector输入decoder，找出对应的sequence</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610160241278.png" alt="image-20200610160241278" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610162641873.png" alt="image-20200610162641873" style="zoom:50%;"></p>
<h5 id="Sequence-to-sequence-Auto-encoder-Speech"><a href="#Sequence-to-sequence-Auto-encoder-Speech" class="headerlink" title="Sequence-to-sequence Auto-encoder - Speech"></a>Sequence-to-sequence Auto-encoder - Speech</h5><p>比如有一段演讲语音，现在我想搜索audio中提到了白宫的部分，这时我们就需要做如下工作</p>
<p>把输入的audio segment转化成vector，再把spoken query的audio segment转化成vector，再计算这两者的相似程度，就可以得到搜寻的结果</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610163540450.png" alt="image-20200610163540450" style="zoom:50%;"></p>
<p><strong>audio-&gt;vector</strong></p>
<p>把输入的audio segment分成4部分$x_1,x_2,x_3,x_4$，输入RNN，这里的RNN充当了一个encoder的角色，最后一个时间点在memory里存的值就是整段audio的information</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610163053020.png" alt="image-20200610163053020" style="zoom:50%;"></p>
<p>但是这个RNN的encoder是没办法train的，所以这里加入了RNN的decoder。encoder的memory里存的值作为decoder的第一个input，产生一个sequence，这个y1和x1越接近越好，再根据y1产生y2，y3，y4。这个训练的target是$y_1,y_2,y_3,y_4$与$x_1,x_2,x_3,x_4$越接近越好</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610163303283.png" alt="image-20200610163303283" style="zoom:50%;"></p>
<p>这个RNN的encoder和decoder是一起 train，其中任何一个都是没办法单独训练的，一起train就有一个target</p>
<p>将speech转化为vector之后，再进行可视化</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610164950240.png" alt="image-20200610164950240" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer详解</title>
    <url>/2020/07/13/Transformer/</url>
    <content><![CDATA[<p>本文是对论文《Attention is all you need》的理解。transform模型不仅可以应用到NLP的机器翻译领域，还可以应用到其他非NLP领域，是非常有科研潜力的一个方向。</p>
<h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>transform模型由多个encoder和decoder构成，如下图所示</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200712185243525.png" alt="image-20200712185243525" style="zoom: 67%;"></p>
<h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>Encoder由N=6个相同的layer组成，layer就是上图中左边的方框部分，左边的英文字母“Nx”表示layer的数量，这里N=6。每个layer都有两个sub-layer，即multi-head attention和position-wise feed-forward network。每个sub-layer还加入了residual connection和layer normalization，因此每个sub-layer的输出为</p>
<script type="math/tex; mode=display">
\rm sub\_layer_{output}=LayerNorm(x+Sublayer(x))</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200712214746642.png" alt="image-20200712214746642" style="zoom:67%;"></p>
<h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>Decoder也由6个相同的layer组成，layer就是上图的右边的方框部分，此时N=6。layer由3个sub-layer组成，除了encoder提到的两个sub-layer，还有Masked Multi-Head Attention。</p>
<p>新加入的masked multi-head attention，可以保证对位置i的预测只依赖位置小于i的已知输出，不会接触到未来位置的信息。</p>
<h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><p>attention有三个输入$Q,K,V$。计算attention的过程，即使用一个Query，计算它和每个Key的相似度作为权重，再对所有的Value进行加权求和。</p>
<p>attention可以通过以下式子计算，先计算Q和K的点乘，再进行scale（除以$\sqrt{d_k}$），输入softmax函数，再与V进行点乘，即可得到输出。</p>
<script type="math/tex; mode=display">
{\rm Attention}(Q,K,V)={\rm softmax}(\frac{QK^T}{\sqrt{d_k}})V</script><p>我们把输入的embedding全都放到一个矩阵$X$内，矩阵的每一行表示输入序列的一个word，$W^Q,W^K,W^V$表示Q，K，V对应的权重矩阵，可以通过网络进行学习。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200712215525200.png" alt="image-20200712215525200" style="zoom: 50%;"></p>
<p>得到Q，K，V之后，就可以进行self-attention的计算，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200712220405217.png" alt="image-20200712220405217" style="zoom: 50%;"></p>
<p>Q1：为什么选择dot-product attention，而不是additive attention？</p>
<p>A：dot-product attention可以使用高度优化过后的矩阵运算代码。</p>
<p>Q2：为什么要除以$\sqrt{d_k}$这一项？</p>
<p>A：（1）在$d_k$很小的时候，如果不进行scaling，dot-product attention和additive attention的performance相差不大；如果$d_k$值很大，那么点乘的值就会很大，不做scaling的话，效果就没有additive attention好。（2）如果$d_k$的值很大，点乘的结果也很大，输入softmax函数之后的梯度就会变得很小，不利于back propagation。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200712192858975-20200713101139647.png" alt="image-20200712192858975"></p>
<h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><p>multi-head attention则是将Q，K，V分别进行h次不同的<strong>线性变换</strong>，再把这些不同的attention结果连接起来，即</p>
<script type="math/tex; mode=display">
\rm MultiHead(Q,K,V)=Concat(head_1,..,head_h)W^O\\
\quad head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)</script><p>multi-head attention的流程图如下，1) input sequence为<span style="color: green">Thinking Machines</span>；2) embedding每个word，转化成vector；3) 分别计算8个attention head的Q，K，V；4) ；使用上一步的结果Q，K，V来计算attention的值；5) 将$Z^0,…,Z^7$的结果连接去来，形成一个大矩阵，再用这个大矩阵来乘矩阵$W^O$，得到最后的结果Z。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200713120312773.png" alt="image-20200713120312773"></p>
<h4 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h4><p>由于这个模型并没有包含RNN和CNN，并没有考虑输入序列的order，我们必须在embedding之后把序列的order也考虑进去，把位置信息positional encoding加上embedding，就得到了带位置信息的embedding。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200713100210115.png" alt="image-20200713100210115"></p>
<p>作者使用了不同频率的正弦和余弦函数来完成这项工作，</p>
<script type="math/tex; mode=display">
PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\\
PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})</script><p>其中pos就表示position，i表示dimension。positional encoding的每个维度都对应着一个正弦曲线。</p>
<p>NLP任务还需要考虑到单词的相对位置，根据下列正余弦的变化公式，</p>
<script type="math/tex; mode=display">
\rm sin(\alpha+\beta)=sin\alpha cos\beta+cos\alpha sin\beta\\
cos(\alpha+\beta)=cos\alpha cos\beta-sin\alpha sin\beta</script><p>可得出：现在对于固定的offset k，$PE_{pos+k}$可以表示成$PE_{pos}$的线性变换，</p>
<h4 id="Residual-Connection"><a href="#Residual-Connection" class="headerlink" title="Residual Connection"></a>Residual Connection</h4><p>在每个encoder的sub-layer中，在layer normalization之后，都有一个residual connection。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200713101600838.png" alt="image-20200713101600838" style="zoom:67%;"></p>
<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>优点：transform模型不仅可以应用到NLP的机器翻译领域，还可以应用到其他非NLP领域，是非常有科研潜力的一个方向。</p>
<p>缺点：（1）没有使用RNN和CNN，使模型并不能捕捉一些局部特征，使用Transform+RNN+CNN效果可能会更好；（2）Transform丢失的位置信息在NLP其实是非常重要的，加入position encoding其实只是一个权宜之计，并不能从根本上改变transform的结构缺陷。</p>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><ol>
<li><a href="https://zhuanlan.zhihu.com/p/48508221" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/48508221</a></li>
<li><p><a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">https://jalammar.github.io/illustrated-transformer/</a></p>
</li>
<li><p>Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need [C]//Advances in Neural Information Processing Systems. 2017: 5998-6008.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Knowledge Geaph</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>PCA</title>
    <url>/2020/06/28/dimension-reduction/</url>
    <content><![CDATA[<h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><p>unsupervised learning可以分为两大类，dimension reduction和generation</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628104819015.png" alt="image-20200628104819015" style="zoom:50%;"></p>
<h4 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628105521936.png" alt="image-20200628105521936" style="zoom:50%;"></p>
<p>先根据input之间的相似度建立一颗树，再用一条线切一刀，如果使用红色的线切，那么可以分为2个大类</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628110013187.png" alt="image-20200628110013187" style="zoom:50%;"></p>
<h4 id="Distributed-Representation"><a href="#Distributed-Representation" class="headerlink" title="Distributed Representation"></a>Distributed Representation</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628110332232.png" alt="image-20200628110332232" style="zoom:50%;"></p>
<h4 id="Dimension-Reduction"><a href="#Dimension-Reduction" class="headerlink" title="Dimension Reduction"></a>Dimension Reduction</h4><p>用2维就可以表示这些特征，并不需要用到3D</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628110540545.png" alt="image-20200628110540545" style="zoom:50%;"></p>
<p>这些3只有倾斜的角度不一样，用1D即可表示</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628110631658.png" alt="image-20200628110631658" style="zoom:50%;"></p>
<p>主要有两种方法，feature selection和PCA</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628110752052.png" alt="image-20200628110752052" style="zoom:50%;"></p>
<h4 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h4><p>如果reduce to 1D，我们使用$z_1=w^1 \cdot  x$，使得$x$投影到$w_1$上，即达到了降维的目的，那么我们如何来评价降维的好坏呢？</p>
<p>我们可以使用降维之后数据的variance来评价，variance越大越好</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628112108807.png" alt="image-20200628112108807" style="zoom:50%;"></p>
<p>如果reduce to 2D，那么现在就需要投影到两个不同的方向$(w^1,w^2)$上，再来与$x$做inner product，得到$z_1,z_2$，再分别计算这两者的variance；其中$w^1,w^2$要满足一定的条件，即$w^1\cdot w^2=0$，两者是垂直的，可以保证是不同的方向</p>
<p>那么W就是一个正交矩阵，向量之间相互正交，且向量模长都是1</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628112628933.png" alt="image-20200628112628933" style="zoom:50%;"></p>
<h5 id="Formula"><a href="#Formula" class="headerlink" title="Formula"></a>Formula</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628114149626.png" alt="image-20200628114149626" style="zoom:50%;"></p>
<p>由于$a^Tb$是一个scalar，所以可以直接加上转置符号</p>
<script type="math/tex; mode=display">
\begin{aligned}
(a\cdot b)^2&=a^Tba^Tb\\
&=a^Tb(a^Tb)^T
\end{aligned}</script><p>其中$Ex=\bar x$，协方差$Cov(x)$为</p>
<script type="math/tex; mode=display">
\begin{aligned}
Cov(x)&=\frac{1}{N}\sum(x-Ex)(x-Ex)^T \\
&=\frac{1}{N}\sum(x-\bar x)(x-\bar x)^T
\end{aligned}</script><p>令协方差为S，那么我们现在的问题是 maximizing $(w^1)^TSw^1$，限制条件是</p>
<script type="math/tex; mode=display">
||w^1||_2=(w^1)^Tw^1=1</script><p>先找到$w^1$，可以maximizing $(w^1)^TSw^1$，这里使用了拉格朗日乘数法，再对$w^1$求偏微分，得</p>
<script type="math/tex; mode=display">
Sw^1=\alpha w^1</script><p>即$w^1$为特征向量 eigenvector</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628115707351.png" alt="image-20200628115707351" style="zoom:50%;"></p>
<p>$w^1$为矩阵S的特征向量，对应的特征值$\lambda_1$是最大的</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628115903683.png" alt="image-20200628115903683" style="zoom:50%;"></p>
<p>再找到$w^2$，对$w^2$求偏微分，得</p>
<script type="math/tex; mode=display">
Sw^2-\alpha w^2-\beta w^1=0</script><p>两边同时乘上$(w^1)^T$，得</p>
<script type="math/tex; mode=display">
(w^1)^TSw^2-\alpha (w^1)^Tw^2-\beta(w^1)^T w^1=0</script><p>代入$(w^1)^Tw^1=1,(w^2)^Tw^1=0$，</p>
<script type="math/tex; mode=display">
\begin{aligned}
&(w^1)^TSw^2-\alpha (w^1)^Tw^2-\beta(w^1)^T w^1\\
&=(w^1)^TSw^2-\beta
\end{aligned}</script><p>代入$Sw^1=\lambda w^1$,</p>
<script type="math/tex; mode=display">
\begin{aligned}
&(w^1)^TSw^2\\
&=((w^1)^TSw^2)^T=(w^2)^TS^Tw^1\\
&=(w^2)^TSw^1=\lambda (w^2)^Tw^1=0
\end{aligned}</script><p>那么</p>
<script type="math/tex; mode=display">
0-\alpha\ 0-\beta\ 1=0\quad\rightarrow \beta=0</script><p>代入$Sw^2-\alpha w^2-\beta w^1=0$，可得</p>
<script type="math/tex; mode=display">
Sw^2-\alpha w^2=0\quad \rightarrow Sw^2=\alpha w^2</script><p>可得出$w^2$为矩阵S的特征向量，对应的特征值$\lambda_2$是第二大的</p>
<h5 id="Decorrelation"><a href="#Decorrelation" class="headerlink" title="Decorrelation"></a><strong>Decorrelation</strong></h5><script type="math/tex; mode=display">
W=\begin{pmatrix} 
(w_1)^T\\
(w_2)^T \\
\vdots
\\
(w_K)^T
\end{pmatrix}</script><p>$(w^1)^T$表示W的第一行，且$(w^1)^Tw^1=1,(w^2)^Tw^1=0$，因此$Ww^1=e_1,…,Ww^K=e_K$，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628123018232.png" alt="image-20200628123018232" style="zoom:50%;"></p>
<p>可得出$Cov(z)$是一个对角矩阵，只有正对角线上有元素</p>
<h5 id="Another-Point-of-View"><a href="#Another-Point-of-View" class="headerlink" title="Another Point of View"></a>Another Point of View</h5><p>下图中的7可以由三个部分组成，即$u^1,u^3,u^5$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628123840304.png" alt="image-20200628123840304" style="zoom:50%;"></p>
<p>那么我们目标就是找到这K个component，使得$||(x-\bar x)-\hat x||_2$达到最小值</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628124332233.png" alt="image-20200628124332233" style="zoom:50%;"></p>
<p>x可以分为$x^1,x^2,…$，对应的$c_1$也可以分为$c_1^1,c_1^2,…$，这样就形成了三个矩阵</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628125033081.png" alt="image-20200628125033081" style="zoom:50%;"></p>
<p>那么我们怎么来最小化矩阵之间的最小差值呢？</p>
<p>下图中的U对应PCA中的权重矩阵W，为前文求出来的K个特征向量，为K个component，$\sum,V$表示C矩阵</p>
<p>对于矩阵$XX^T$的K个最大的特征值，矩阵U的每一列表示就表示这些特征值所对应的K个特征向量</p>
<p>做SVD求解出来的U矩阵，就是协方差矩阵$Cov(z)$所对应的特征向量，也就是PCA得出来的解</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628125515261.png" alt="image-20200628125515261" style="zoom:50%;"></p>
<p>其中$c_k$可以用另外一种形式表达出来，$c_k=(x-\bar x)\cdot w^k$，$\cdot$表示做inner product</p>
<p>如果此时K=2，那么$c_1=\sum_{i=1}^3(x-\bar x)\cdot w^k_i$，可以用neural network的形式表达出来，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628155642780.png" alt="image-20200628155642780" style="zoom:50%;"></p>
<p>那么$c_1$乘上$w_i^1$，就可以得到output为$\hat x_i$，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628160122130.png" alt="image-20200628160122130" style="zoom:50%;"></p>
<p>对于$c_2$也有类似的结果</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628160212513.png" alt="image-20200628160212513" style="zoom:50%;"></p>
<p>对于network的output为$\hat x_i$，应与$x-\bar x$之间的error最小化</p>
<p>那么我们就可以把这个结构看成是具有一个hidden layer的network，其output和input应该越接近越好，这就可以叫做<strong>Autoencoder</strong></p>
<p>Q：既然是neural network，那么我们可以用gradient descent来得到和PCA一样的最优解吗？</p>
<p>A：用PCA求解出来的w是相互正交的，可以让reconstruction error最小化，但gradient descent求解出来的w并不能保证这一点，而且并不能使这个reconstruction error比PCA方法更小</p>
<p><span style="color: red">如果是在linear的情况下，使用PCA比较好，用network就会很麻烦；但network可以是deep的，可以中间有很多个hidden layer，这被称为<strong>Deep Autoencoder</strong></span></p>
<h5 id="Weakness-of-PCA"><a href="#Weakness-of-PCA" class="headerlink" title="Weakness of PCA"></a>Weakness of PCA</h5><ul>
<li>unsupervised，输入的data是没有label的，PCA会找一种方式使得降维之后data的variance最大，就可能出现左上的结果，这时两者的class是不一样的，如果还是继续投影到红线上，就会出现两个class的data相互交错的局面；</li>
<li>Linear，对于立体的data，如果还是继续pca，得到的结果也会非常不理想</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628161709298.png" alt="image-20200628161709298" style="zoom:50%;"></p>
<h5 id="Pokemon"><a href="#Pokemon" class="headerlink" title="Pokémon"></a>Pokémon</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628162244781.png" alt="image-20200628162244781" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628162630167.png" alt="image-20200628162630167" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628162636915.png" alt="image-20200628162636915" style="zoom:50%;"></p>
<h5 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628162743780.png" alt="image-20200628162743780" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Unsupervised Learning</tag>
        <tag>Dimension Reduction</tag>
        <tag>PCA</tag>
      </tags>
  </entry>
  <entry>
    <title>Word Embedding</title>
    <url>/2020/06/11/Word-Embedding/</url>
    <content><![CDATA[<h4 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h4><p>如果使用1-of-N encoding，每个单词用一个vector表示，那么5个单词就需要5个vector；如果我们把同一个类别的单词都放到那个类里，即属于class1的单词有dog、cat、bird，属于动物类的单词，同理可以得出class2，class3；</p>
<p>但只做classify是不够的，这些class之间也有一些其他的联系；比如class1属于动物，class3属于植物，他们都是生物，只做classify并不能体现这种联系；</p>
<p>现在我们把每个word都project到一个两个dimension上， 水平的dimension可以是表示生物（class1，class3）和其他类别class2之间的差距，竖直的dimension可以是会动（class2，class3）的和不会动class1之间的差距；</p>
<p>如果现在有10w个单词，1-of-N encoding就需要10w个vector，但word embedding可能只需要50维左右，就可以表示这些所有的word。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611094055907.png" alt="image-20200611094055907" style="zoom:50%;"></p>
<p>word2vec是一个无监督学习问题，如果network的input为“Apple”，要输出其对应的vector </p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611095916810.png" alt="image-20200611095916810" style="zoom:50%;"></p>
<p>下面我们将叙述生成词向量的两种主要手段。</p>
<h4 id="Count-based"><a href="#Count-based" class="headerlink" title="Count based"></a>Count based</h4><p>基于计数的方法，记录文本中词的出现次数。如果两个单词$w_i,w_j$常常一起出现，那么我们就认为其对应的vector $V(w_i),V(w_j)$之间就是非常接近的。</p>
<p>用$N_{i,j}$表示$w_i,w_j$在同一个document中出现的次数，那么我们希望找到对应的$V(w_i),V(w_j)$，其做inner product的值和这个次数越接近越好。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611102225747.png" alt="image-20200611102225747" style="zoom:50%;"></p>
<h4 id="Prediction-based"><a href="#Prediction-based" class="headerlink" title="Prediction-based"></a>Prediction-based</h4><p>基于预测的方法，即可以通过上下文预测中心词，也可以通过中心词预测上下文。中心词即我们要预测的词。在下文中，$w_i$是我们要预测的值，$…,w_{i-2},w_{i-1}$就是其对应的上下文；</p>
<p>现在我们把$w_{i-1}$的one-hot encoding作为网络的输入，网络的输出为每个词作为下一个词$w_i$输出的概率，如果词袋中有10w个词，那么输出的维度就对应为10w维。</p>
<p>把网络中第一个hidden layer的input拿出来，即$z=(z_1,z_2,…)^T$；如果输入为不同1-of-N encoding，那么对应的z也是不一样的。我们就可以用z来代表一个word，即z就是我们要寻找的word vector $V(w)$。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611102755128.png" alt="image-20200611102755128" style="zoom: 40%;"></p>
<p>如果现在有两个training text，“蔡英文 宣誓就职”、“马英九 宣誓就职”，如果输入的1-of-N encoding是“蔡英文”或“马英九”，那么我们希望在网络的输出概率中，“宣誓就职”的概率是最大的。同理我们也可以把网络的第一个hidden layer的输入作为z，就是我们要寻找的word vector $V(w)$。</p>
<p>这时我们就需要中间的hidden layer来做这样一件事，如果输入为不同的词汇（“蔡英文”，“马英九”），那么我们希望中间的hidden layer可以把不同的词汇project到相同的空间，这样网络的输出才可能都是“宣誓就职”对应的概率最大。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705122246038.png" alt="image-20200705122246038" style="zoom:50%;"></p>
<h5 id="Sharing-weight"><a href="#Sharing-weight" class="headerlink" title="Sharing weight"></a><strong>Sharing weight</strong></h5><p>只考虑前面的一个词汇，来预测下一个词汇会很难，我们可以考虑前面的几个词汇。现在我们来叙述考虑前两个词汇的结果。</p>
<p>现在我们并不能像之间的neural network那样，所有的输入都连成一个vector作为输入。但实际上，我们希望不同的one-hot vector的同一维度之间是tie在一起的。即$w_{i-1},w_{i-2}$的第一维对应$z_1$，其对应的weight是一样的；同理$w_{i-1},w_{i-2}$的第二维对应$z_1$，其对应的weight是一样的，……</p>
<p>Q：为什么不同的one-hot vector的同一维度之间是tie在一起的呢？</p>
<p>A：我们现在把同一个word放到$w_{i-1},w_{i-2}$的位置， 如果每一维对应的weight都不一样，那么实际上就输入了两个vector。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611103812991.png" alt="image-20200611103812991" style="zoom:50%;"></p>
<p>如果我们设置$x_{i-1},x_{i-2}$的长度都是$|V|$，$z$的长度是$|Z|$，那么</p>
<script type="math/tex; mode=display">
z=W_1x_{i-2}+W_2x_{i-1}</script><p>其中$W_1=W_2=W$，那么$z=W(x_{i-2}+x_{i-1})$，也就得到了word vector $V(w)$，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611104833054.png" alt="image-20200611104833054" style="zoom:50%;"></p>
<p>那么在实际的网络训练中，我们如何保证$W_1=W_2$呢？</p>
<p>首先需要将$w_1,w_2$初始化为相同的值，那么</p>
<script type="math/tex; mode=display">
w_i\leftarrow w_i-\eta \frac{\partial C}{\partial w_i}\\
w_j\leftarrow w_j-\eta \frac{\partial C}{\partial w_j}</script><p>由于gradient的值不同，$w_1,w_2$在更新一次参数之后就不相等了，必须保证每次更新之后的值还是一样的，因此</p>
<script type="math/tex; mode=display">
w_i\leftarrow w_i-\eta \frac{\partial C}{\partial w_i}-\eta \frac{\partial C}{\partial w_j}\\
w_j\leftarrow w_j-\eta \frac{\partial C}{\partial w_j}-\eta \frac{\partial C}{\partial w_i}</script><p>这样每次更新的值都一样的，也就保证了$w_1=w_2$</p>
<h5 id="Training"><a href="#Training" class="headerlink" title="Training"></a><strong>Training</strong></h5><p>对于输入“潮水、退了”，我们希望network的输出和“就”越接近越好，即最小化cross entropy</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611103701528.png" alt="image-20200611103701528" style="zoom:50%;"></p>
<h5 id="Various-Architectures"><a href="#Various-Architectures" class="headerlink" title="Various Architectures"></a>Various Architectures</h5><p>CBOW：根据上下文的词汇$w_{i-1},w_{i+1}$来预测中心词$w_i$;</p>
<p>Skip-gram：根据中心词$w_i$来预测上下文$w_{i-1},w_{i+1}$。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611110004840.png" alt="image-20200611110004840" style="zoom:50%;"></p>
<h5 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h5><p>vec(Rome) - vec(Italy) $\approx$ vec(Berlin) - vec(Germany)，Italy和Rome之间有is-capital-of的关系，这种关系也恰好在Madrid和Spain之间出现。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200611110430525.png" alt="image-20200611110430525" style="zoom:50%;"></p>
<p>如果现在有人问机器一个问题，Rome和Italy之间的关系就像是Berlin和什么的关系？我们就可以通过计算vec(Berlin) $\approx$ vec(Rome) - vec(Italy)  + vec(Germany)得出结果。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200705125922175.png" alt="image-20200705125922175" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>word embedding</tag>
      </tags>
  </entry>
  <entry>
    <title>Generative Models</title>
    <url>/2020/06/29/generative-models/</url>
    <content><![CDATA[<blockquote>
<p>本文主要叙述了集中generative models，包括PixelRNN、VAE，GAN（生成对抗网络）；还叙述了auto-encoder和VAE的区别，以及VAE进行改进的地方，通过高斯混合模型来对VAE的原理进行了详细介绍。</p>
</blockquote>
<h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><p>Generative Models可以分为三大类：pixelRNN、autoencoder、GAN；</p>
<p>现在machine可以对猫狗进行分类，在将来machine就可以通过自己的学习画出一只猫来。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629141407487.png" alt="image-20200629141407487" style="zoom: 67%;"></p>
<h4 id="PixelRNN"><a href="#PixelRNN" class="headerlink" title="PixelRNN"></a>PixelRNN</h4><h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>如果要生成一张图像，我们可以先生成这张图像的第一个红色pixel，再根据这个pixel生成下一个蓝色pixel，每个pixel可以用一个三维的vector来进行表示</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629141521860.png" alt="image-20200629141521860" style="zoom: 67%;"></p>
<p>这是一个无监督学习的过程，并不需要对data进行标注</p>
<p>如果我们现在把下图中的狗下半身遮住，可以让machine学习出下半身的图片</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629141945697.png" alt="image-20200629141945697" style="zoom: 67%;"></p>
<p>也可以用到声讯号</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629142331286.png" alt="image-20200629142331286" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629142353897.png" alt="image-20200629142353897" style="zoom:50%;"></p>
<h5 id="Practicing-Generation-Models-Pokemon-Creation"><a href="#Practicing-Generation-Models-Pokemon-Creation" class="headerlink" title="Practicing Generation Models: Pokémon Creation"></a>Practicing Generation Models: Pokémon Creation</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629142516414.png" alt="image-20200629142516414" style="zoom: 67%;"></p>
<p>做这个实验时，有一些tips；</p>
<p>由于每个pixel都使用三维的vector来表示（RGB），只有三个channel的值相差特别大时，才会有颜色特别鲜明的图片，但学习结果并不能保证这一点，因此图片会会比较灰蒙蒙的，最后得出来的结果会不太好；</p>
<p>因此，每个pixel最好使用1-of-N encoding来表示，输入一个绿色的方块，vector中只有绿色方块对应的dimensions为1，其他都是0；但有$256\times256$种颜色，颜色种类非常繁多，可以先对类似的颜色做一个<strong>clustering</strong>，类似的颜色都用同一种颜色来表示，可以把颜色数量缩小到167种</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629142535096.png" alt="image-20200629142535096" style="zoom: 67%;"></p>
<p>下面开始正式做实验</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629143804656.png" alt="image-20200629143804656" style="zoom: 67%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629143843226.png" alt="image-20200629143843226" style="zoom: 33%;"></p>
<h4 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h4><h5 id="Auto-encoder"><a href="#Auto-encoder" class="headerlink" title="Auto-encoder"></a>Auto-encoder</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629144022672.png" alt="image-20200629144022672" style="zoom: 67%;"></p>
<h5 id="VAE-1"><a href="#VAE-1" class="headerlink" title="VAE"></a>VAE</h5><p>VAE不像autoencoder那样，直接得出code，经过了一些变换，即$c_i=exp(\sigma_i)\times e_i+m_i$，也是来最小化reconstruction error</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629144250780.png" alt="image-20200629144250780" style="zoom: 67%;"></p>
<p>minimize的目标还有</p>
<script type="math/tex; mode=display">
\sum_{i=1}^3(exp(\sigma_i)-(1+\sigma_i)+(m_i)^2)</script><h5 id="Pokemon-Creation"><a href="#Pokemon-Creation" class="headerlink" title="Pokémon Creation"></a>Pokémon Creation</h5><p>input一个宝可梦图像，进行encoder、decoder，得到reconstruct之后的图像，其中code是10-dim的；</p>
<p>现在我们只选择其中的2个纬度出来，其他纬度的值都固定不变，对这个二维的坐标轴，放入不同的点，再输入对应的Decoder，观察其合成出来的image；如果使用不同维度的点，就可以观察到不同维度生成的image；根据不同的维度，我们就可以根据自己的需要，调整这些维度的值，从而得出我们想要的结果</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629150656140.png" alt="image-20200629150656140" style="zoom: 50%;"></p>
<p>部分结果展示</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629151406932.png" alt="image-20200629151406932" style="zoom:50%;"></p>
<h5 id="Why-VAE"><a href="#Why-VAE" class="headerlink" title="Why VAE?"></a>Why VAE?</h5><p>对于一张满月的图像，先进行encode，再进行decode，使得input和output之间的差值最小化；对半月的图像输入也如此</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629152107223.png" alt="image-20200629152107223" style="zoom: 67%;"></p>
<p>对于encoder的其中一个输出$\sigma_i$，表示noise的variance，需要再输入exp函数进行计算，保证其值是大于0的，是machine自己学习的</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629153400892.png" alt="image-20200629153400892" style="zoom: 67%;"></p>
<p>但只有noise是不够的，还需要加一些限制</p>
<script type="math/tex; mode=display">
\sum_{i=1}^3(exp(\sigma_i)-(1+\sigma_i)+(m_i)^2)</script><p>在下图中，蓝色的线表示$exp(\sigma_i)$，红色的线表示$(1+\sigma_i)$，绿色的线表示两者的差值，可以发现在原点处差值是最小的，$\sigma _i$的值接近1，variance的值接近1</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629153918937.png" alt="image-20200629153918937" style="zoom:50%;"></p>
<p>$(m_i)^2$为正则项regularization term，让结果不那么overfiting</p>
<p>在下图中，为一个高维的坐标（用一维进行了展示），曲线表示image是宝可梦的概率，可以在图中看到，对于一些很像宝可梦的image，$P(x)$值很高，但对于一些比较模棱两可的image，$P(x)$值就很低</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629154606838.png" alt="image-20200629154606838" style="zoom: 33%;"></p>
<p>那么我们到底要怎么来评估这个probability distribution呢？答案是高斯混合模型</p>
<h5 id="Gaussian-Mixture-Model"><a href="#Gaussian-Mixture-Model" class="headerlink" title="Gaussian Mixture Model"></a>Gaussian Mixture Model</h5><p>对于下图中的曲线图，蓝色表示多个高斯模型，黑色曲线表示这些模型通过一定的weight进行叠加之后的结果</p>
<p>现在有很多个gaussian model（1，2，3，4，5…），且都对应了自己的weight $P(m)$（蓝色方块），<span style="color: red">要先根据weight选对应的gaussian，再决定到底要从gaussian中sample哪些data</span> ;我们使用$P(m)$表示每个gaussian的weight，$P(x|m)$表示从对应的gaussian中选出data x的概率</p>
<p>x并不是代表着某个类别，而是用一个vector来进行表示，每个维度表示不同的特征，即 <u>Distributed representation is better than cluster</u></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629155005437.png" alt="image-20200629155005437" style="zoom: 67%;"></p>
<p>我们现在从正态分布中sample一个data z，z的每个dimension就表示某种attribute；</p>
<p>根据z我们就可以得出高斯分布mean和variance，即$\mu (z),\sigma(z)$，z有无穷多个可能，mean和variance也有无穷多个可能</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629160314318.png" alt="image-20200629160314318" style="zoom: 67%;"></p>
<p>在最下方的图中，我们可以认为最中间的圆点被sample到的几率最大，其他圆点被sample到的几率就相对较小；<span style="color: red">每个z被sample到之后，根据某个function，计算出对应的mean和variance，都对应着不同的gaussian model</span></p>
<p>这个function可以是一个neural network，input为z，output为$\mu(z),\sigma(z)$</p>
<p>由于现在是连续的z，$P(z)$的形式也发生了变化，</p>
<script type="math/tex; mode=display">
P(x)=\mathop{\int }_zP(z)P(x|z){\rm d}z</script><h5 id="Maximizing-Likelihood"><a href="#Maximizing-Likelihood" class="headerlink" title="Maximizing Likelihood"></a><strong>Maximizing Likelihood</strong></h5><p>现在我们需要找到z和$\mu(z),\sigma(z)$之间的关系，用network来进行计算的时候也需要一个评估手段，这里我们采用的是最大化$L=\sum_xlogP(x)$，即最大化我们已经看到的image x的likelihood；</p>
<p>那么我们现在就有一个NN的参数需要调整，来最大化likelihood L；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629162138964.png" alt="image-20200629162138964" style="zoom: 67%;"></p>
<p>现在我们还有另外一个distribution $q(z|x)$，和前一个是相反的，其中$z|x$表示给出x，再把x输入网络$NN’$，得到属于z的gaussian distribution，其mean和variance，即$\mu’(x),\sigma’(x)$</p>
<p>$P(x|z)$表示z决定了x的distribution，$q(z|x)$表示x决定了z的distribution</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629163947560.png" alt="image-20200629163947560" style="zoom:45%;"></p>
<p>其中$P(z,x)=P(x)P(z|x)$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629165559403.png" alt="image-20200629165559403" style="zoom:50%;"></p>
<p>我们本来需要调整的参数是$P(x|z)$，使得$P(x)$取得最大值，从而使likelihood取得最大值；但现在的lower bound为$L_b$，是$log(P(x))$的最小值，其中$P(z)$是已知的，不知道的参数是$P(x|z),q(z|x)$，两项都需要调整</p>
<p>Q：这里为什么突然多了一项需要调整的参数？</p>
<p>A：这里并不知道lower bound和likelihood之间的关系到底是怎样的，有可能升高了lower bound，但likelihood反而下降了；引入q就可以解决这个问题</p>
<p>根据原式子，即$P(x)=\mathop{\int }_zP(z)P(x|z){\rm d}z$，只和$P(x|z)$有关，与$q(z|x)$是无关的，即下图中<span style="color: purple">方框</span>部分是不变的，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629170614939.png" alt="image-20200629170614939" style="zoom: 67%;"></p>
<p><strong>如果我们现在固定$P(x|z)$，想通过$q(z|x)$来使$L_b$ 最大化，那么对应的KL divergence就会最小化，到一种很极端的情况，KL divergence会变为0；如果$L_b$继续上升，那么肯定会超出该区域，因此对应的$logP(x)$也会继续上升；KL divergence不断变小的过程，$q(z|x),P(z|x)$之间的差距也会不断缩小</strong></p>
<p>因此，这个过程不仅让likelihood变得越来越大，也会找到和$P(x|z)$接近的$q(z|x)$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629171631388.png" alt="image-20200629171631388" style="zoom:50%;"></p>
<p>对于$q(z|x)$，其中$z|x$表示给出x，再把x输入网络$NN’$，得到属于z的gaussian distribution，就可以知道z是从什么样的gaussian distribution得出来的</p>
<h5 id="Connection-with-Network"><a href="#Connection-with-Network" class="headerlink" title="Connection with Network"></a><strong>Connection with Network</strong></h5><p>现在我们的目标就是最小化$q(z|x),P(z)$之间的KL divergence，即使$q(z|x)$和normal distribution $P(z)$越接近越好，即minimize</p>
<script type="math/tex; mode=display">
\sum_{i=1}^3(exp(\sigma_i)-(1+\sigma_i)+(m_i)^2)</script><p>还需要maximizing</p>
<script type="math/tex; mode=display">
P(x)=\mathop{\int }_zP(z)P(x|z){\rm d}z</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629172034668.png" alt="image-20200629172034668" style="zoom:50%;"></p>
<p>对于input的x，我们先输入网络$NN’$，得到获取z的gaussian distribution；sample得到z之后，再输入网络$NN$，得到获取x的gaussian distribution，使得新分布的mean和x越接近越好</p>
<h5 id="KL散度"><a href="#KL散度" class="headerlink" title="KL散度"></a>KL散度</h5><blockquote>
<p>所谓KL散度，是指当某分布q(x)被用于近似p(x)时的信息损失。</p>
</blockquote>
<p>计算公式为</p>
<script type="math/tex; mode=display">
KL(p||q)=\sum p(x)log\frac{p(x)}{q(x)}</script><h5 id="Problems-of-VAE"><a href="#Problems-of-VAE" class="headerlink" title="Problems of VAE"></a>Problems of VAE</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629174600157.png" alt="image-20200629174600157" style="zoom:50%;"></p>
<h4 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h4><h5 id="The-evolution-of-generation"><a href="#The-evolution-of-generation" class="headerlink" title="The evolution of generation"></a>The evolution of generation</h5><p>v1：会generate一些奇怪的图片，然后会有一个第一代的discriminator，来辨别到底哪一个是real image；</p>
<p>V2： generater会根据上一次discriminater的结果进行调整，第二代生成的image就和真实的image更像了，再把image输入第二代discriminator，再进行比较；</p>
<p>…….</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629175524852.png" alt="image-20200629175524852" style="zoom:50%;"></p>
<h5 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h5><p>对于generator，可以看作是VAE里面的decoder，我们通常选择从某个distributionsample出来的vector，输入到第一代的generator，再生成相对应的image，有多少个vector，就生成多少个image；</p>
<p>把真实的image输入discriminator，与generator生成的image进行比较；并对这些data进行标注，real image标注为1，其余标注为0</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629201306815.png" alt="image-20200629201306815" style="zoom:50%;"></p>
<h5 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h5><p>首先随机sample一个vector，作为Generator的输入，generator会生成对应的image，再把这个image输入discriminator，输出是real image的概率，对于第一代（v1），generator还不成熟，因此概率只有0.87；generator接下来会调整自己的参数，使discriminator认为其生成的image是real image；</p>
<p>这generator+discriminator就相当于一个network，整体目标是使discriminator认为generator生成的image是real image，就可以根据这个指标来使用梯度下降算法，进行back propagation，来不断调整generator的参数；</p>
<p><span style="color: red">注：在训练过程中应该固定discriminator的参数，来调整generator的参数</span></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629201707553.png" alt="image-20200629201707553" style="zoom:50%;"></p>
<h5 id="Why-GAN-is-hard-to-train"><a href="#Why-GAN-is-hard-to-train" class="headerlink" title="Why GAN is hard to train?"></a>Why GAN is hard to train?</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200629202903543.png" alt="image-20200629202903543" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>GAN</tag>
        <tag>Unsupervised Learning</tag>
        <tag>Autoencoder</tag>
        <tag>VAE</tag>
        <tag>PixelRNN</tag>
      </tags>
  </entry>
  <entry>
    <title>fGAN</title>
    <url>/2020/07/10/fGAN/</url>
    <content><![CDATA[<h4 id="f-divergence"><a href="#f-divergence" class="headerlink" title="f-divergence"></a>f-divergence</h4><p>$p(x),q(x)$分别表示x从分布P，Q中sample出来的概率。f可以是不同的函数，必须是凸函数，且$f(1)=0$。f-divergence的式子如下，可以表示P和Q之间的差异，</p>
<script type="math/tex; mode=display">
D_f(P||Q)=\int_x q(x)f\left(\frac{p(x)}{q(x)}\right)dx</script><p>那么为什么这个式子可以表示P和Q之间的差异呢？</p>
<p>如果现在$q(x)=p(x)$，那么$D_f(P||Q)=0$，表示q和p之间的距离为0.</p>
<p>如果q和p有一些很小的差距，算出来的divergence就大于0.</p>
<script type="math/tex; mode=display">
\int_x q(x)f\left(\frac{p(x)}{q(x)}\right)dx=0\geq  f\left(\int_x q(x)\frac{p(x)}{q(x)}dx\right)=f(1)=0</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710155622091.png" alt="image-20200710155622091" style="zoom:60%;"></p>
<p>如果$f(x)=xlogx$，那么$D_f(P||Q)$就是KL divergence；</p>
<p>如果$f(x)=-logx$，那么$D_f(P||Q)$就是Reverse divergence；</p>
<p>如果$f(x)=(x-1)^2$，那么$D_f(P||Q)$就是Chi Square；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710162446585.png" alt="image-20200710162446585" style="zoom:60%;"></p>
<h4 id="Fenchel-Conjugate"><a href="#Fenchel-Conjugate" class="headerlink" title="Fenchel Conjugate"></a>Fenchel Conjugate</h4><p>每个凸函数都有一个Conjugate function $f^*$，是由x和$f(x)$导出来的。对于值的计算，我们可以通过穷举所有t的值代入，看到底哪个t可以使$xt-f(x)$的值最大。即</p>
<script type="math/tex; mode=display">
f^*(t)=\mathop{\rm max}_{x\in dom(f)}\{xt-f(x\}</script><p>对于值的计算来举一个例子，当$t=t_1$时，</p>
<script type="math/tex; mode=display">
f^*(t)=\mathop{\rm max}_{x\in dom(f)}\{xt_1-f(x\}</script><p>这时$x$的取值范围为$x=\{x_1,x_2,x_3\}$，代入x的值，计算$x_1t_1-f(x_1),x_2t_1-f(x_2),x_3t_1-f(x_3)$的值，$f^*(t)$即为三者最大；</p>
<p>代入$t_2$的值来计算$f^(t_2)$的值；</p>
<p>……</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710162725028.png" alt="image-20200710162725028" style="zoom:60%;"></p>
<p>这样每个t值都带进去算很麻烦，因此出现了第二种方案，把$xt-f(x)$用图形描述出来。找出这些直线的upper bound，可以发现$f^*(t)$是凸函数。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710164729970.png" alt="image-20200710164729970" style="zoom:60%;"></p>
<p>现在假设$f(x)=xlogx$，代入$xt-f(x)$计算，画出$x=\{0.1,1,10,…\}$时的函数图像，并找出这些直线的upper bound，如下图所示，如果进行了很多次运算，这些直线的upper bound和$f^*(t)=exp(t-1)$的图像很接近。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710165445558.png" alt="image-20200710165445558" style="zoom:60%;"></p>
<p>下图是这个过程具体的证明，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710165854738.png" alt="image-20200710165854738" style="zoom:60%;"></p>
<h4 id="Connection-with-GAN"><a href="#Connection-with-GAN" class="headerlink" title="Connection with GAN"></a><strong>Connection with GAN</strong></h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710170013115.png" alt="image-20200710170013115" style="zoom:60%;"></p>
<p>$f(x)$与$f^*(x)$互为Conjugate function。把$x=\frac{p(x)}{q(x)}$代入，得</p>
<script type="math/tex; mode=display">
D_f(P||Q)=\int_x q(x)f\left\{\mathop{\rm max}_{x\in dom(f)}\{\frac{p(x)}{q(x)}t-f^*(t)\}\right\}dx</script><p>我们现在可以用一个discriminator D，来帮助我们求解这个max的问题，输入为x，输出就是满足条件的t，就不用穷举所有的t才能找到我们的最优解。如果用$D(x)$来替代x，就可以表示$D_f(P||Q)$的lower bound，即</p>
<script type="math/tex; mode=display">
\begin{aligned}
D_f(P||Q)&\geq\int_x q(x)f\left(\frac{p(x)}{q(x)}D(x)-f^*(D(x))\right)dx\\
&=\int_xp(x)D(x)dx-\int_xq(x)f^*(D(x))dx
\end{aligned}</script><p>如果随便找一个D，最后得出来的值肯定比divergence的值小；但如果是找一个最优（max）的D，预测出来的t就是最准的，就可以使结果逼近divergence，即</p>
<script type="math/tex; mode=display">
D_f(P||Q)\approx \mathop{\rm max}_D\int_xp(x)D(x)dx-\int_xq(x)f^*(D(x))dx</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710173339038.png" alt="image-20200710173339038" style="zoom:60%;"></p>
<p>$V(G,D)$可以有不同的形式，不同的divergence就有不同的$V(G,D)$。</p>
<p>下图中列出了不同的divergence和generator。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710173859040.png" alt="image-20200710173859040"></p>
<p>可以用不同的divergence又有什么优点呢？</p>
<h4 id="Mode-Collapse"><a href="#Mode-Collapse" class="headerlink" title="Mode Collapse"></a>Mode Collapse</h4><p>当我们在训练GAN的时候，可能会遇到mode collapse，real data的distribution是非常宽泛的，但generated data的distribution可能会非常小。比如我们在生成二次元人物的时候，可能会出现下图中的结果，某张特定的人脸开始蔓延，变得到处都是，同一张人脸会不断反复地出现。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710174639258.png" alt="image-20200710174639258" style="zoom: 50%;"></p>
<h4 id="Mode-Dropping"><a href="#Mode-Dropping" class="headerlink" title="Mode Dropping"></a>Mode Dropping</h4><p>mode dropping的情况比mode collapse要稍微简单一点，现在real data有两种不同的distribution，而generator只会产生一种distribution的数据。</p>
<p>Generator第一次会先产生一些白皮肤的人，再进行一次generator，会产生一些黄皮肤的人，再进行一次generator，会产生一些黑皮肤的人。每次只产生一种分布的数据。</p>
<p><img src="/Users/liufang/Library/Application Support/typora-user-images/image-20200710175725435.png" alt="image-20200710175725435" style="zoom: 50%;"></p>
<p>会出现这个问题，一个很可能的原因就是divergence选得不好。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200710175856174.png" alt="image-20200710175856174" style="zoom: 50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>GAN</tag>
        <tag>fGAN</tag>
        <tag>f-divergence</tag>
        <tag>Fenchel Conjugate</tag>
      </tags>
  </entry>
  <entry>
    <title>基于知识图谱和GCN的应用和开发</title>
    <url>/2020/07/03/knowledge-graph/</url>
    <content><![CDATA[<h4 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h4><p>深度学习的主要思想有三个：</p>
<ul>
<li><p>权重分配；</p>
</li>
<li><p>层级结构：每一层训练的结果都依赖于上一层；</p>
</li>
<li><p>欧几里得空间数据：比如图像数据，可以通过二维坐标(x,y)进行表示，语音信号是一个一维的数据，围棋的话也是一个二维数据，目前的大多数数据都是可以用欧几里得空间数据进行表示的，即可以投影到坐标轴上</p>
</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702101528758-20200706215024224.png" alt="image-20200702101528758"></p>
<p>但在实际生活中，有很多数据并不能用欧几里得数据来进行表示，比如在微信上，每个人都相当于一个节点，朋友之间的关系相当于是一条边，相当于图，没有任何的空间信息，是<u>非欧几里得的空间结构</u>（eg. 社交网络、科学网络等） </p>
<p>那要怎么对这类信息进行训练呢？传统的深度学习方法（CNN和RNN等）并没有办法训练这类数据，因此就产生了图卷积（GCN）</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702103428339.png" alt="image-20200702103428339"></p>
<h4 id="图卷积背景和基本框架"><a href="#图卷积背景和基本框架" class="headerlink" title="图卷积背景和基本框架"></a>图卷积背景和基本框架</h4><h5 id="图卷积发展历史"><a href="#图卷积发展历史" class="headerlink" title="图卷积发展历史"></a>图卷积发展历史</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702104325966.png" alt="image-20200702104325966"></p>
<h5 id="邻接矩阵"><a href="#邻接矩阵" class="headerlink" title="邻接矩阵"></a>邻接矩阵</h5><p>无向图的邻接矩阵是对角矩阵</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702104743073.png" alt="image-20200702104743073"></p>
<h5 id="图卷积基本框架"><a href="#图卷积基本框架" class="headerlink" title="图卷积基本框架"></a>图卷积基本框架</h5><p>在下图中，A为邻接矩阵，H为特征矩阵（$N\times F$），，N为节点个数，F表示每个节点的特征数；比如我们每个人在社交网络中都是一个节点，也有一些对应的特征，名字、年龄、出生日期，这些特征就构成了这个特征矩阵$N\times F$；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702104938456.png" alt="image-20200702104938456"></p>
<p>在这个基本框架中，input为邻接矩阵A，第一个hidden layer把特征矩阵点乘到每一个节点上进行训练，再进入下一层，再进行点乘，…，一直不断重复这个过程。每次训练更新的参数是特征矩阵，最后的output和input是一样的，图结构没有发生变化，但特征矩阵达到了收敛（基本上不会发生变化），最后再对图中的每个节点进行分类；</p>
<p><span style="color: red">把节点和特征矩阵进行点乘，就实现了信息在节点和社区间进行传递</span></p>
<h4 id="图卷积"><a href="#图卷积" class="headerlink" title="图卷积"></a>图卷积</h4><p>那么下面我们将具体介绍节点和特征矩阵是怎样进行点乘的，怎样一步步进行训练。</p>
<h5 id="回顾CNN"><a href="#回顾CNN" class="headerlink" title="回顾CNN"></a>回顾CNN</h5><p>我们先回顾一下CNN是如何做卷积操作的，如下图所示，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702110939157.png" alt="image-20200702110939157"></p>
<h5 id="图卷积-1"><a href="#图卷积-1" class="headerlink" title="图卷积"></a>图卷积</h5><p>这是卷积操作的参数更新公式，</p>
<script type="math/tex; mode=display">
h_i^{l+1}=\sigma(W_0^{(l)}h_0^{(l)}+W_1^{(l)}h_1^{(l)}+\cdot\cdot\cdot+W_8^{(l)}h_8^{(l)})</script><p>图卷积相比较于卷积，在$h_0^{(l)}W_0^{(l)}$的基础上，多了邻接矩阵的信息$\frac{1}{c_{ij}}h_j^{(l)}$，<span style="color: red">权重矩阵W也没有卷积操作那么重要</span>，其参数更新公式为，</p>
<script type="math/tex; mode=display">
h_i^{l+1}=\sigma(h_0^{(l)}W_0^{(l)} + 
\sum_{j\in N_i} \frac{1}{c_{ij}}h_j^{(l)}W_1^{(l)})</script><p>更新参数的时候，通过周围的节点来更新该节点（下图中红色节点），与该节点不直接相连的节点无关</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702111915112.png" alt="image-20200702111915112"></p>
<p>那么我们就可以通过以下公式来更新特征矩阵，</p>
<script type="math/tex; mode=display">
H^{(l+1)}=\sigma(\tilde AH^{(l)}W^{(l)})</script><p>其中A为邻接矩阵，H为特征矩阵，W为权重矩阵，本次的特征矩阵$H^{(l+1)}$根据上一次的特征矩阵$H^{(l)}$生成；</p>
<p>如果只根据邻接矩阵来更新H，会出现一些问题：</p>
<ul>
<li>无法提取自身信息，邻接矩阵对角线上的元素始终是0（如果没有自环），自身的信息在训练过程中是无法考虑进去的；</li>
<li>$\tilde A$没有归一化</li>
</ul>
<p>我们需要把参数更新公式进行一些变化，首先解决邻接矩阵对角线元素为0的问题，</p>
<script type="math/tex; mode=display">
H^{(l+1)}=\sigma((\tilde D-\tilde A)H^{(l)}W^{(l)})</script><p><span style="color: red">其中</span>矩阵$\tilde D$是一个对角矩阵（degree matrix），对角线上的数值表示了每个节点所相连的边数。</p>
<p>归一化处理主要列出两种方式，要根据不同的输入数据来进行选择，其中第一种归一化方式比较常用</p>
<script type="math/tex; mode=display">
H^{(l+1)}=\sigma(\tilde D^{-\frac{1}{2}}(\tilde D-\tilde A)\tilde D^{-\frac{1}{2}}H^{(l)}W^{(l)}) \quad\quad (1)\\
H^{(l+1)}=\sigma(\tilde D^{-1}(\tilde D-\tilde A)\tilde H^{(l)}W^{(l)})</script><h5 id="图卷积的计算"><a href="#图卷积的计算" class="headerlink" title="图卷积的计算"></a>图卷积的计算</h5><p>下面我们将一个具体的例子来讲述公式(1),</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702114057283.png" alt="image-20200702114057283"></p>
<p>在上图中，degree matrix表示了每个节点所相连的边数，比如2表示1号节点相连的边有2条，该对角矩阵也就是前文提到的$\tilde D$，$(\tilde D-\tilde A)$就是laplacian matrix；</p>
<p>我们将特征矩阵$H_0$和权重矩阵W进行随机初始化，再代入公式(1)，即可得出训练完成的结果$H_N$，再求出每一列的最大值，即MaxValue$(H(i))=[1,1,4,1,1,4]$，即分类结果，其中第1，2，4，5个节点分类为1，第3，6个节点分类为4，把整个图的节点分成了两类；</p>
<p>那么为什么可以这样分类呢？我们可以回到原图labeled graph中，节点1，2，5相当于是一个全连接图，可以当作一个类别，而节点4，5都和三个节点相连，有三条边，也可以当作是一个类别；节点3，6就没有前几个节点那样关系比较紧密，因此被分成了第二个类别</p>
<p>我们可以得出以下两点：</p>
<ul>
<li>对于随机产生的特征矩阵，依靠图自身的结构，就可以得到一个很好的分类效果；</li>
<li>在这个例子中，进行10次运算，特征矩阵就收敛了，收敛速度非常快</li>
</ul>
<p>再来回顾下这个核心公式，所有的图卷积的变化和应用都离不开这个公式。W不那么重要，特征矩阵$H^{(l)}$表示每个人根据自己的实际特征来进行特征提取；邻接矩阵$\tilde A$表示每个人（节点）之间的关系，关系不一样就会构建出不同的图</p>
<script type="math/tex; mode=display">
H^{(l+1)}=\sigma(\tilde D^{-\frac{1}{2}}(\tilde D-\tilde A)\tilde D^{-\frac{1}{2}}H^{(l)}W^{(l)}) \quad\quad (1)</script><p><span style="color: red">那么我们要在图卷积进行创新的话，就只有改变邻接矩阵A和特征矩阵H，即如何新建这个图、如何提取这个特征；由于机器学习和深度学习技术的不断发展，特征提取的方法已经非常成熟，<u>关键的创新点还是如何新建这个图</u></span></p>
<h5 id="边信息嵌入"><a href="#边信息嵌入" class="headerlink" title="边信息嵌入"></a>边信息嵌入</h5><p>图中的边信息有时候是包含权重的，我们也应该把边的信息值也考虑进去，即把原图转化为一个线图；在下图中，原图中节点为i，j，m，边为a，b，c，转化为线图后，节点变成a，b，c，边为i，j，m，把边变成了点，把点变成了边</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702122014466.png" alt="image-20200702122014466"></p>
<p>经过两次不同的输入$h_{(i,j)}^l,h_j^{l+1}$，分别对应原图和线图，这样训练就可以把边的信息也整合进去</p>
<h5 id="Example-1-半监督图分类"><a href="#Example-1-半监督图分类" class="headerlink" title="Example 1: 半监督图分类"></a>Example 1: 半监督图分类</h5><p>在下图中，黑色圈点表示已经知道了分类的类别，其他的没有被黑色线条包围的点表示没有进行分类，目标是预测未分类的节点的类别，这里我们还定义了一个损失函数L，Z表示GCN的输出结果（上文的公式(1)）</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702123720321.png" alt="image-20200702123720321"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702124159182.png" alt="image-20200702124159182"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702124253302.png" alt="image-20200702124253302"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702124330022.png" alt="image-20200702124330022"></p>
<p>得到实验结果，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702124418966.png" alt="image-20200702124418966"></p>
<h5 id="Example-2-科学家网络"><a href="#Example-2-科学家网络" class="headerlink" title="Example 2: 科学家网络"></a>Example 2: 科学家网络</h5><p>每篇论文都会有一个题目，互相引用这篇论文则会形成一个关系，我们的任务是根据论文的题目来预测论文的类别（统计、物理、数学）</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702124600168.png" alt="image-20200702124600168"></p>
<p>这个例子只用了2层的GCN，其中$\hat AXW^{(0)}$表示第一层，$\hat A \ {\rm ReLU}(\hat AXW^{(0)})W^{(1)}$表示第二层，再输入softmax函数，整个训练过程更新的参数是X，从表格中的数据也可以看出，GCN取得了非常不错的效果</p>
<h4 id="基于知识图谱的图卷积"><a href="#基于知识图谱的图卷积" class="headerlink" title="基于知识图谱的图卷积"></a>基于知识图谱的图卷积</h4><h5 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h5><blockquote>
<p>知识图谱是Google用于增强其搜索引擎功能的知识库。本质上, 知识图谱旨在描述真实世界中存在的各种实体或概念及其关系,其构成一张巨大的语义网络图，节点表示实体或概念，边则由属性或关系构成。现在的知识图谱已被用来泛指各种大规模的知识库。 </p>
</blockquote>
<p>如果你输入搜索引擎中国这个词，相应的会出来中国的一些特征，比如中国的城市、国土面积、人口等；中国和国家都可以看作是图中的一个节点，中国和国家的关系就可以看作是一条边</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702145803375.png" alt="image-20200702145803375"></p>
<h5 id="图卷积：知识图谱"><a href="#图卷积：知识图谱" class="headerlink" title="图卷积：知识图谱"></a>图卷积：知识图谱</h5><p>先把数据用图表示出来，用户为U，实体为V，$y_{uv}=1$则表示用户u和实体v之间有联系，比如我对电影很感兴趣；每个实体u之间会有关系r，比如电影和教父；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702150629999.png" alt="image-20200702150629999"></p>
<p>将图卷积应用到知识图谱，可以用以下公式来进行更新，不断更新特征矩阵H直至其收敛即可，</p>
<script type="math/tex; mode=display">
H^{(l+1)}=\sigma(\tilde D_u^{-\frac{1}{2}}\tilde A_u\tilde D_u^{-\frac{1}{2}}H^{(l)}W^{(l)}) \quad\quad</script><p>那么我们怎么把实体推荐给用户呢？我们又怎么知道用户是不是对这个实体感兴趣呢？</p>
<p>（1）我们可以针对每个用户再新建一个图，先引入一个函数$S_u(\cdot)$，表示这条边（关系）对用户的重要性；</p>
<p>（2）再把原来的实体矩阵变成一个有权重的矩阵，比如边$r_1$输入函数$S_u(\cdot)$，得到$S_u(r_1)$，就是新的关系值。<u>每个用户都有一个邻接矩阵，再对每个用户的邻接矩阵进行训练，就可以得出用户到底对哪个电影感兴趣</u>；</p>
<p>（3）图卷积；</p>
<p>（4）<strong>标签顺滑</strong>，比如对于一个动物，机器可以很自信地确定这个动物到底是猫还是狗，输出值只有0或者1；由于机器过于自信，可能会导致很多额外的误差，我们现在就可以把这个值改成0.1或者0.9，就可以得到更好的结果。</p>
<p>总结：先新建一个图（knowledge graph，KG），再把用户对感兴趣的实体新建一个权重矩阵，再进行图卷积和标签顺滑。那么现在我们的损失函数就有了一些小变化，结合了图卷积和标签顺滑，即</p>
<script type="math/tex; mode=display">
L=J(\hat y_{uv})+\lambda R(A_u)</script><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702153035542.png" alt="image-20200702153035542"></p>
<p>下面我们将简要叙述这个训练的过程，图中的水平虚线表示decision boundary：</p>
<p>（a）其中蓝色的点表示用户感兴趣的点（observed）；在其下方有一些其他的点（unobserved），我们希望把黄色的点（被黄色方框围住）就推荐给用户，让用户去选择，而剩下蓝色的点，则是用户不感兴趣的，不推荐给用户；</p>
<p>（b）一共要进行两层的训练，对于第一层的训练结果，我们现在把其中两个黄色的点就推荐给用户了；</p>
<p>（c）对于第二层的训练结果，就把其中的三个点推荐给用户了，但最左端的点也在decision boundary之上，也会被推荐给用户，模型在训练过程中会存在一定的误差，</p>
<p>（d）这里就开始对第二个用户的模型进行训练；</p>
<p>（e）这是加入了标签顺滑（label smoothing）的结果，推荐会更加精准，在图中，我们可以看到用户不感兴趣的点进行下移，用户感兴趣的点（a图中的黄色方框部分）则上移，尽量移到decision boundary之上</p>
<h5 id="Label-Smoothing-Regularization"><a href="#Label-Smoothing-Regularization" class="headerlink" title="Label Smoothing Regularization"></a>Label Smoothing Regularization</h5><p>（1）在下图的能量方程中，$l_u(e_i)$表示用户u是否对实体$e_i$感兴趣（值为0或1），能量方程可以把相同的label保存下来，把不同的label就去掉了；（2）再找到使得能量方程取得最小值的$l_u^<em>$；（3）再通过KL距离函数求得$y_{uv}$和$\hat l_u^</em>(v)$之间的相似度，也是求最小值，可得$R(A)$，再代入损失函数</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702155919603.png" alt="image-20200702155919603"></p>
<p>$\lambda$可以不断做调整，来找到损失函数的最小值</p>
<h5 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h5><p>输入为知识图谱G，新建一个邻接矩阵M后；（2）再把用户对感兴趣的实体新建一个权重矩阵（输入mapping函数S），再进行图卷积更新特征矩阵，即$e_u^{(i)}\leftarrow \sigma (\cdot)$；（3）标签顺滑，更新$l_u^{(i)}(e)$，再代入损失函数中的 J；不断地进行这个过程，只到损失函数收敛为止</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702162309077.png" alt="image-20200702162309077"></p>
<h5 id="结果演示"><a href="#结果演示" class="headerlink" title="结果演示"></a>结果演示</h5><p>下图是几种推荐算法的比较，推荐给用户Top-K的实体，看最后的准确率如何，可以发现GCN的结构取得了非常不错的效果。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702163653325.png" alt="image-20200702163653325"></p>
<p>下面是一些可视化的对比，图1是推荐食物，可以看到GCN的结构取得了很好的效果；图2表示取不同的$\lambda$，得到的$R(A_u)$的值；图三对比了不同算法的运行速度，可以看到KGCN用时最少，速度最快</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702163938245.png" alt="image-20200702163938245"></p>
<h4 id="研究方向"><a href="#研究方向" class="headerlink" title="研究方向"></a>研究方向</h4><h5 id="人体姿势识别"><a href="#人体姿势识别" class="headerlink" title="人体姿势识别"></a>人体姿势识别</h5><p> 人体的关节就可以看作是一个节点，关节之间的骨骼就可以看作是边，结合起来就是一张图；</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702165243019.png" alt="image-20200702165243019"></p>
<p>核心框架：输入为一个视频，视频的每个时间序列都是一张图像，每一帧的视频取出来就是一张图像；把每一帧的图像先构建成一个骨骼和关节点图，再通过时间的顺序把每一帧对应的关节点都连接起来，形成一张大图，再输入GCN进行训练，通过分类器识别出这是一个跑步的动作。</p>
<p>在上图中的左下角中，横轴表示每一帧的骨骼图，纵轴表示时间，把相应的骨骼节点连接起来，变成一张大图，再输入GCN进行训练。</p>
<p>下图为实验结果，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702170313601.png" alt="image-20200702170313601"></p>
<h5 id="超图"><a href="#超图" class="headerlink" title="超图"></a>超图</h5><p>超图是图的一个基本概念，下图中的图有8个点，选择不同的点，可以有多种排列组合。如果选一个点，一共有个点可以选择，那么就有8种（$C_n^1$）组合。如果我选两个点，比如$n_3,n_8$，这两个点构成一个子图，就有6种（$C_n^2$）组合。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200702170734267.png" alt="image-20200702170734267"></p>
<p>选出不同的点后，得到不同的邻接矩阵，再进行训练。每一层选出不同组合的邻接矩阵再输入GCN进行图卷积。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703093426889.png" alt="image-20200703093426889"></p>
<p>超图卷积的核心公式就是下图中的公式(11)</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703093638629.png" alt="image-20200703093638629"></p>
<h5 id="图构建"><a href="#图构建" class="headerlink" title="图构建"></a>图构建</h5><p>对于给定的数据，新建图之后再输入GCN进行分类，我们要怎么画成一张图呢？</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703093921828.png" alt="image-20200703093921828"></p>
<p>在上图中，observed dynamics表示5个分子的运动轨迹图，我们的目标是构建新图，并预测分子的下一步轨迹；对于人体的动作，我们也得通过学习才能知道，到底是以左手还是右手为重心。</p>
<p>在下图中，展示了图构建的大概过程，编码器和解码器都是GCNs。对于编码器部分，先进行了从边到节点($v\rightarrow e$)的过程，再从节点到边($e\rightarrow v$)，再从边到节点($v\rightarrow e$)；对于解码器，先从边到点，再从点到边，进行图卷积，最后再进行分类，得到下一步的运动轨迹。</p>
<p>其中x表示节点，z表示x这个点与i、j相连的概率，z是一个概率分布。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703095234400.png" alt="image-20200703095234400"></p>
<p>预测下一步运动轨迹：在下图中，我们可以看出预测图和真实值之间差距并不大，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703100157533.png" alt="image-20200703100157533"></p>
<h5 id="子图嵌入"><a href="#子图嵌入" class="headerlink" title="子图嵌入"></a>子图嵌入</h5><p>在原来的GCN中，我们每次只会考虑一个点；如果我们现在把多个点（关联性很大的点）表示成一个点，再输入GCN，效果会不会更好呢？这也是一个很火的研究方向。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703101250920.png" alt="image-20200703101250920"></p>
<h4 id="场景应用"><a href="#场景应用" class="headerlink" title="场景应用"></a>场景应用</h4><h5 id="用户推荐系统"><a href="#用户推荐系统" class="headerlink" title="用户推荐系统"></a>用户推荐系统</h5><p>在搜索中推荐相关联的朋友给你。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703102812106.png" alt="image-20200703102812106"></p>
<h5 id="舆情监控和控制"><a href="#舆情监控和控制" class="headerlink" title="舆情监控和控制"></a>舆情监控和控制</h5><p>每个国家都需要舆情监控，比如关于假消息和真消息的分类或者监控，在网络传播过程中找到假消息源，进行封锁等，用GCN都可以起到很好的作用。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703102736481.png" alt="image-20200703102736481"></p>
<h5 id="自动驾驶"><a href="#自动驾驶" class="headerlink" title="自动驾驶"></a>自动驾驶</h5><p>所有的车都有一个车载系统，每个车载系统之间进行联网，就可以构成非常大的图。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703102850234.png" alt="image-20200703102850234"></p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200703103300945.png" alt="image-20200703103300945"></p>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p>本文是观看杨栋老师在B上讲解的直播课程《基于知识图谱和图卷积神经网络的应用和开发》的笔记，直播地址：<a href="https://live.bilibili.com/11869202" target="_blank" rel="noopener">https://live.bilibili.com/11869202</a> ，贪心科技官网录播回放：<a href="https://www.greedyai.com/course/139" target="_blank" rel="noopener">https://www.greedyai.com/course/139</a></p>
]]></content>
      <categories>
        <category>Knowledge Geaph</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>知识图谱</tag>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>Network Compression</title>
    <url>/2020/06/27/network-conpression/</url>
    <content><![CDATA[<h4 id="Why-？"><a href="#Why-？" class="headerlink" title="Why ？"></a>Why ？</h4><p>在未来我们可能需要model放到model device上面，但这些device上面的资源是有限的，包括存储控价有限和computing power有限</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627102237291.png" alt="image-20200627102237291" style="zoom:50%;"></p>
<h4 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627102418338.png" alt="image-20200627102418338" style="zoom:50%;"></p>
<h4 id="Network-Pruning"><a href="#Network-Pruning" class="headerlink" title="Network Pruning"></a>Network Pruning</h4><h5 id="Network-can-be-pruned"><a href="#Network-can-be-pruned" class="headerlink" title="Network can be pruned"></a>Network can be pruned</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627102602673.png" alt="image-20200627102602673" style="zoom:50%;"></p>
<h5 id="Network-Pruning-1"><a href="#Network-Pruning-1" class="headerlink" title="Network Pruning"></a>Network Pruning</h5><p>对于训练好的network，我们要判断其weight和neural的重要性：</p>
<ul>
<li>如果某个weight接近于0，那么我们可以认为这个neural是不那么重要的，是可以pruning的；如果是某个很正或很负的值，该weight就被认为对该network很重要；</li>
<li>如果某个neural在给定的dataset下的输出都是0，那么我们就可以认为该neural是不那么重要的</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627102810722.png" alt="image-20200627102810722" style="zoom:50%;"></p>
<p>在评估出weight和neural的重要性后，再进行排序，来移除一些不那么重要的weight和neural，这样network就会变得smaller，但network的精确度也会随之降低，因此还需要进行fine-tuning</p>
<p>最好是每次都进行小部分的remove，再进行fine-tuing，如果一次性remove很多，network的精确度也不会再恢复</p>
<h5 id="Why-Pruning"><a href="#Why-Pruning" class="headerlink" title="Why Pruning?"></a>Why Pruning?</h5><p>Q：为什么不直接train一个小的network呢？</p>
<p>A：小的network比较难train，<a href="https://www.youtube.com/watch?v=_VuWvQUMQVk" target="_blank" rel="noopener">大的network更容易optimize</a></p>
<p><strong>Lottery Ticket Hypothesis</strong></p>
<p>我们先对一个network进行初始化（<span style="color: red">红色的weight</span>），再得到训练好的network（<span style="color: purple">紫色的weight</span>），再进行pruned，得到一个pruned network</p>
<ul>
<li><p>如果我们使用pruned network的结构，再进行random init（<span style="color: green">绿色的weight</span>），会发现这个network不能train下去</p>
</li>
<li><p>如果我们使用pruned network的结构，再使用original random init（<span style="color: red">红色的weight</span>），会发现network可以得到很好的结果</p>
</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627104519698.png" alt="image-20200627104519698" style="zoom:50%;"></p>
<p>作者就说train这个network就像买大乐透一样，有的random可以tranin起来，有的不可以</p>
<p><strong>Rethinking the Value of Network Pruning</strong></p>
<p>Scratch-E/B表示使用real random initialization，并不是使用original random initialization，也可以得到比fine-tuing之后更好的结果</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627105605249.png" alt="image-20200627105605249" style="zoom:50%;"></p>
<h5 id="Pratical-Issue"><a href="#Pratical-Issue" class="headerlink" title="Pratical Issue"></a>Pratical Issue</h5><p>如果我们现在进行weight pruning，进行weight pruning之后的network会变得不规则，有些neural有2个weight，有些neural有4个weight，这样的network是不好implement出来的；</p>
<p>GPU对矩阵运算进行加速，但现在我们的weight是不规则的，并不能使用GPU加速；</p>
<p>实做的方法是将pruning的weight写成0，仍然在做矩阵运算，仍然可以使用GPU进行加速；但这样也会带来一个新的问题，我们并没有将这些weight给pruning掉，只是将它写成0了而已</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627110043391.png" alt="image-20200627110043391" style="zoom:50%;"></p>
<p>实际上做weight pruning是很麻烦的，通常我们都进行neuron pruning，可以更好地进行implement，也很容易进行speedup</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627111357372.png" alt="image-20200627111357372" style="zoom:50%;"></p>
<h4 id="Knowledge-Distillation"><a href="#Knowledge-Distillation" class="headerlink" title="Knowledge Distillation"></a>Knowledge Distillation</h4><h5 id="Student-and-Teacher"><a href="#Student-and-Teacher" class="headerlink" title="Student and Teacher"></a>Student and Teacher</h5><p>我们可以使用一个small network（student）来学习teacher net的输出分布（1:0.7…），并计算两者之间的cross-entropy，使其最小化，从而可以使两者的输出分布相近</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627111854934.png" alt="image-20200627111854934" style="zoom:50%;"></p>
<p>Q：那么我们为什么要让student跟着teacher去学习呢？</p>
<p>A：teacher提供了比label data更丰富的资料，比如teacher net不仅给出了输入图片和1很像的结果，还说明了1和7长得很像，1和9长得很像；因此，student跟着teacher net学习，是可以得到更多的information的</p>
<h5 id="Ensemble"><a href="#Ensemble" class="headerlink" title="Ensemble"></a>Ensemble</h5><p>在kaggle上打比赛，很多人的做法是将多个model进行ensemble，通常可以得到更好的精度。但在实际生活中，设备往往放不下这么多的model，这时我们就可以使用Knowledge Distillation的思想，使用student net来对teacher进行学习，在实际的应用中，我们只需要student net的model就好</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627112630920.png" alt="image-20200627112630920" style="zoom:50%;"></p>
<h5 id="Temperature"><a href="#Temperature" class="headerlink" title="Temperature"></a>Temperature</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627113311508.png" alt="image-20200627113311508" style="zoom:50%;"></p>
<h4 id="Parameter-Quantization"><a href="#Parameter-Quantization" class="headerlink" title="Parameter Quantization"></a>Parameter Quantization</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627114647806.png" alt="image-20200627114647806" style="zoom:50%;"></p>
<h4 id="Architecture-Design（most）"><a href="#Architecture-Design（most）" class="headerlink" title="Architecture Design（most）"></a>Architecture Design（most）</h4><h5 id="Low-rank-approximation"><a href="#Low-rank-approximation" class="headerlink" title="Low rank approximation"></a>Low rank approximation</h5><p>中间插入一个linear层，大小为K，那么也可以减少需要训练的参数</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627115631787.png" alt="image-20200627115631787" style="zoom:50%;"></p>
<h5 id="Review-Standard-CNN"><a href="#Review-Standard-CNN" class="headerlink" title="Review: Standard CNN"></a><strong>Review: Standard CNN</strong></h5><p>每个filter要处理所有的channel</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627120134937.png" alt="image-20200627120134937" style="zoom:50%;"></p>
<h5 id="Depthwise-Separable-Convolution"><a href="#Depthwise-Separable-Convolution" class="headerlink" title="Depthwise Separable Convolution"></a><strong>Depthwise Separable Convolution</strong></h5><p>每个filter只处理一个channel，不同channel之间不会相互影响</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627120350081.png" alt="image-20200627120350081" style="zoom:50%;"></p>
<p>和一般的convolution是一样的，有4个filter，就有4个不同的matrix</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627120519412.png" alt="image-20200627120519412" style="zoom:50%;"></p>
<p>第一步用到的参数量为$3\times3\times2=18$，第二步用到的参数量为$2\times4=8$，一共有26个参数</p>
<h5 id="Standard-CNN-vs-Depthwise-Separable-Convolution"><a href="#Standard-CNN-vs-Depthwise-Separable-Convolution" class="headerlink" title="Standard CNN vs Depthwise Separable Convolution"></a>Standard CNN vs <strong>Depthwise Separable Convolution</strong></h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627121053539.png" alt="image-20200627121053539" style="zoom:50%;"></p>
<p>对于普通的卷积，需要的参数量为$(k\times k\times I)\times O$；对于Depthwise Separable Convolution，需要的参数量为$k\times k\times I+I\times O$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627121419582.png" alt="image-20200627121419582" style="zoom:50%;"></p>
<h4 id="Dynamic-Computation"><a href="#Dynamic-Computation" class="headerlink" title="Dynamic Computation"></a>Dynamic Computation</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627122127490.png" alt="image-20200627122127490" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627122144833.png" alt="image-20200627122144833" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>network compression</tag>
      </tags>
  </entry>
  <entry>
    <title>Semi-supervised</title>
    <url>/2020/06/10/semi-supervised/</url>
    <content><![CDATA[<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610195909860.png" alt="image-20200610195909860" style="zoom:50%;"></p>
<p>对于猫狗分类问题，如果只有一部分data有label，还有其他很大一部分data是unlabeled，那么我们可以认为unlabeled data对我们网络的训练是无用的吗？</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610200040869.png" alt="image-20200610200040869" style="zoom:50%;"></p>
<p>Q：Why semi-supervised learning helps ?</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610200322008.png" alt="image-20200610200322008" style="zoom:50%;"></p>
<p>A：如图所示，图中灰色圆点表示unlabeled data，其他圆点表示labeled data。如果没有unlabeled data，此时可以用一条竖直的线将猫狗进行分类，boundary为竖直的那条线；但unlabeled data的分布也可以告诉我们一些信息，对我们的训练也是有帮助的，有了unlabeled data，此时的boundary为斜直线</p>
<h4 id="Semi-supervised-Learning-for-Generative-Model"><a href="#Semi-supervised-Learning-for-Generative-Model" class="headerlink" title="Semi-supervised Learning for Generative Model"></a>Semi-supervised Learning for Generative Model</h4><h5 id="Intuitive"><a href="#Intuitive" class="headerlink" title="Intuitive"></a>Intuitive</h5><p>不考虑unlabeled data，只有labeled data</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610201326618.png" alt="image-20200610201326618" style="zoom:50%;"></p>
<p>如果把unlabeled data也考虑进来，此时的boundary 也发生了变化</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610201826012.png" alt="image-20200610201826012" style="zoom:50%;"></p>
<h5 id="Formulation"><a href="#Formulation" class="headerlink" title="Formulation"></a>Formulation</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610204004390.png" alt="image-20200610204004390" style="zoom:50%;"></p>
<p>不同的maximum likelihood对比</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610204526467.png" alt="image-20200610204526467" style="zoom:50%;"></p>
<h4 id="Low-density-Separation-Assumption"><a href="#Low-density-Separation-Assumption" class="headerlink" title="Low-density Separation Assumption"></a>Low-density Separation Assumption</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610214224919.png" alt="image-20200610214224919" style="zoom:50%;"></p>
<h5 id="Self-training"><a href="#Self-training" class="headerlink" title="Self-training"></a>Self-training</h5><p>有labeled data和unlabeled data，重复以下过程：</p>
<ul>
<li>从labeled data中tarin了模型$f^*$；</li>
<li>将$f^*$应用到unlabeled data，得到带label的数据，称为Pseudo-label</li>
<li>从unlabeled data中移出这部分data，并加入labeled data；要移除哪部分data，要根据具体的限制条件而定</li>
<li>有了更多的label data，就可以继续训练我们的模型，返回第一步</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610205046125.png" alt="image-20200610205046125" style="zoom:50%;"></p>
<p>Q：这种训练方式对regression 有用吗 ？</p>
<p>W：不能，regression输出的是一个真实的值</p>
<p><strong>hard label vs soft label</strong></p>
<p>self-training用的是hard label；generative model用的是soft label</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610210239919.png" alt="image-20200610210239919" style="zoom:50%;"></p>
<h5 id="Entropy-based-Regularization"><a href="#Entropy-based-Regularization" class="headerlink" title="Entropy-based Regularization"></a>Entropy-based Regularization</h5><p>如果输出的每个类别的概率是相近的，那么这个模型就比较bad；输出的类别差距很大，比如某个类别的概率为1，其他都是0；我们可以用$E(y^u)$来衡量</p>
<script type="math/tex; mode=display">
E(y^u)=-\sum_{m=1}^5y_m^uln(y_m^u)</script><p>对于第一个和第二个distribution，那么$E(y^u)=0$；</p>
<p>对于第三个distribution，那么$E(y^u)=-ln(\frac{1}{5})=ln5$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610211113227.png" alt="image-20200610211113227" style="zoom:50%;"></p>
<p>那么我们现在就可以重新设计loss function，用cross entropy来估计$y^r,\hat y ^r$之间的差距，即$C(y^r,\hat y^r)$，使用labeled data，还加上了一个regularization term</p>
<script type="math/tex; mode=display">
L=\sum_{x^T}C(y^r,\hat y^r)+\lambda \sum_{x^u} E(y^u)</script><h5 id="Outlook-Semi-supervised-SVM"><a href="#Outlook-Semi-supervised-SVM" class="headerlink" title="Outlook: Semi-supervised SVM"></a>Outlook: Semi-supervised SVM</h5><p>对于unlabeled data，如果是SVM 二分类问题，可以把所有的unlabeled data都穷举为Class1或Class2，列举出所有可能的方案，再找出对应的boundary，计算loss，可以发现下图中黑色方框图具有最小的loss</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610213305033.png" alt="image-20200610213305033" style="zoom:50%;"></p>
<h4 id="Smoothness-Assumption"><a href="#Smoothness-Assumption" class="headerlink" title="Smoothness Assumption"></a>Smoothness Assumption</h4><h5 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610214254778.png" alt="image-20200610214254778" style="zoom:50%;"></p>
<p>假设：如果x是similar的，那么他们的y也是一样的</p>
<p>这样的假设是非常不精确的，下面我们做出一个更加精确的假设：</p>
<ul>
<li>x是分布不均匀的，有的地方很密集，有的地方很稀疏</li>
<li>$x^1,x^2$中间有个high density region，那么label $y^1,y^2$就很可能是一样的；但$x^2,x^3$中间没有high density region，其label相同的概率就非常小</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610214546977.png" alt="image-20200610214546977" style="zoom:50%;"></p>
<p>对于下图中的数字，2之间是有过渡形态的，所以这两个图片是similar的；而2与3之间没有过渡形态，因此是不similar的</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610215523269.png" alt="image-20200610215523269"></p>
<p>比较直观的做法是先进行cluster，再进行label</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610221020376.png" alt="image-20200610221020376" style="zoom:50%;"></p>
<h5 id="Graph-based-Approach"><a href="#Graph-based-Approach" class="headerlink" title="Graph-based Approach"></a>Graph-based Approach</h5><p>那么我们到底要怎么才能知道$x^1,x^2$到底在high density region是不是close呢 ？</p>
<p>我们可以把data point用图来表示，图的表示有时是比较nature，有时需要我们自己找出来point之间的联系</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610221342226.png" alt="image-20200610221342226" style="zoom:50%;"></p>
<p> <strong>Graph Construction</strong></p>
<p>首先定义不同point之间的相似度$s(x^i,x^j)$，可以通过以下两个算法来添加edge：</p>
<ul>
<li>KNN，对于图中红色的圆点，与其最相近的三个（k=3）neighbor相连接</li>
<li>e-Neighborhood，对于周围的neighbor，只有和他相似度大于1的才会被连接起来</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610221803745.png" alt="image-20200610221803745" style="zoom:50%;"></p>
<p>edge并不是只有相连和不相连两种选择而已，也可以给edge一些weight，让这个weight和这两个point之间的相似度成正比</p>
<p>labeled data会影响他的邻居，如果这个point是class1，那么他周围的某些point也可能是class1</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610224513039.png" alt="image-20200610224513039" style="zoom:50%;"></p>
<p><strong>Definition</strong></p>
<p>对于下图中的两幅图，如果从直观上看，我们可以认为左边的图更smooth</p>
<p>现在我们用数字来定量描述，S的定义如下</p>
<script type="math/tex; mode=display">
S=\frac{1}{2}\sum_{i,j}w_{i,j}(y^i-y^j)^2</script><p>根据公式我们可以算出左图的S=0.5，右图的S=3，值越小越smooth，越小越好</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610224714665.png" alt="image-20200610224714665" style="zoom:50%;"></p>
<p>对原来的S进行改造一下，$S=y^TLy$</p>
<p>其中$L=D-W$，W为权重矩阵，D表示将weight每行的和放到对角线的位置</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610225336692.png" alt="image-20200610225336692" style="zoom:50%;"></p>
<p>loss function其中一项就包括cross entropy计算的loss；smoothness的量S，前面再乘上一个可以调整的参数$\lambda$ ，$\lambda S$就表示一个regularization term</p>
<p>网络的整体目标是使loss function 取得最小值，即cross entropy项和smoothness都必须要达到最小值，和其他的网络一样，计算相应的gradient，做gradient descent即可</p>
<p>如果要计算smoothness不一定非要在output的地方，也可以是其他位置，比如hidden layer拿出来进行一些transform，或者直接拿hidden layer，都可以计算smoothness</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610225851786.png" alt="image-20200610225851786" style="zoom:50%;"></p>
<h4 id="Better-Representation"><a href="#Better-Representation" class="headerlink" title="Better Representation"></a>Better Representation</h4><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200610230634932.png" alt="image-20200610230634932" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>semi-supervised</tag>
        <tag>smoothness</tag>
      </tags>
  </entry>
  <entry>
    <title>Sequence to Sequence</title>
    <url>/2020/06/28/seq2seq/</url>
    <content><![CDATA[<h4 id="Generation"><a href="#Generation" class="headerlink" title="Generation"></a>Generation</h4><p>可以generation文字，也可以generation图片</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627143309760.png" alt="image-20200627143309760" style="zoom:50%;"></p>
<p>如果想生成一张图片，我们可以把图片的每个pixel看成一个character，从而组成一个sentence，比如下图中蓝色pixel就表示blue，红色的pixel表示red,……</p>
<p>接下来就可以用language model来生成图片，最初时间步的输入是特殊字符BOS</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627143546864.png" alt="image-20200627143546864" style="zoom:50%;"></p>
<p>对于一般的generation，如下图的右上部分所示，先根据蓝色pixel生成红色pixel，再根据红色pixel生成黄色pixel，再根据黄色pixel生成灰色pixel，….，并没有考虑pixel之间的位置关系</p>
<p>还有另外一个比较理想的生成图像方法，如果考虑了pixel之间的位置关系，如下图的右下部分所示，黑色pixel由附近的红色和灰色pixel生成</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627144748287.png" alt="image-20200627144748287" style="zoom:50%;"></p>
<p>在上图的左上角部分，该lstm块输入了三组参数，也输出了三组参数，把这些方块叠起来就可以达到中部位置图像的效果</p>
<p>对于一个$3\times 3$的画布，就是我们想要生成的图像的大小，一开始画布上还没有任何内容；现在在画布的左下角放入一个convolution的filter， 把其输出在放到3D的lstm的左下角（input），由于lstm也是有memory的，经过不断的process，可以到左上角，从而产生蓝色的pixel；再把蓝色的pixel放到画布的左下角</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627150215465.png" alt="image-20200627150215465" style="zoom:50%;"></p>
<p>把filter向右移，结合上次蓝色pixel的输出，根据上文的步骤，就可以生成红色的pixel，再把红色pixel放到画布对应的位置</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627150953989.png" alt="image-20200627150953989" style="zoom:50%;"></p>
<h4 id="Conditional-Generation"><a href="#Conditional-Generation" class="headerlink" title="Conditional Generation"></a>Conditional Generation</h4><p>必须是有条件的generation，要联系上下文</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627152018372.png" alt="image-20200627152018372" style="zoom:50%;"></p>
<p>如果要对一张图片进行解释，我们先使用一个CNN model将image转化为一个vector（红色方框），先生成了第一个单词”A”，在生成第二个单词时，还需要将整个image的vector作为输入，不然RNN可能会忘记image的一些信息</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627152512575.png" alt="image-20200627152512575" style="zoom:50%;"></p>
<p>RNN也可以用来做机器翻译，比如我们想要翻译“机器学习”，就先把这四个字分别输入绿色的RNN里面，最后一个时间节点的输出（红色方框）就包含了整个sentence的information，这个过程称之为<strong>Encoder</strong></p>
<p>把encoder的information再作为另外一个rnn的input，再进行output，这个过程称之为<strong>Decoder</strong></p>
<p>encoder和decoder是jointly train的，这两者的参数可以是一样的，也可以是不一样的</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627153002771.png" alt="image-20200627153002771" style="zoom:50%;"></p>
<p> encoder将之前所有说过的话都进行整合了，再作为decoder的input</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627154111622.png" alt="image-20200627154111622" style="zoom:50%;"></p>
<h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><h5 id="Dynamic-Conditional-Generation"><a href="#Dynamic-Conditional-Generation" class="headerlink" title="Dynamic Conditional Generation"></a>Dynamic Conditional Generation</h5><p>如果需要翻译的文字非常复杂，无法用一个vector来表示，就算可以表示也没办法表示全部的关键信息，这时候如果decoder每次input的都还是同一个vector，就不会得到很好的结果</p>
<p>因此我们现在先把“机器”作为decoder的第一个输入$c^1$，这样就可以得到更好的结果</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627154318421.png" alt="image-20200627154318421" style="zoom:50%;"></p>
<h5 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h5><p>绿色方块为RNN中hidden layer的output，分别是$h^1,h^2,h^3,h^4$；还有一个vector $z^0$，是可以通过network学出来的；把这两者输入一个match函数，可以得到$h^1,z^0$之间的match分数 $\alpha _0^1$</p>
<p>这个match函数可以自己设计，可以有参数，也可以没有参数</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627154905302.png" alt="image-20200627154905302" style="zoom:50%;"></p>
<p>得出的$\alpha _0^1,\alpha _0^2,\alpha _0^3,\alpha _0^4$再输入softmax函数，得到$\hat\alpha _0^1,\hat\alpha _0^2,\hat\alpha _0^3,\hat\alpha _0^4$，乘上$h^i$再加起来，就可以得到$c^0$，来作为decoder input，根据decoder，可得出第一个翻译的单词“machine”；$z^1$为decoder RNN其中hidden layer的输出，用$z^1$再来得出相应的decoder input</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627155414554.png" alt="image-20200627155414554" style="zoom:50%;"></p>
<p>再来计算$z^1,h^1$之间的match分数$\alpha_1^1$，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627155937349.png" alt="image-20200627155937349" style="zoom:50%;"></p>
<p>再继续算出$c^1$作为decoder的input，从而得出第二个单词“learning”</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627160046413.png" alt="image-20200627160046413" style="zoom:50%;"></p>
<p>一直重复这个过程，只到全部翻译完成</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627160411647.png" alt="image-20200627160411647" style="zoom:50%;"></p>
<h5 id="Speech-Recognition"><a href="#Speech-Recognition" class="headerlink" title="Speech Recognition"></a>Speech Recognition</h5><p>语音信号也可以看成一系列的vector sequence，第一个红色方框所在的帧match分数很高，就把对应的vector放到decoder，machine就会得出“h”</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627193908934.png" alt="image-20200627193908934" style="zoom:50%;"></p>
<h5 id="Image-Caption-Generation"><a href="#Image-Caption-Generation" class="headerlink" title="Image Caption Generation"></a>Image Caption Generation</h5><p>image可以分成多个region，每个region可以用一个vector来表示，计算这些vector和$z^0$之间match的分数，为0.7、0.1….，再进行weighted sum，得到红色方框的结果，再输入RNN，得到Word1，此时hidden layer的输出为$z^1$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627195250525.png" alt="image-20200627195250525" style="zoom:50%;"></p>
<p>再计算$z^1$和vector之间的match分数，把分数作为RNN的input，从而得出Word 2</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627195511422.png" alt="image-20200627195511422" style="zoom:50%;"></p>
<p>这里是一些具体的例子</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627195914955.png" alt="image-20200627195914955" style="zoom:50%;"></p>
<p>但也会产生一些不好的结果</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627200010931.png" alt="image-20200627200010931" style="zoom:50%;"></p>
<h5 id="Memory-Network"><a href="#Memory-Network" class="headerlink" title="Memory Network"></a>Memory Network</h5><p>memory network是有一篇文章，问machine一个问题，machine能够给出对应的答案</p>
<p>先将document分成多个vector，再给出这些vector与问题q之间match的分数$\alpha^1,..,\alpha^N$，与$x^i$分别相乘，得到extracted information，作为DNN的input，再进行不断的训练，从而得出对应的answer</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627200439344.png" alt="image-20200627200439344" style="zoom:50%;"></p>
<p>还有一个更加复杂的版本</p>
<p>match的部分和extracted的部分不一定是一样的，相当于输入了两组不同的参数，比如可以把$x^i$乘上一个matrix，就可以变成另外一组参数$h^i$，这个matrix是可以自动学出来的</p>
<p>这时计算extracted information的方式就发生了一些变化；sum之后的值不仅输入DNN，还会与问题q进行结合，再重新计算match分数，不断进行一个反复思考的过程</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627201014065.png" alt="image-20200627201014065" style="zoom:50%;"></p>
<p>q可以用来计算attention，计算出extracted information之后，与q加起来；上一次sum的结果会再继续参与计算attention，再extract，再进行sum，作为DNN的input，得到最后的答案</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627201933880.png" alt="image-20200627201933880" style="zoom:50%;"></p>
<p>可以把这个模型看作是两层layer的network，也有back propagation来更新network的参数</p>
<h5 id="Neural-Turing-Machine"><a href="#Neural-Turing-Machine" class="headerlink" title="Neural Turing Machine"></a>Neural Turing Machine</h5><p>Neural Turing Machine不仅可以读取memory的内容，也可以根据match分数来修改memory的内容；即你不仅可以通过memory读取information，也可以把information写到memory里面去</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627202652334.png" alt="image-20200627202652334" style="zoom:50%;"></p>
<p>对于初始值的memory $m_0^i$，计算出对应的match分数$\hat\alpha_0^i$，在进行weighted sum，得出$r^0$，来作为另外一个network f的input，这个network相当于是一个controller，f可以是DNN、SVM、GRU等；</p>
<p>$r^0,x^1$作为该network的输入，可以得出三个output，这些output可以控memory，比如新的attention是什么，新的memory里面的值如何进行修改</p>
<p>新的attention计算加入了$k^1$，即$\alpha_1^i=cos(m_0^i,k^1)$，再输入softmax，得到新的$\hat\alpha_1^i$的distribution</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627203211682.png" alt="image-20200627203211682" style="zoom:50%;"></p>
<p>$e^1$里面的值表示把原来memory里面的值清空，$a^1$表示把新的值写入memory，通过下面的公式再计算新的memory $m_1^i$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627204251989.png" alt="image-20200627204251989" style="zoom:50%;"></p>
<p>得到新的memory $m_1^i$之后，再计算出新的attention $\hat\alpha_1^i$，再进行weighted sum得到$r^1$，加上新的$x^2$作为network的input，得出新的output，再来对整个network进行操控</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627204747750.png" alt="image-20200627204747750" style="zoom:50%;"></p>
<h4 id="Tips-for-Generation"><a href="#Tips-for-Generation" class="headerlink" title="Tips for Generation"></a>Tips for Generation</h4><h5 id="Attention-1"><a href="#Attention-1" class="headerlink" title="Attention"></a>Attention</h5><p>$\alpha_t^i$下标表示time，上标表示component，该视频包括4个component，有4个时间节点；在第一个时间节点产生attention $\alpha_1^1,\alpha_1^2,\alpha_1^3,\alpha_1^4$，生成了第一个word $w_1$</p>
<p>attention也是可以调节的，有时会出现一些<strong>bad attention</strong>，在产生第二个word（$w_2$,<span style="color: red">women</span>）时，focus到第二个component，在产生第四个word $w_4$时，也focus到第二个component上，也是<span style="color: green">women</span>，多次attent在同一个frame上，就会产生一些很奇怪的结果</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627205536622.png" alt="image-20200627205536622" style="zoom:50%;"></p>
<p><strong>Good Attention</strong>：至少要包含input的每一个frame，每个frame都应该attent一下，每个frame进行attent的量也不能太多，这个量最好是同等级的</p>
<p>Q：那么如何保证这个标准呢？</p>
<p>A：有学者提出了一个regularization term，有一个新的可学习的参数$\tau$，$\sum_t\alpha_t^i$表示所有attention的sum，再计算$\sum_i(\tau-\sum_t\alpha_t^i)^2$的值，这个值越小越好；对于上文所说的bad attention的情况，这个值算出来是很大的，因此network就会再进行不断地学习，只到term的值不断减小</p>
<h5 id="Mismatch-between-Train-and-Test"><a href="#Mismatch-between-Train-and-Test" class="headerlink" title="Mismatch between Train and Test"></a>Mismatch between Train and Test</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627213016984.png" alt="image-20200627213016984" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627213436202.png" alt="image-20200627213436202" style="zoom:50%;"></p>
<p><strong>Exposure Bias</strong>：在training的时候，input为真正的答案；但在testing的时候，input为上一个时间节点的output</p>
<p>在下图中，由于RNN从来没有到右子树B训练过，但如果在testing时一开始就遇到了B，network就会出现错误</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627214936987.png" alt="image-20200627214936987" style="zoom:50%;"></p>
<h5 id="Modifying-Training-Process"><a href="#Modifying-Training-Process" class="headerlink" title="Modifying Training Process?"></a>Modifying Training Process?</h5><p>那么我们如何解决这种mismatch问题呢？可以尝试modify训练process</p>
<p>如果machine现在output B，即使是错误的output（和reference A不一样），我们也应该让这个错误的output作为下一次的input，那么training和testing就是match的</p>
<p>但在实际操作中，training是非常麻烦的；现在我们使用gradient descent来进行training，第一个gradient的方向告诉我们要把A的几率增大，output B，在第二个时间点，input为B，看到reference为B，把B的几率增大，就可以和reference相对应起来；</p>
<p>但实际上，第一个output为A的几率上升，那么output发生了变化，第二个时间点的input就发生了变化，是A；那么我们之前学习到的让B上升就没有意义了</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627215358053.png" alt="image-20200627215358053" style="zoom:50%;"></p>
<h5 id="Scheduled-Sampling"><a href="#Scheduled-Sampling" class="headerlink" title="Scheduled Sampling"></a><strong>Scheduled Sampling</strong></h5><p>由于使用Modifying Training Process很难train，现在我们就使用Scheduled Sampling</p>
<p>对于到底是model里的ouput，还是reference来作为input，我们可以给一个几率；铜板是正面，就使用model的output，如果是反面，就用reference</p>
<p>右上角的图，纵轴表示from reference的几率，一开始只看reference，reference的几率不断变小，model的几率不断增加</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627221402859.png" alt="image-20200627221402859" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627221933432.png" alt="image-20200627221933432" style="zoom:50%;"></p>
<h5 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627221956837.png" alt="image-20200627221956837" style="zoom:50%;"></p>
<p>同时走两条path</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627222144464.png" alt="image-20200627222144464" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627222358014.png" alt="image-20200627222358014" style="zoom:50%;"></p>
<h5 id="Better-idea？"><a href="#Better-idea？" class="headerlink" title="Better idea？"></a>Better idea？</h5><p>为什么不直接把distribution作为下一个input呢？即下图中的右图</p>
<p>但右边的结果会比较差，对于几率接近的输入，左图会sample一个几率最高的（高兴），但右图会保留这个distribution</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627222713634.png" alt="image-20200627222713634" style="zoom:50%;"></p>
<h5 id="Object-level-v-s-Component-level"><a href="#Object-level-v-s-Component-level" class="headerlink" title="Object level v.s. Component level"></a>Object level v.s. Component level</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627223140253.png" alt="image-20200627223140253" style="zoom:50%;"></p>
<p>应该是考虑整个sentence的准确率如何，而不应该只考虑单个的word</p>
<h5 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning?"></a>Reinforcement learning?</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627223424266.png" alt="image-20200627223424266" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200627223531560.png" alt="image-20200627223531560" style="zoom:50%;"></p>
<h4 id="Pointer-Network"><a href="#Pointer-Network" class="headerlink" title="Pointer Network"></a>Pointer Network</h4><p>定义如下，找到其中的几个point，将剩下的point包围起来</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628095239086.png" alt="image-20200628095239086" style="zoom:50%;"></p>
<h5 id="Seq2seq"><a href="#Seq2seq" class="headerlink" title="Seq2seq"></a>Seq2seq</h5><p>input为4个point，但output只有三个point ($P_1,P_2,P_4$)，那这个问题还是sequence to sequence吗？</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628095644699.png" alt="image-20200628095644699" style="zoom:50%;"></p>
<p>由于output的个数是提前设定好的，并不能动态改变；<span style="color: red">在training的时候，output很只有50个point，但在testing的时候，output有100个point，由于之前只学习了50个point，因此现在并不能生成51-100之间的点</span>；因此这种做法是不可取的</p>
<p>这里我们对network进行了改进</p>
<p>对于输入的5个point（加入了END），先得到RNN中hidden layer的输出（$h^0,h^1,h^2,h^3,h^4,h^5$），计算attention weight（match score），再把这个distribution输入argmax函数，输出概率最大的point，即 1</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628100108506.png" alt="image-20200628100108506" style="zoom:50%;"></p>
<p>把这个output 1再输入另外一个RNN network，其hidden layer的输出为$z^1$，再根据$z^1,h^i$来计算相应的attention weight，输入argmax函数，得到output为 4</p>
<p>再把point 4作为另外一个RNN network的input，其hidden layer的输出为$z^2$，….</p>
<p>这个process会一直持续，知道END出现</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628101004928.png" alt="image-20200628101004928" style="zoom:50%;"></p>
<p><span style="color: red">对于改进后的network，如果现在的input是100个，那么其output就会有100个不同的选择，没有了之前的限制</span></p>
<h5 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h5><p>对于机器翻译，如果input包含一些地名、人名，并不需要进行翻译，这是我们就可以用pointer network，将这些名词直接copy</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628102849708.png" alt="image-20200628102849708" style="zoom:50%;"></p>
<p>对于chat-bot，也有一些名词并不需要学习，可以直接copy</p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Sequence to Sequence</tag>
      </tags>
  </entry>
  <entry>
    <title>T-distributed Stochastic Neighbor Embedding (t-SNE)</title>
    <url>/2020/06/28/t-SNE/</url>
    <content><![CDATA[<blockquote>
<p>本文主要叙述了t-SNE，即T-distributed Stochastic Neighbor Embedding ；先介绍了LLE的主要思想，再总结了它的缺点，从而引出t-SNE；</p>
</blockquote>
<h4 id="Manifold-Learning"><a href="#Manifold-Learning" class="headerlink" title="Manifold Learning"></a>Manifold Learning</h4><p>在高维空间里，距离该点很远的点很可能与这个点也是有关联的，因此我们可以把3-D的空间进行降维，那么我们就可以更方便地进行clustering或unsupervised learning 任务</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628202133893.png" alt="image-20200628202133893" style="zoom:50%;"></p>
<h4 id="Locally-Linear-Embedding-LLE"><a href="#Locally-Linear-Embedding-LLE" class="headerlink" title="Locally Linear Embedding (LLE)"></a>Locally Linear Embedding (LLE)</h4><p>用$w_{ij}$表示$x_i,x_j$之间的联系，先找到使得$\sum_i ||x^i-\sum_jw_{ij}x^j||_2$最小化的$w_{ij}$，再根据这个$w_{ij}$来找到降维的结果$z_i,z_j$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628202738023.png" alt="image-20200628202738023" style="zoom:50%;"></p>
<p>如果并不知道之前的$x_i,x_j$，那么就可以用LLE这种方法，也可以得出$z_i,z_j$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628203501209.png" alt="image-20200628203501209" style="zoom:50%;"></p>
<h4 id="Laplacian-Eigenmaps"><a href="#Laplacian-Eigenmaps" class="headerlink" title="Laplacian Eigenmaps"></a>Laplacian Eigenmaps</h4><p>Review: 在之前的semi-supervised learning中，如果$x^1,x^2$在一个high density region内是相近的，那么我们就可以认为$\hat y^1,\hat y^2$也有类似的表现</p>
<p>如果$y^i,y^j$是connected的，那么其$w_{ij}$就是对应的相似度；如果没有connect，其$w_{ij}$就是0</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628204015777.png" alt="image-20200628204015777" style="zoom:50%;"></p>
<p>我们也可以得出类似smoothness的式子，计算$z^i,z^j$之间的smoothness</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628205136836.png" alt="image-20200628205136836" style="zoom:50%;"></p>
<p>那么我们现在的目标就是找到$z^i,z^j$，来使S达到最小值，还需要有一些额外的constrains</p>
<p>现在我们对z加入一些constrains，如果降维后z的维数是M，那么我们就不希望取出来的这些点还生活在比M还低维的空间里面；我们现在希望把塞进高维空间的低维空间展开，我们就不希望展开之后的点在一个更低维的空间里面</p>
<h4 id="T-distributed-Stochastic-Neighbor-Embedding-t-SNE"><a href="#T-distributed-Stochastic-Neighbor-Embedding-t-SNE" class="headerlink" title="T-distributed Stochastic Neighbor Embedding (t-SNE)"></a>T-distributed Stochastic Neighbor Embedding (t-SNE)</h4><p>对于之前的LLE方法，类似的data之间是很close的，<u>但不同类别之间的data却没有分开，是叠成一团的</u></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628210651621.png" alt="image-20200628210651621" style="zoom:50%;"></p>
<p><strong>为了找到对应的$z^i,z^j$</strong>，先计算$x^i,x^j$之间的相似度$S(x^i,x^j)$，再得出一个分布 $P(x^j|x^i)$；还要计算$z^i,z^j$之间的相似度$S’(z^i,z^j)$，得出分布$Q(z^j|z^i)$；</p>
<p>这两个分布应该越接近越好，使用L来表示</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628211215304.png" alt="image-20200628211215304" style="zoom:50%;"></p>
<p>可以使用gradient descent，有了L函数，再分别对$z^i,z^j$求偏微分即可</p>
<p>但t-SNE要对所有的point之间都求similarity，因此计算量比较大，在数据量很大的情况下电脑的计算速度会非常慢</p>
<p>因此，对于很高的dimensions，通常先做降维（PCA），比如可以降维到50维，再使用t-SNE降到2维</p>
<p><span style="color: red">通常我们使用t-SNE来对高维的数据进行可视化</span></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628211326492.png" alt="image-20200628211326492" style="zoom:50%;"></p>
<p>在上图中，红色曲线表示SNE，蓝色曲线表示t-SNE，纵轴表示distribution；如果我们想要维持相同的distribution，即在同一个水平线上，就达到了如图所示的效果；相同的几率，t-SNE的$||z^i-z^j||_2$之间的距离越大</p>
<p>如果本来就离得很近，那么经过t-SNE之间的距离还是很小；如果本来就离得很远，那么从原来的distribution拉到t-SNE之后，距离会更远；</p>
<p>到实际的例子中，<span style="color: red">如果本来是同一个类别的data，由于这些data之间的距离很近，不会收到t-SNE很大的影响；但如果是属于不同类别的data，距离是比较远的，t-SNE会放大这种距离</span></p>
<p>对于下图中的MNIST，先使用PCA进行降维，再进行可视化，就可以得到下图中的good visualization</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628213646582.png" alt="image-20200628213646582" style="zoom:50%;"></p>
<p>下图中有一个更加直观的例子，使用t-SNE算法，运用gradient descent的思想，不同类别的data之间的距离会越来越大</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628214657615.png" alt="image-20200628214657615" style="zoom:50%;"></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200628214721028.png" alt="image-20200628214721028" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Unsupervised Learning</tag>
        <tag>Dimension Reduction</tag>
        <tag>LLE</tag>
        <tag>t-SNE</tag>
      </tags>
  </entry>
  <entry>
    <title>多处理机</title>
    <url>/2020/03/11/%E5%A4%9A%E5%A4%84%E7%90%86%E6%9C%BA/</url>
    <content><![CDATA[<h5 id="计算机分类"><a href="#计算机分类" class="headerlink" title="计算机分类"></a>计算机分类</h5><ul>
<li>按照<u>Flynn分类</u>法，可将计算机分为<u>SISD、SIMD、MISD、MIMD</u>四类。</li>
<li><u>冯氏分类</u>法，<u>字串位串、字串位并、字并位串、字并位并</u>。</li>
<li><u>Handler分类法</u>。</li>
</ul>
<h5 id="存储器系统结构"><a href="#存储器系统结构" class="headerlink" title="存储器系统结构"></a>存储器系统结构</h5><ul>
<li>集中式存储器：各处理器可共享一个集中式的物理存储器；</li>
<li>分布式存储器：存储器分布到各个处理器中。<ul>
<li>如果大多数的访存都是针对本地节点进行的，就可以<u>降低</u>对存储器和互联网络的<u>带宽要求</u>；</li>
<li>对本地存储器的<u>访问延迟小</u>。</li>
<li>缺点：处理器之间的<u>通信</u>较为<u>复杂</u>。</li>
</ul>
</li>
</ul>
<p>存储器在物理上分布到各节点中，但<u>逻辑地址空间</u>的组织方式以及处理器之间<u>通信方式</u>不同：</p>
<ul>
<li>把在物理上分离的存储器作为一个<u>统一的共享逻辑空间</u>进行编址，不同处理器上的同一个物理地址指向同一个存储单元。<u>分布式共享存储器系统（DSM）</u>。采用<u>共享存储器</u>通信机制。</li>
<li>把每个节点中的存储器编址为一个<u>独立的地址空间</u>，不同节点的地址空间之间是相互独立的，不同处理器上的同一个物理地址指向不同的存储单元。<u>机群</u>。采用<u>消息传递</u>机制。</li>
</ul>
<h5 id="通信机制"><a href="#通信机制" class="headerlink" title="通信机制"></a>通信机制</h5><p><strong>共享存储器</strong></p>
<p>处理器之间的通信是通过load和store指令对相同存储器地址进行读/写来实现的。</p>
<ul>
<li>与常用的对称式多处理机的通信机制兼容；</li>
<li>处理机之间的通信方式比较复杂时，编程相对容易；</li>
<li>在开发程序时，可以将重点放到对性能影响较大的数据访问上；</li>
<li>通信数据量小时，通信开销小，贷款利用好；</li>
<li>可以通过Cathe缓冲技术来减少远程通信的频度。</li>
</ul>
<p><strong>消息传递</strong></p>
<p>处理器之间是通过显示地发送消息来进行通信的，把消息看成是一个远程进程调用（RPC）。</p>
<ul>
<li>硬件更简单；</li>
<li>通信是显示的，可以更容易搞清楚何时发生通信以及通信开销是多少；</li>
<li>显示通信可以使编程者重点注意并行计算的主要通信开销，使之有可能开发出结构更好、性能更高的并行程序；</li>
<li>可以减少不当同步带来错误的可能性。</li>
</ul>
<h5 id="并行处理面临的挑战"><a href="#并行处理面临的挑战" class="headerlink" title="并行处理面临的挑战"></a>并行处理面临的挑战</h5><ul>
<li>程序中的并行性有限；</li>
<li>相对较大通信开销。</li>
<li>计算：Amdahl</li>
</ul>
<script type="math/tex; mode=display">
加速比=\frac{1}{\frac{可加速部分比例}{部件加速比}+(1-可加速部分比例)}</script><script type="math/tex; mode=display">
S=\frac{1}{\frac{Fe}{Se}+(1-Fe)}</script><h5 id="并行计算机系统结构（MIMD）"><a href="#并行计算机系统结构（MIMD）" class="headerlink" title="并行计算机系统结构（MIMD）"></a>并行计算机系统结构（MIMD）</h5><ul>
<li>PVP</li>
<li><strong>SMP</strong>，对称式共享存储器多处理机</li>
<li>DSM</li>
<li><strong>MPP</strong>，大规模并行处理机</li>
<li><strong>机群</strong></li>
</ul>
<h5 id="对称式共享存储器多处理机（SMP）"><a href="#对称式共享存储器多处理机（SMP）" class="headerlink" title="对称式共享存储器多处理机（SMP）"></a>对称式共享存储器多处理机（SMP）</h5><p>也称为集中式共享多处理机，一般由十几个处理器构成，各处理器共享一个集中式的物理存储器，这个主存相对于各处理器的关系是对称的。</p>
<ul>
<li>商用处理器</li>
<li>共享存储器</li>
<li>集中共享</li>
</ul>
<h5 id="大规模并行处理机（MPP）"><a href="#大规模并行处理机（MPP）" class="headerlink" title="大规模并行处理机（MPP）"></a>大规模并行处理机（MPP）</h5><p>按照当前的标准，具有几百台~几千台处理机的任何机器都是大规模并行处理系统。</p>
<ul>
<li>处理节点使用商用微处理器，每个节点可以有多个微处理器；</li>
<li>具有较好的可扩展性；</li>
<li>采用分布式非共享的存储器，各节点有自己的地址空间；</li>
<li>采用消息传递的通信机制。</li>
</ul>
<h5 id="机群"><a href="#机群" class="headerlink" title="机群"></a>机群</h5><p>是一种价格低廉、易于构建、扩放性极强的并行计算机系统。它由多台同构或者异构的独立计算机通过高性能网络和局域网连在一起，协同完成特定的并行计算任务。从用户的角度看，它就是一个单一的、集中的计算资源。</p>
<ul>
<li>商用处理器</li>
<li>分布非共享式</li>
<li>消息传递</li>
</ul>
]]></content>
      <categories>
        <category>计算机体系结构</category>
      </categories>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>机群</tag>
        <tag>MPP</tag>
        <tag>SMP</tag>
        <tag>消息传递</tag>
        <tag>共享存储器</tag>
      </tags>
  </entry>
  <entry>
    <title>Gradient Descent</title>
    <url>/2020/06/04/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</url>
    <content><![CDATA[<h4 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h4><p>梯度下降的目标是使损失函数L最小化，$\theta^* = arg\ min\ L(\theta)$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603152209444.png" alt="image-20200603152209444" style="zoom:50%; "></p>
<h4 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h4><p>从$\theta_0$开始，先计算出$\theta_0$的梯度，其中红色箭头表示梯度的方向，蓝色箭头表示移动的方向。<br><u>梯度的方向是函数值在这个点增长最快的方向，想要使损失函数L的值达到最小值，就必须要往相反的方向运动。</u></p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603152117920.png" alt="image-20200603152117920" style="zoom: 50%;"></p>
<p>学习率会出现以下四种不同的情况：</p>
<ul>
<li>学习率太小，即图中蓝色的线，每次跨越的步长很小很小，梯度每次变化的值也小，模型要达到local minima，就必须需要更多的训练时间；</li>
<li>学习率太大，即图中绿色的线，每次跨越的步长会很大，很可能形成在山谷之间震荡的现象；</li>
<li>学习率特别大，即图中黄色的线，就很可能会直接跳出local minima，loss会越来越大；</li>
<li>学习率刚好合适，即图中红色的线，每次跨越的步长非常合适，达到local minima的时间也不需要特别多。</li>
</ul>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603161210664.png" alt="image-20200603161210664" style="zoom:50%;"></p>
<p>由于手动设置learning rate会导致很多问题，就出现了一些自适应的梯度调整方法。</p>
<ul>
<li>刚开始训练时，我们离local minimum的距离还很远，因此可以使用稍大的learning rate；</li>
<li>在经过多次的训练后，离local minimum的距离已经很近了，所以这时可以使用小的learning rate；</li>
<li>在经过t次的训练后，learning rate可以衰减为$\eta^t=\frac{\eta}{\sqrt{t+1}}$</li>
</ul>
<h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><blockquote>
<p>Divide the learning rate of each parameter by the <strong>root mean square of its previous derivatives</strong></p>
</blockquote>
<h5 id="理论推导"><a href="#理论推导" class="headerlink" title="理论推导"></a>理论推导</h5><p>使用这个公式来更新参数w，$w^{t+1} \leftarrow  w^t - \frac{\eta^t}{\sigma^t}g^t$<br>其中，t表示第t次的update，$g^t = \frac{\partial L(\theta ^t)}{\partial w}$，是损失函数L对参数w的导数，$\sigma^t$表示其先前导数的均方根（root mean square）</p>
<p>计算w的具体例子如下所示，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603165047318.png" alt="image-20200603165047318" style="zoom: 50%; "></p>
<p>得出了$\sigma^t$和$\eta^t$的表达式后，再带入原式，消除分母上的$\sqrt{t+1}$即可得出下面的公式，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603165417281.png" alt="image-20200603165417281" style="zoom:50%;"></p>
<h5 id="Contradiction"><a href="#Contradiction" class="headerlink" title="Contradiction"></a>Contradiction</h5><p>如上图所示，对于一般的梯度下降算法（vanilla gradient descent），当梯度g越大时，步长就越大；对于Adagrad，   $g^t$在分子上，梯度越大步长也越大，$\sum_{i=0}^t(g^i)^2$在分母上，数值越大步长也就越小，看似出现了一个矛盾。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603172539601.png" alt="image-20200603172539601" style="zoom:50%;"></p>
<p>有学者对此也做出了解释，认为Adagrad可以解释$g^t$和$\sum_{i=0}^t(g^i)^2$之间的反差，造成了反差的效果。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603173305074.png" alt="image-20200603173305074" style="zoom:50%;"></p>
<p><strong>gradient越大，函数值离minima的距离就越远</strong>这个说法不一定在所有情况下都是成立的。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603174942400.png" alt="image-20200603174942400" style="zoom:50%;"></p>
<p>对于左图中的两个参数w1和w2，画两条直线，保持其中一个变量不变，得出另一个变量的变化曲线，分别对应右图中的曲线。在右图中，对于w1中的a点和w2中的c点，c点距离minimum的距离最近，但梯度却更大。因此在分析梯度和步长时，我们不能只考虑一阶导数的大小，还必须要要考虑二阶导数的大小，即$y^{‘’}=2a$。</p>
<p>右图中的w1曲线，曲率半径比w2的曲线更大，一阶导数变化得更平缓，因此二阶导数的变化就比w2大</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603175920345.png" alt="image-20200603175920345" style="zoom:50%;"></p>
<p>再来回顾下Adagrad中每次更新w的表达式，$w^{t+1}\leftarrow w^t - \frac{\eta}{\sqrt{\sum_{t=0}^t(g^i)^2}}g^t$</p>
<p>一阶导数用$g^t$表示，二阶导数的值则用分母中的$\sum_{i=0}^t(g^i)^2$来进行评估，即使用一阶导数的值来表示二阶导数的值。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603180425226.png" alt="image-20200603180425226" style="zoom:50%;"></p>
<h4 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h4><p>对于传统的梯度下降算法，损失函数L的计算包含了所有的样本;<br>随机梯度下降算法，损失函数$L^n$则只使用其中一个样本，计算效率可以提高很多</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603215238564.png" alt="image-20200603215238564" style="zoom:50%;"></p>
<p>对比示意图如下，</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603215557501.png" alt="image-20200603215557501" style="zoom:50%;"></p>
<h4 id="Feature-scaling"><a href="#Feature-scaling" class="headerlink" title="Feature scaling"></a>Feature scaling</h4><h5 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h5><blockquote>
<p>Make different features have the same scaling</p>
</blockquote>
<p>使不同量级的数据集都具有相同的规模，比如x2的都是大于100的值，经过feature scaling，就可以使x2的数值范围和x1相接近。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603220106332.png" alt="image-20200603220106332" style="zoom:50%;"></p>
<p>x2对应w2，x1对应w1，对于同一个w1、w2，但x2的数值不同</p>
<p>在左图中，由于x1的数值相对于x2来说都很小，x1的变化对于y来说影响很小，w1对y的影响也很小，对loss的影响也小，因此梯度$\frac{\partial L}{\partial w_1}$在w1方向的变换也比较平缓；x2的数值较大，对loss的影响也大，因此梯度$\frac{\partial L}{\partial w_2}$在w2方向的变换就比较sharp</p>
<p>在右图中，x1和x2的规模（scale）是接近的，对y的影响不相上下，对loss的影响也差不多</p>
<h5 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h5><p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603225755433.png" alt="image-20200603225755433" style="zoom:50%;"></p>
<p>计算方式：用$m_i$表示当前样本的平均值，$\sigma_i$为当前样本的标准差，i表示维度，$x_i^r$表示第r个example，使用公式$x_i^r \leftarrow \frac{x_i^r-m_i}{\sigma_i}$来进行归一化计算</p>
<p>feature scaling其实就是将每一个example都进行归一化，使之服从标准正态分布$f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$</p>
<script type="math/tex; mode=display">
\frac{X-\bar {X}}{\sqrt{D(x)}} \sim N(0,1)</script><h4 id="Gradient-Descent-Theory"><a href="#Gradient-Descent-Theory" class="headerlink" title="Gradient Descent Theory"></a>Gradient Descent Theory</h4><h5 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h5><p>在求解最小化问题时，$\theta^* = arg\ min\ L(\theta)$，每次更新$\theta$的值，并不一定能使$𝐿(\theta^0) &gt;𝐿(\theta^1)&gt;𝐿(\theta^2) &gt;⋯$成立</p>
<p>对于给出的$\theta^1,\theta^2$，我们要如何根据这些值来找出最小的loss？这也是我们接下来会研究的问题</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200604110917247.png" alt="image-20200604110917247" style="zoom:50%;"></p>
<h5 id="Taylor-Series"><a href="#Taylor-Series" class="headerlink" title="Taylor Series"></a>Taylor Series</h5><p>泰勒公式定义如下，将函数h(x)在x=x0处展开</p>
<script type="math/tex; mode=display">
h(x) = \sum_{k=0}^{\infty}\frac{h^{(k)}(x_0)}{k!}(x-x_0)^k</script><p>当$x\rightarrow x_0$时，表达式可写为$h(x)\approx h(x_0)+h’(x_0)(x-x_0)$</p>
<p>对于二元函数，当$x\rightarrow x_0,y\rightarrow y_0$时，相应的表达式可以简化为</p>
<script type="math/tex; mode=display">
h(x,y)\approx h(x_0,y_0)+\frac{\partial h(x_0,y_0)}{\partial x}(x-x_0) + \frac{\partial h(x_0,y_0)}{\partial y}(y-y_0)</script><h5 id="Back-to-Formal-Derivation"><a href="#Back-to-Formal-Derivation" class="headerlink" title="Back to Formal Derivation"></a>Back to Formal Derivation</h5><p>损失函数loss可以用以下公式表示，</p>
<script type="math/tex; mode=display">
L(\theta)=L(a,b)+\frac{\partial L(a,b)}{\partial \theta_1}(\theta_1-a) +\frac{\partial L(a,b)}{\partial \theta_2}(\theta_2-b)</script><p>简化表达形式，令$s=L(a,b),\ u=\frac{\partial L(a,b)}{\partial \theta_1},\ v=\frac{\partial L(a,b)}{\partial \theta_2}$</p>
<p>则$L(\theta)\approx s+u(\theta_1-a)+v(\theta_2-b)$</p>
<p>如下图所示，在图中red circle的范围内，找到$\theta_1,\theta_2$，使得loss最小化，设red circle的半径为d，圆心坐标为(a,b)，就新增了一个限制条件$(\theta_1-a)^2+(\theta_2-b)^2\leq d^2$</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200604114112061.png" alt="image-20200604114112061" style="zoom:50%;"></p>
<p>由于s与$\theta_1,\theta_2$不相关，这里把s项去掉，$L(\theta)$可以转为内积</p>
<script type="math/tex; mode=display">
\begin{aligned}

L(\theta)_{min} & \approx   u(\theta_1-a)+v(\theta_2-b) \\
 & =  (u,v)\cdot(\theta_1-a,\theta_2-b) \\
 & =  (u,v)\cdot(\Delta \theta_1,\Delta\theta_2)
\end{aligned}</script><p>当$(\Delta \theta_1,\Delta\theta_2)$与$(u,v)$方向相反时，两者的内积为最小值，由于两者的模长不同，用参数$\eta$来表示两者之间的关系。</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200604114656448.png" alt="image-20200604114656448" style="zoom:50%;"></p>
<p>由于$\Delta\theta_1=\theta_1-a,\Delta\theta_2=\theta_2-b$，则$\theta_1=a+\Delta\theta_1,\theta_2=b+\Delta\theta_2$，可得出</p>
<script type="math/tex; mode=display">
\begin{bmatrix} \theta_1 \\ \theta_2  \end{bmatrix}
=
\begin{bmatrix} a \\ b  \end{bmatrix}
- \eta \begin{bmatrix} u \\ v  \end{bmatrix}

=
\begin{bmatrix} a \\ b  \end{bmatrix}
- \eta \begin{bmatrix} \frac{\partial L(a,b)}{\partial \theta_1} \\ \frac{\partial L(a,b)}{\partial \theta_2}  \end{bmatrix}</script><h5 id="Limitation"><a href="#Limitation" class="headerlink" title="Limitation"></a>Limitation</h5><p>在真实的实验环境中，我们往往会设置一个临界值（比如$10^{-4}$），当该点的梯度小于该值（即$\approx 0$）时，就停止训练。</p>
<p>因此，gradient descent的限制是，gradient为0的点并不一定是local minimum，还有可能是saddle point，也有可能是接近于0的点</p>
<p><img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200604123420086.png" alt="image-20200604123420086" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>ML-notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>gradient descent</tag>
        <tag>adagrad</tag>
        <tag>SGD</tag>
      </tags>
  </entry>
  <entry>
    <title>指令级并行</title>
    <url>/2020/03/11/%E6%8C%87%E4%BB%A4%E7%BA%A7%E5%B9%B6%E8%A1%8C/</url>
    <content><![CDATA[<h5 id="指令级并行"><a href="#指令级并行" class="headerlink" title="指令级并行"></a>指令级并行</h5><p>指令之间存在的并行性，利用它，计算机可以并行执行两条或两条以上的指令。开发ILP的途径有两种：</p>
<ul>
<li>资源重复，重复设置多个处理部件；</li>
<li>采用流水线技术，使指令重叠并行执行。</li>
</ul>
<p>指令的<strong>静态调度</strong>：依靠编译器进行静态调度，以减少相关和冲突。它不是在程序执行的过程中，而是在编译期间进行调度和优化。</p>
<p>指令的<strong>动态调度</strong>：能够在保持程序数据流和异常行为的情况下，通过硬件对指令执行顺序进行重新安排，以提高流水线的利用率且减少停顿现象。是由硬件在程序执行时进行优化的。</p>
<p>基本程序块：一串连续的代码除了入口和出口以外，没有其他的分支指令和转入点。</p>
<p>不精确异常：当执行指令i导致发生异常时，处理机的现场与严格按照程序顺序执行时指令i的现场不同。</p>
<p>精确异常：发生异常时，处理机的现场与严格按照程序顺序执行指令i的现场相同。</p>
<h5 id="相关与冲突"><a href="#相关与冲突" class="headerlink" title="相关与冲突"></a>相关与冲突</h5><ul>
<li>数据相关</li>
<li>名相关，名指指令所访问的寄存器或存储器单元的名称。<ul>
<li>反相关</li>
<li>输出相关</li>
</ul>
</li>
<li>控制相关</li>
</ul>
<p>流水线冲突 指由于相关的存在，使得指令流中的下一条指令不能在指定的时钟周期执行。<u>结构冲突</u>是硬件资源冲突引起的，<u>数据冲突</u>是数据相关和名相关引起的，<u>控制冲突</u>是控制相关引起的。</p>
<p>为了保证程序执行的正确性，必须保证的最关键的两个特性是：</p>
<ul>
<li><u>数据流</u></li>
<li>保持<u>异常行为</u>：无论怎么改变指令的执行顺序，都不能改变程序中异常的发生情况。</li>
</ul>
<h4 id="指令的动态调度"><a href="#指令的动态调度" class="headerlink" title="指令的动态调度"></a>指令的动态调度</h4><p>能够在保持程序数据流和异常行为的情况下，通过硬件对指令执行顺序进行重新安排，以提高流水线的利用率且减少停顿现象。优点：</p>
<ul>
<li>能够处理一些<u>编译时情况不明的相关</u>（比如涉及存储器访问的相关），并简化了编译器；</li>
<li>能够使本来是面向某一流水线优化编译的代码在<u>其他动态调度的流水线上</u>也能执行。</li>
</ul>
<h5 id="记分牌"><a href="#记分牌" class="headerlink" title="记分牌"></a>记分牌</h5><p>把流水线中的译码段ID分解成了两个段：流出和读操作数。记录的信息由三部分组成：</p>
<ul>
<li>指令状态表</li>
<li>功能部件状态表</li>
<li>结果寄存器状态表</li>
</ul>
<h5 id="Tomasulo"><a href="#Tomasulo" class="headerlink" title="Tomasulo"></a>Tomasulo</h5><p>核心思想：（1）记录和检测指令相关，操作数一旦就绪就立即执行，把发生RAW冲突的可能性较小到最低；（2）通过寄存器换名来消除WAR冲突和WAW冲突。寄存器换名是通过保留站来实现，它保存正在流出和等待流出指令所需要的操作数。</p>
<p>基本思想：只要操作数有效，就取其放到保留站中，避免指令流出时才到寄存器取数据，这就使得即将执行的指令从相应的保留站中读取数据，而不用到寄存器中。指令的执行结果也是直接放到等待数据的其它保留站中去。一条指令流出时，存放操作数的寄存器被换名为该寄存器对应的保留站的名称。</p>
<h4 id="动态分支预测"><a href="#动态分支预测" class="headerlink" title="动态分支预测"></a>动态分支预测</h4><p>用硬件动态地进行分支处理的方法。在程序运行的过程中，根据分支指令过去的表现预测将来的行为。如果分支行为发生了变化，预测结果也跟着变化。</p>
<h5 id="分支历史表（BHT）"><a href="#分支历史表（BHT）" class="headerlink" title="分支历史表（BHT）"></a>分支历史表（BHT）</h5><p>用来记录相关指令最近一次或几次的执行情况是成功还是失败，并根据此进行预测。</p>
<h5 id="分支目标缓冲器（BTB）"><a href="#分支目标缓冲器（BTB）" class="headerlink" title="分支目标缓冲器（BTB）"></a>分支目标缓冲器（BTB）</h5><p>是一种动态分支预测技术。将（1）<u>执行过的成功分支指令的目标地址</u>（2）<u>预测的分支目标地址</u> 记录在一张硬件表格中。在每次取指令时，用该指令的地址与BTB表中所有项目的第一个字段进行比较，以便尽早知道指令是否分支成功，尽早知道目标地址，达到减少分支开销的目的。</p>
<p>BHT方法是在<u>译码ID</u>段，就能获得分支目标地址、顺序下一条指令以及预测的结果。BTB方法在<u>取址IF</u>段就能知道这些信息。</p>
<h5 id="基于硬件的前瞻执行"><a href="#基于硬件的前瞻执行" class="headerlink" title="基于硬件的前瞻执行"></a>基于硬件的前瞻执行</h5><p>是解决控制相关的方法，它对分支指令的结果进行猜测，然后按照这个猜测继续取址、流出和执行后续的指令。但指令执行的结果不写回寄存器或存储器，而是放到一个称为ROB（再定序列缓冲器）中。等到相应的指令得到“确认”（即确实是应该执行）后，才将结果写入寄存器或存储器中。</p>
<p>把三种思想结合在了一起：</p>
<ul>
<li><u>动态分支预测</u>。用来选择后续执行的指令。</li>
<li>在控制相关的结果尚未出来之前，<u>前瞻</u>地执行后续指令。</li>
<li>用<u>动态调度</u>对基本块的各种组合进行<u>跨基本块的组合</u>。</li>
</ul>
<h4 id="超标量流水线"><a href="#超标量流水线" class="headerlink" title="超标量流水线"></a>超标量流水线</h4><p>是一种多指令流出技术。它在每个时钟周期流出的指令条数不固定，依代码的具体情况而定，但有个上限。</p>
<h4 id="超长指令字"><a href="#超长指令字" class="headerlink" title="超长指令字"></a>超长指令字</h4><p>是一种多指令流出技术。在每个时钟周期内流出的指令数都是固定的，这些指令构成一条长指令或者一个指令包，在这个指令包中，指令之间的并行性是通过指令显式地表现出来的。这种处理机的指令调度由编译器静态完成。</p>
]]></content>
      <categories>
        <category>计算机体系结构</category>
      </categories>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>指令级并行</tag>
        <tag>指令相关</tag>
        <tag>指令冲突</tag>
        <tag>动态调度</tag>
        <tag>记分牌</tag>
        <tag>Tomasulo</tag>
        <tag>BHT</tag>
        <tag>BTB</tag>
        <tag>ROB</tag>
        <tag>前瞻执行</tag>
        <tag>超标量</tag>
        <tag>VLIW</tag>
        <tag>超长指令字</tag>
      </tags>
  </entry>
  <entry>
    <title>流水线</title>
    <url>/2020/03/25/%E6%B5%81%E6%B0%B4%E7%BA%BF/</url>
    <content><![CDATA[<h4 id="流水线的特点"><a href="#流水线的特点" class="headerlink" title="流水线的特点"></a>流水线的特点</h4><ul>
<li>把一个处理过程<u>分解</u>为若干个子过程，每个过程由一个专门的功能部件来实现</li>
<li>各<u>功能段的时间</u>应尽可能相等，否则将引起流水线堵塞和断流</li>
<li>每一个段的后面都要有一个<u>缓冲寄存器</u>，称为流水寄存器</li>
<li>适合于大量<u>重复</u>的时序过程，只有在源源不断地输入任务，才能充分发挥流水线的效率</li>
<li>需要有<u>通过时间与排空时间</u></li>
</ul>
<p>在指令流水线中，解决控制相关的方法主要有：<u>冻结或排空流水线</u>、<u>预测分支失败</u>、<u>预测分支成功</u>、<u>延迟分支</u>。</p>
<h4 id="减少分支延迟"><a href="#减少分支延迟" class="headerlink" title="减少分支延迟"></a>减少分支延迟</h4><p>三种通过软件（编译器）来处理的方法（静态的方法）：预测分支失败/成功，延迟分支。</p>
<p><strong>3种方法的共同特点</strong>是：它们对分支的处理方法在<u>程序执行过程中</u>是<u>始终不变</u>的，它们要么总是预测分支失败，要么总是预测分支成功。</p>
<h5 id="预测分支失败"><a href="#预测分支失败" class="headerlink" title="预测分支失败"></a>预测分支失败</h5><p>沿着失败的指令继续处理指令，就好像什么都没发生过似的。当确定分支指令是失败时，说明预测正确，流水线正常流动；当确定分支指令是成功时，流水线就把分支指令之后取出的指令转化为空操作，并按分支目标地址重新取指令执行。</p>
<h5 id="预测分支成功"><a href="#预测分支成功" class="headerlink" title="预测分支成功"></a>预测分支成功</h5><p>当流水线ID段检测到分支指令后，一旦算出了分支目标地址，就开始从该目标地址取指令执行。</p>
<h5 id="延迟分支"><a href="#延迟分支" class="headerlink" title="延迟分支"></a>延迟分支</h5><p>主要思想是从逻辑上“延长”分支指令的执行时间。把延迟分支看成是由分支指令和若干个延迟槽构成，不管分支是否成功，都要执行延迟槽内的指令。</p>
<ul>
<li><p>从前调度</p>
</li>
<li><p>从目标处调度</p>
</li>
<li><p>从失败出调度</p>
</li>
</ul>
<p>三种调度方法的优缺点分别是：<img src="/2020/03/25/流水线/image-20200314165639374.png" alt="三种调度方法的优缺点"></p>
<h4 id="流水线的额外开销"><a href="#流水线的额外开销" class="headerlink" title="流水线的额外开销"></a>流水线的额外开销</h4><ul>
<li>流水寄存器延迟，流水线段与段之间都要设置流水寄存器；</li>
<li>时钟偏移开销，流水线中的时钟到达各流水寄存器的最大差值时间，时钟到达各流水寄存器的时间不是完全相同的。</li>
</ul>
]]></content>
      <categories>
        <category>软件工程</category>
      </categories>
      <tags>
        <tag>流水线</tag>
        <tag>吞吐率</tag>
        <tag>加速比</tag>
        <tag>效率</tag>
      </tags>
  </entry>
  <entry>
    <title>测试</title>
    <url>/2020/03/12/%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<h5 id="测试步骤"><a href="#测试步骤" class="headerlink" title="测试步骤"></a>测试步骤</h5><ul>
<li>模块（单元）测试</li>
<li>子系统测试</li>
<li>系统（集成）测试</li>
<li>验收（确认）测试</li>
<li>平行运行</li>
</ul>
<h5 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h5><p>主要使用白盒测试技术。集中检测软件设计的最小单元-模块，对重要的执行通路进行测试，可以发现模块内部的错误。可以用代码审查和计算机测试，来完成单元测试工作。测试重点为：</p>
<ul>
<li>模块接口</li>
<li>局部数据结构</li>
<li>重要的执行通路</li>
<li>出错处理通路</li>
<li>边界条件</li>
</ul>
<h5 id="集成测试"><a href="#集成测试" class="headerlink" title="集成测试"></a>集成测试</h5><p>是测试和组装软件的系统化技术，把模块组装成程序，在组装的过程进行测试。</p>
<p>非渐增式：先分别测试所有模块，再把这些模块组合成要求的程序；</p>
<p>渐增式：把下一个将要测试的模块同已经测试好的那些模块组合到一起进行测试；</p>
<ul>
<li>自底向上集成</li>
<li>自顶向下集成</li>
</ul>
<p>模块测试的特点是主要应用白盒测试，对多个模块的测试可以并发进行；集成测试的特点是可以在测试过程发现接口问题。</p>
<p>回归测试：重新执行已经做过测试的某个测试的子集。</p>
<p><img src="/2020/03/12/测试/test.png" alt></p>
]]></content>
      <categories>
        <category>软件工程</category>
      </categories>
      <tags>
        <tag>软件工程</tag>
        <tag>单元测试</tag>
        <tag>集成测试</tag>
        <tag>白盒测试</tag>
        <tag>黑盒测试</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机体系结构重要考点</title>
    <url>/2020/03/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E9%87%8D%E8%A6%81%E8%80%83%E7%82%B9/</url>
    <content><![CDATA[<h4 id="同时多线程"><a href="#同时多线程" class="headerlink" title="同时多线程"></a>同时多线程</h4><p>线程：进程内的一个相对独立、可独立调度和指派的执行单元，它比进程要“轻巧”得多。线程中包含有调度所需的信息，但它自己基本上不拥有系统资源，只拥有在运行过程中必不可少的一点资源。线程切换时，只需保存和设置少量寄存器的内容，开销很小。</p>
<p>线程级并行：指并行执行两个或两个以上的线程。</p>
<p>同时多线程（SMT）：是一种在多流出、动态调度的处理器上结合了线程级并行和指令级并行的技术，它是多线程技术的一种改进。通过寄存器命名和动态调度机制，来自各个独立线程的多条指令可以同时流出，而不用考虑它们之间的相互依赖关系。</p>
<h5 id="实现多线程的主要方法"><a href="#实现多线程的主要方法" class="headerlink" title="实现多线程的主要方法"></a>实现多线程的主要方法</h5><p><u>细粒度多线程</u>：在每条指令之间都可以进行线程的切换，从而使得多个线程可以交替执行。</p>
<ul>
<li>优点：能够隐藏由长时间停顿带来的吞吐率的损失，也能够隐藏由段时间停顿带来的损失。</li>
<li>缺点：减慢了单个线程的执行。即使没有任何停顿的线程也不能连续执行，而且会因为其他线程的指令的插入执行而被延迟。</li>
</ul>
<p><u>粗粒度多线程</u>：线程之间的切换只发生在时间较长的停顿出现的时候。</p>
<ul>
<li>优点：减少了切换次数，很大程度上也不会降低单个线程的执行速度。</li>
<li>缺点：减少吞吐率损失的能力有限。由于实现粗粒度多线程的CPU只能执行单个线程的指令，不能交叉执行多个线程，因此当发生停顿时，流水线必须排空或暂停。</li>
</ul>
<h4 id="存储系统"><a href="#存储系统" class="headerlink" title="存储系统"></a>存储系统</h4><h5 id="写策略"><a href="#写策略" class="headerlink" title="写策略"></a>写策略</h5><p>写直达法：在执行写操作时，不仅把数据写入Cathe中相应的块，而且也写入下一级存储器。减少CPU写停顿的常用方法是，采用写缓冲器。</p>
<p>写回法：只把数据写入Cathe中相应的块，不写入下一级存储器。这些最新数据只有在相应的块被替换出时，才被写回下一级存储器。速度快，要设置“修改位”。</p>
<p>写不命中时，</p>
<p>按写分配法：写不命中时，把所写单元的块从主存中调入Cathe，然后再进行写入。与<u>写回法</u>配合。</p>
<p>不按写分配法：写不命中时，直接写入下一级存储器而不将相应的块调入Cathe。</p>
<h5 id="三种类型的Cathe不命中"><a href="#三种类型的Cathe不命中" class="headerlink" title="三种类型的Cathe不命中"></a>三种类型的Cathe不命中</h5><p><u>强制性不命中</u>：当第一次访问一个快时，该块不在Cathe中，需从下一级存储器中调入Cathe。（冷启动访问不命中，首次访问不命中）</p>
<p><u>容量不命中</u>：如果<u>程序执行时所需的块</u>不能全部调入Cathe中，当某些块被调出后，若又重新被访问，就会发生不命中。</p>
<p><u>冲突不命中</u>：在组相联或者直接映像Cathe中，若太多的块映像到同一组（块）中，则会出现该组中某个块被别的块替换掉，然后又重新被访问的情况。（碰撞不命中，冲突不命中）</p>
<p>关系：</p>
<ul>
<li><strong>相联度</strong>越高，冲突不命中就越少；</li>
<li>强制性不命中和容量不命中不受相联度的影响；</li>
<li>强制性不命中不受<strong>Cathe容量</strong>的影响，但容量不命中却随着容量的增加而减少。</li>
</ul>
<h4 id="IO系统"><a href="#IO系统" class="headerlink" title="IO系统"></a>IO系统</h4><h5 id="I-O系统的可靠性、可用性和可信性"><a href="#I-O系统的可靠性、可用性和可信性" class="headerlink" title="I/O系统的可靠性、可用性和可信性"></a>I/O系统的可靠性、可用性和可信性</h5><p>可靠性：系统从某个初始参考点开始<u>一直连续提供服务的能力</u>，用平均无故障时间（<strong>MTTF</strong>）来衡量。MTTF的导数是失效率。系统总体的失效率是各部件的失效率之和。系统<u>中断服务的时间</u>用平均修复时间（<strong>MTTR</strong>）来衡量。</p>
<p>可用性：系统正常工作的时间在连续两次服务间隔时间中所占的比例。</p>
<script type="math/tex; mode=display">
可用性=\frac{MTTF}{MTTF+MTTR}</script><p>   平均失效间隔时间，MTBF</p>
<script type="math/tex; mode=display">
MTBF=MTTF+MTTR</script><p>可信性：是服务的质量，是<u>不可度量</u>的。</p>
<h5 id="廉价磁盘冗余阵列-RAID"><a href="#廉价磁盘冗余阵列-RAID" class="headerlink" title="廉价磁盘冗余阵列 (RAID)"></a>廉价磁盘冗余阵列 (RAID)</h5><p>RAID0：<u>无冗余和无校验</u>的磁盘阵列，成本最低。</p>
<p>RAID1：<u>镜像</u>磁盘阵列。</p>
<p>RAID2：采用纠错的<u>海明码</u>的磁盘阵列，冗余盘数量为$log_2m$，m为数据盘的个数。</p>
<p>RAID3：<u>位</u>交叉<u>奇偶校验</u>，只需要一个校验盘。</p>
<p>RAID4：<u>块</u>交叉<u>奇偶校验</u>，只需要一个校验盘。</p>
<p>RAID5：<u>无独立校验</u>的<u>奇偶校验</u>磁盘阵列，将校验信息分布到磁盘阵列中的各个磁盘。</p>
<p>以上奇偶校验磁盘阵列只是在<u>一个盘</u>出现故障的情况下，仍能继续工作和恢复数据。</p>
<p>RAID6：<u>P+Q双校验</u>磁盘阵列，可以容忍<u>两个磁盘</u>出错。</p>
<p>RAID10：RAID1+0，先进行镜像（RAID1），再进行条带存放（RAID0）。<br>RAID01：RAID0+1，先进行条带存放（RAID0），再进行镜像（RAID1）。</p>
]]></content>
      <categories>
        <category>计算机体系结构</category>
      </categories>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>机群</tag>
        <tag>MPP</tag>
        <tag>SMP</tag>
        <tag>消息传递</tag>
        <tag>共享存储器</tag>
        <tag>同时多线程</tag>
        <tag>RAID</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机体系结构的基础知识</title>
    <url>/2020/03/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<h4 id="计算机系统结构、组成和实现"><a href="#计算机系统结构、组成和实现" class="headerlink" title="计算机系统结构、组成和实现"></a>计算机系统结构、组成和实现</h4><p>计算机系统结构是指传统机器程序员所看到的计算机属性，即概念结构与功能特性。</p>
<p>在设计主存系统的时候，确定主存容量、寻址方式等属于计算机体系结构；确定时钟周期、逻辑设计等属于计算机组成；而主存存储芯片的类型、线路设计、微组装技术属于计算机实现。</p>
<p>计算机组成是计算机体系结构的逻辑实现，计算机实现是计算机体系结构的物理实现。一种体系结构可以有多种组成，一种组成可以有多种实现。</p>
<h4 id="计算机系统结构的分类"><a href="#计算机系统结构的分类" class="headerlink" title="计算机系统结构的分类"></a>计算机系统结构的分类</h4><h5 id="Flynn分类法"><a href="#Flynn分类法" class="headerlink" title="Flynn分类法"></a>Flynn分类法</h5><p>按照指令流和数据流的多倍性进行分类的。</p>
<ul>
<li>SISD，标量流水线</li>
<li>SIMD，向量流水线</li>
<li>MISD，只是一种人为的划分，目前并没有实际的机器</li>
<li>MIMD，多处理机，<ul>
<li>机群，</li>
<li>PVP（并行向量处理机），</li>
<li>SMP（对称式共享存储器多处理机），</li>
<li>MPP（大规模并行处理机），</li>
<li>DSM（分布式共享存储器多处理机）。</li>
</ul>
</li>
</ul>
<h5 id="冯氏分类法"><a href="#冯氏分类法" class="headerlink" title="冯氏分类法"></a>冯氏分类法</h5><p>用系统的最大并行度对计算机进行分类。最大并行度$P_m$为：单位时间内能够处理的最大二进制位数。</p>
<ul>
<li>字串位串</li>
<li>字串位并</li>
<li>字并位串</li>
<li>字并位并</li>
</ul>
<h5 id="Handler分类法"><a href="#Handler分类法" class="headerlink" title="Handler分类法"></a>Handler分类法</h5><p>根据并行度和流水线来分类。</p>
<h4 id="计算机体系结构设计的主要方法"><a href="#计算机体系结构设计的主要方法" class="headerlink" title="计算机体系结构设计的主要方法"></a>计算机体系结构设计的主要方法</h4><h5 id="由上往下"><a href="#由上往下" class="headerlink" title="由上往下"></a>由上往下</h5><p>从层次结构中的最上面一级开始，逐层往下设计各层机器。先设计软件，再设计硬件。</p>
<h5 id="由下往上"><a href="#由下往上" class="headerlink" title="由下往上"></a>由下往上</h5><p>从层次结构的最下面一级开始，逐层往上设计各层的机器。先设计硬件，再设计软件。</p>
<h5 id="从中间开始"><a href="#从中间开始" class="headerlink" title="从中间开始"></a>从中间开始</h5><p>这里的“中间”是指层次结构中软硬件的交界面，一般指的是传统机器语言级和操作系统级之间。先进行软硬件的分工，确定好这个界面，从这个界面开始，软件设计者开始往上设计操作系统、编译系统等，硬件设计者开始往下设计传统机器级、微程序机器级等。优点：</p>
<ul>
<li>软件和硬件并行设计可以缩短设计周期，设计过程中可以交流协调，是一种交互式的、很好的设计方案。</li>
</ul>
<h4 id="实现软件的可移植性"><a href="#实现软件的可移植性" class="headerlink" title="实现软件的可移植性"></a>实现软件的可移植性</h4><h5 id="统一高级语言"><a href="#统一高级语言" class="headerlink" title="统一高级语言"></a>统一高级语言</h5><p>可以解决所有计算机之间的软件移植，比如Java语言。</p>
<h5 id="采用系列机"><a href="#采用系列机" class="headerlink" title="采用系列机"></a>采用系列机</h5><p>系列机是指同一厂家生产的具有相同的系统结构，但具有不同组成和实现的同一系列不同型号的计算机。</p>
<p>采用系列机可以解决<u>同一系列的计算机</u>之间的软件移植。</p>
<ul>
<li>向上兼容，程序不修改就可以运行于比它<u>高档</u>的计算机</li>
<li>向后兼容，程序不修改就可以运行于在它<u>之后投入市场</u>的计算机</li>
</ul>
<h5 id="模拟和仿真"><a href="#模拟和仿真" class="headerlink" title="模拟和仿真"></a>模拟和仿真</h5><p>可以在不同体系结构的计算机之间相互移植，在一种计算机上实现另一种计算机的指令系统。</p>
<p><strong>模拟</strong>是用<u>软件</u>的方法在一台计算机（宿主机）上实现另一台计算机（虚拟机）的指令系统。运行速度慢，性能较差。</p>
<p><strong>仿真</strong>是用现有计算机（宿主机）的<u>微程序</u>去解释实现另一台计算机（目标机）的指令系统。</p>
<p>主要区别在于解释执行所用的语言。仿真是用微程序去解释，其解释程序放在控制存储器中；模拟是用机器语言去解释，其解释程序放在主存中。仿真的运行速度更快，但仿真只能在系统结构差异不大的计算机之间使用。</p>
<h4 id="摩尔定律"><a href="#摩尔定律" class="headerlink" title="摩尔定律"></a>摩尔定律</h4><p>集成电路芯片上所集成的晶体管数目每隔18个月就翻一番。</p>
<h4 id="并行性"><a href="#并行性" class="headerlink" title="并行性"></a>并行性</h4><p>计算机系统的并行性具有不同的等级。</p>
<p>从处理数据的角度来看：</p>
<ul>
<li>字串位串，每次只对一个字的一位进行处理</li>
<li>字串位并</li>
<li>字并位串</li>
<li>字并位并</li>
</ul>
<p>从执行程序的角度来看：</p>
<ul>
<li>指令内部并行，单指令之间各种微操作并行</li>
<li>指令级并行，并行或并发执行两条或两条以上指令</li>
<li>线程级并行<ul>
<li>线程是一个相对独立、可独立调度和指派的执行单元，它比进程要轻巧。</li>
</ul>
</li>
<li>任务级或过程级并行</li>
<li>作业或程序级并行</li>
</ul>
<h4 id="指令集结构"><a href="#指令集结构" class="headerlink" title="指令集结构"></a>指令集结构</h4><p>根据CPU内部状态，可以将指令系统的机构分为：</p>
<ul>
<li><u>堆栈</u>型结构</li>
<li><u>累加器</u>型结构</li>
<li><u>通用寄存器</u>结构</li>
</ul>
<p>将通用存储器型结构进一步细分为：<u>RR型、RM型、MM型</u>。</p>
<h4 id="向量的处理方式"><a href="#向量的处理方式" class="headerlink" title="向量的处理方式"></a>向量的处理方式</h4><ul>
<li>横向处理方式，按行的方式从左到右进行计算的，相当于进行了N次循环；</li>
<li>纵向处理方式，按列的方式从上到下进行计算的，将整个向量按相同的运算处理完之后，再去进行别的运算；</li>
<li>纵横处理方式，又称为分组处理方式，把向量分成若干组，组内按照纵向方式处理，一次处理各组。</li>
</ul>
<p>后面两种方式都适用于向量处理机。</p>
<h4 id="提高向量处理机性能的常用技术"><a href="#提高向量处理机性能的常用技术" class="headerlink" title="提高向量处理机性能的常用技术"></a>提高向量处理机性能的常用技术</h4><ul>
<li>设置多个处理部件，使它们并行工作；</li>
<li>采用链接技术，加快一串向量指令的执行；</li>
<li>采用循环开采技术，加快循环的处理；</li>
<li>采用多处理机系统，进一步提高系统性能。</li>
</ul>
<p><strong>链接</strong>：具有<u>先写后读相关（RAW）</u>的两条指令，在不出现<u>功能部件</u>冲突和其他<u>$V_i$（寄存器）</u>冲突的情况下，可以把功能部件链接起来进行流水处理。</p>
<p><strong>分段开采技术</strong>：当向量的长度大于向量处理器的长度时，必须把长向量分为长度固定的段，然后循环分段处理，每一次循环只处理一个向量段。</p>
]]></content>
      <categories>
        <category>计算机体系结构</category>
      </categories>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>Flynn分类法</tag>
        <tag>冯氏分类法</tag>
        <tag>Handler分类法</tag>
        <tag>模拟</tag>
        <tag>仿真</tag>
        <tag>系列机</tag>
        <tag>分段开采</tag>
        <tag>链接</tag>
      </tags>
  </entry>
  <entry>
    <title>软件工程学概述</title>
    <url>/2020/03/08/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E5%AD%A6%E6%A6%82%E8%BF%B0/</url>
    <content><![CDATA[<h4 id="软件危机"><a href="#软件危机" class="headerlink" title="软件危机"></a>软件危机</h4><ol>
<li><p>定义<br>在计算机软件的开发和维护过程会出现的一系列严重问题。包含两方面的问题：如何开发软件，以满足对软件日益增长的需求；如何维护数量不断膨胀的已有软件。</p>
</li>
<li><p>典型表现：</p>
<ul>
<li>软件成本日益增长；</li>
<li>软件开发效率低；</li>
<li>软件质量差；</li>
<li>软件维护困难；</li>
<li>软件开发速度跟不上计算机发展速度；</li>
<li>软件通常没有合适的文档资料。</li>
</ul>
</li>
<li><p>产生原因：</p>
<ul>
<li>技术原因<ul>
<li>软件<strong>规模</strong>越来越大；</li>
<li>软件的<strong>复杂度</strong>越来越高；</li>
</ul>
</li>
<li>管理原因<ul>
<li>软件开发缺乏正确的理论指导，过分依赖个人技巧和创造性；</li>
<li>对用户需求没有完整正确的认识，就匆忙着手忙于编写程序。</li>
</ul>
</li>
</ul>
</li>
<li><p>消除软件危机的途径</p>
<ul>
<li>对计算机软件有一个正确的认识；</li>
<li>还必须充分认识到软件开发是一种组织良好、管理严密、各类人员协同配合、共同完成的工程项目；</li>
<li>推广使用在时间中总结出来的开发软件的成功的技术和方法，并且探索更好更加有效的技术和方法；</li>
<li>开发和使用更好的软件工具。</li>
</ul>
</li>
</ol>
<h4 id="软件工程"><a href="#软件工程" class="headerlink" title="软件工程"></a>软件工程</h4><ol>
<li>定义：<ul>
<li>把系统的、规范的、可度量的途经应用于软件开发、运行和维护过程，也就是把工程应用于软件；</li>
<li>研究1提到的途径。</li>
</ul>
</li>
<li>基本原理：<ul>
<li>用分阶段的生命周期计划严格管理</li>
<li>坚持进行阶段评审</li>
<li>实行严格的产品控制</li>
<li>结果应能清楚地审查</li>
<li>采用现代化程序设计技术</li>
<li>开发小组人员应少而精</li>
<li>必须承认不断改进软件工程的必要性</li>
</ul>
</li>
<li>软件工程方法学的3个要素：<u>方法、工具和过程</u>。</li>
</ol>
<h4 id="软件生命周期"><a href="#软件生命周期" class="headerlink" title="软件生命周期"></a>软件生命周期</h4><p>由软件定义、软件开发和运行维护3个时期组成。</p>
<ol>
<li>软件定义<ul>
<li><u>问题定义</u></li>
<li><u>可行性分析</u></li>
<li><u>需求分析</u>，确定目标系统具有哪些功能，并用正式文档记录对目标系统的需求（规格说明书）；</li>
</ul>
</li>
<li>软件开发<ul>
<li><u>总体设计</u></li>
<li><u>详细设计</u></li>
<li><u>单元和编码测试</u></li>
<li><u>综合测试</u>，最基本的是集成测试和验收测试</li>
</ul>
</li>
<li><u>软件维护</u>，包括四类维护性活动：<u>改正性维护，完善性维护，适应性维护，预防性维护</u>。</li>
</ol>
<h4 id="软件过程"><a href="#软件过程" class="headerlink" title="软件过程"></a>软件过程</h4><p>为了获得高质量软件所需要完成的一系列任务的框架，它规定了完成各项任务的工作步骤。</p>
<ol>
<li><p>瀑布模型</p>
<ul>
<li>具有顺序性和依赖性</li>
<li>推迟实现</li>
<li>质量保证<br>是带<u>反馈环</u>的，每个阶段都有必须完成的文档，在每个阶段结束前都要对文档进行评审，是<u>文档驱动</u>的。</li>
</ul>
</li>
<li><p>快速原型模型<br>快速建立起来的可以在计算机上运行的程序，它所能完成的功能往往是最终产品能完成功能的一个子集。不带反馈环。</p>
</li>
<li><p>增量模型<br>把软件产品作为一系列的增量构件来设计、编码、集成和测试。每个构件由多个相互作用的模块构成，并且能够完成特定的功能。<br>优点：</p>
<ul>
<li>能在较短时间内向用户提交可完成部分工作的产品；</li>
<li>使用户有比较充裕的时间学习和适应新产品，减少一个全新软件可能给客户组织带来的冲击。</li>
</ul>
</li>
<li><p>螺旋模型<br>使用原型以及其他方法来尽量降低风险，看作是在每个阶段之前都增加了风险分析过程的快速原型模型。<br>优点：</p>
<ul>
<li>有利于已有软件的重用</li>
<li>有助于把软件质量作为软件开发的一个重要目标</li>
<li>减少了过多测试所带来的风险</li>
<li>维护只是模型的另一个周期，在维护和开发之前并没有本质区别</li>
<li><u>风险驱动</u>，可以及时中断项目。<br>风险驱动也是它的一个弱点，需要专业的风险评估人员。</li>
</ul>
</li>
<li><p>喷泉模型<br>是典型的面向对象的软件过程模型之一，体现了面向对象软件开发过程<u><strong>迭代</strong></u>和<u><strong>无缝</strong></u>的特性。</p>
</li>
</ol>
<h4 id="可行性分析"><a href="#可行性分析" class="headerlink" title="可行性分析"></a>可行性分析</h4><p>包括：</p>
<ul>
<li>技术可行性</li>
<li>经济可行性</li>
<li>操作可行性</li>
<li>法律可行性</li>
</ul>
<h5 id="基本过程"><a href="#基本过程" class="headerlink" title="基本过程"></a>基本过程</h5><ul>
<li>复查系统规模和目标；</li>
<li>研究目前正在使用的系统；</li>
<li>导出新系统的高层逻辑模型；</li>
<li>进一步定义问题；</li>
<li>导出和评价供选择的解法；</li>
<li>推荐行动方针；</li>
<li>草拟开发计划；</li>
<li>书写文档提交审查。</li>
</ul>
<h5 id="数据流图"><a href="#数据流图" class="headerlink" title="数据流图"></a>数据流图</h5><p>基本要素：</p>
<ul>
<li>数据的源点/终点</li>
<li>变换数据的处理</li>
<li>数据存储</li>
<li>数据流</li>
</ul>
<h4 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h4><h5 id="需求分析的任务"><a href="#需求分析的任务" class="headerlink" title="需求分析的任务"></a>需求分析的任务</h5><ul>
<li>确定对系统的综合要求，功能要求，性能要求，接口需求，可靠性和可用性需求；</li>
<li>分析系统的数据要求，建立数据模型（E-R图）；</li>
<li>导出系统的逻辑模型，用数据流图、E-R图、状态转换图、数据字典和主要的处理算法来描述这个逻辑模型；</li>
<li>修正系统开发计划。</li>
</ul>
<h5 id="获取需求的方法"><a href="#获取需求的方法" class="headerlink" title="获取需求的方法"></a>获取需求的方法</h5><ul>
<li>访谈；</li>
<li>面向数据流自顶向下求精，<u>结构化分析方法</u>就是面向<u>数据流</u>自顶向下逐步求精进行需求分析的方法；</li>
<li>简易的应用规格说明技术，提倡用户与开发者密切合作；</li>
<li>快速建立软件模型。</li>
</ul>
<h5 id="需求分析的过程"><a href="#需求分析的过程" class="headerlink" title="需求分析的过程"></a>需求分析的过程</h5><ul>
<li>问题识别</li>
<li>分析与综合</li>
<li>编写需求文档分析</li>
<li>评审</li>
</ul>
<h5 id="需求分析规格说明书"><a href="#需求分析规格说明书" class="headerlink" title="需求分析规格说明书"></a>需求分析规格说明书</h5><p>主要内容为：描述系统的数据要求、功能要求、性能要求、可靠性和可用性要求、出错处理需求、接口需求、约束、逆向需求以及将来可能提出的要求。</p>
<h5 id="主要建模工具"><a href="#主要建模工具" class="headerlink" title="主要建模工具"></a>主要建模工具</h5><ul>
<li>建立功能模型的<u>数据流图</u></li>
<li>建立数据模型的<u>E-R图</u></li>
<li>建立行为模型的<u>状态图</u></li>
<li>层次方框图</li>
<li>Warnier图</li>
<li>IPO图   </li>
</ul>
]]></content>
      <categories>
        <category>软件工程</category>
      </categories>
      <tags>
        <tag>软件工程</tag>
        <tag>软件过程</tag>
        <tag>软件危机</tag>
        <tag>软件生命周期</tag>
        <tag>软件工程方法学</tag>
      </tags>
  </entry>
</search>
